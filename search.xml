<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>简易的Selenium爬虫工具</title>
    <url>/2024/08/03/%E7%AE%80%E6%98%93%E7%9A%84Selenium%E7%88%AC%E8%99%AB%E5%B7%A5%E5%85%B7/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>尝试使用Selenium自动化测试库对Github中指定topic的所有库信息进行爬取！</p>
<span id="more"></span>

<p>首先简单介绍下<a href="https://selenium-python.readthedocs.io/index.html">Selenium库</a>。这是一个针对浏览器应用的自动化测试库，能够模拟人类的点击、敲击键盘、输入文字等操作，因此也能够很好地作为爬虫工具使用。Selenium的使用原理就是首先创建浏览器驱动<code>WebDriver</code>，并获取给定url的HTML，使用<code>Selector</code>对HTML元素进行选取，然后进行自定义的操作。</p>
<p>本文中，我们将使用Selenium库对Github topics页面进行解析，并爬取指定topic下的所有库信息。这里以motion-generation这个主题为例。</p>
<p>首先，我们需要创建<code>WebDriver</code>，并获取指定页面的HTML。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">topic = <span class="string">&quot;motion-generation&quot;</span></span><br><span class="line">driver = webdriver.Chrome()</span><br><span class="line">topic_url = <span class="string">&#x27;https://github.com/topics/&#x27;</span>+topic</span><br><span class="line">driver.get(topic_url)</span><br></pre></td></tr></table></figure>

<p>接下来我们需要对指定页面的HTML进行分析，以提取出我们想要的内容。在浏览器中<code>F12</code>打开开发者工具可以看到网页源码，并且鼠标悬停在代码上方可以在页面上看到对应区域高亮。这里我们需要提取的是库名字、库url、作者名字、star数以及库的描述信息，以其中一个为例：</p>
<div aligh="center">
<img src="/asset/1/html_1.jpg" class="lazyload" data-srcset="/asset/1/html_1.jpg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/>
</div>

<p>可以观察到，所有文章项目都是在<code>//div[@class=&quot;col-md-8 col-lg-9&quot;]/article</code>便签下的，进一步点开可以看到各个项目所在标签。因此我们可以用下面的代码提取出所有的article便签及他们的各个属性。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Extract the articles</span></span><br><span class="line">articles_tags = driver.find_elements(By.XPATH, <span class="string">&#x27;//div[@class=&quot;col-md-8 col-lg-9&quot;]/article&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> article <span class="keyword">in</span> articles_tags:</span><br><span class="line">        topic_item = TopicItems()</span><br><span class="line">        <span class="comment"># 一定得加&#x27;.&#x27;和&#x27;descendant&#x27;，否则会从根节点开始找！！！</span></span><br><span class="line">        topic_item.repo_name = article.find_element(By.XPATH, <span class="string">&#x27;.//descendant::h3/a[@class=&quot;Link text-bold wb-break-word&quot;]&#x27;</span>).text</span><br><span class="line">        topic_item.repo_url = article.find_element(By.XPATH, <span class="string">&#x27;.//descendant::h3/a[@class=&quot;Link text-bold wb-break-word&quot;]&#x27;</span>).get_attribute(<span class="string">&#x27;href&#x27;</span>)</span><br><span class="line">        topic_item.author_name = article.find_element(By.XPATH, <span class="string">&#x27;.//descendant::h3/a[@class=&quot;Link&quot;]&#x27;</span>).text</span><br><span class="line">        topic_item.stars = article.find_element(By.XPATH, <span class="string">&#x27;.//descendant::span[@id=&quot;repo-stars-counter-star&quot;]&#x27;</span>).text</span><br><span class="line">        <span class="comment"># Some articles do not have description</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            topic_item.description = article.find_element(By.XPATH, <span class="string">&#x27;.//descendant::div[@class=&quot;px-3 pt-3&quot;]/p&#x27;</span>).text</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            topic_item.description = <span class="string">&#x27;&#x27;</span></span><br><span class="line">        results.append(topic_item)</span><br></pre></td></tr></table></figure>

<p>需要注意的是，这个页面使用AJAX来动态加载页面，当用户点击<code>Load more...</code>按钮时，AJAX会添加新的article到之前的article后面。因此我们可以先点击所有的<code>Load more...</code>，直到全部加载完之后再来抽取article标签。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        load_more_button = driver.find_element(By.XPATH, <span class="string">&#x27;//form[@class=&quot;ajax-pagination-form js-ajax-pagination&quot;]/button&#x27;</span>)</span><br><span class="line">        load_more_button.submit()</span><br><span class="line">        <span class="comment"># Wait for the next set of articles to load</span></span><br><span class="line">        WebDriverWait(driver, <span class="number">15</span>).until(</span><br><span class="line">            EC.presence_of_element_located((By.XPATH, <span class="string">&#x27;//div[@class=&quot;col-md-8 col-lg-9&quot;]/article&#x27;</span>))</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>需要注意的是，AJAX加载需要时间，因此得显式地等待article标签加载出来，否则会爬取的不够全。<br>最后将爬取到的条目保存到JSON文件中，大功告成！</p>
<div aligh="center">
<img src="/asset/1/json_1.png" class="lazyload" data-srcset="/asset/1/json_1.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/>
</div>

<p>完整的代码参见：<a href="https://github.com/LvXinTao/simple-scraper">simple-scraper</a>。<br><strong>参考资料</strong>：</p>
<ol>
<li><a href="http://www.zvon.org/comp/r/tut-XPath_1.html#intro">XPath Tutorial</a></li>
<li><a href="https://selenium-python.readthedocs.io/index.html">Selenium with Python</a></li>
</ol>
]]></content>
      <categories>
        <category>Coding</category>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>Selenium</tag>
        <tag>Crawler</tag>
      </tags>
  </entry>
  <entry>
    <title>Indigo的博客出生啦！</title>
    <url>/2024/08/01/Indigo%E7%9A%84%E5%8D%9A%E5%AE%A2%E5%87%BA%E7%94%9F%E5%95%A6%EF%BC%81/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>在经过一晚上的折腾后，终于在自己电脑上成功装上了Node.js和Hexo，并通过Github Page搭建起了自己的小站！</p>
<span id="more"></span>

<p>在此鸣谢</p>
<center><div class="tag link"><a class="link-card" title="Volantis主题" href="https://github.com/volantis-x/hexo-theme-volantis"><div class="left"><img src="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/256/safari.png" class="lazyload" data-srcset="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/256/safari.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="/></div><div class="right"><p class="text">Volantis主题</p><p class="url">https://github.com/volantis-x/hexo-theme-volantis</p></div></a></div></center>
<center><div class="tag link"><a class="link-card" title="Hexo框架" href="https://hexo.io/zh-cn/"><div class="left"><img src="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/256/safari.png" class="lazyload" data-srcset="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/256/safari.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="/></div><div class="right"><p class="text">Hexo框架</p><p class="url">https://hexo.io/zh-cn/</p></div></a></div></center>
<center><div class="tag link"><a class="link-card" title="强大的node.js、npm、nvm" href="https://nodejs.org/zh-cn"><div class="left"><img src="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/256/safari.png" class="lazyload" data-srcset="https://unpkg.com/volantis-static@0.0.1654736714924/media/org.volantis/logo/256/safari.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw=="/></div><div class="right"><p class="text">强大的node.js、npm、nvm</p><p class="url">https://nodejs.org/zh-cn</p></div></a></div></center>
它们的文档详尽且有效，使得前端小白能够无痛建站！
]]></content>
      <categories>
        <category>碎碎念</category>
      </categories>
      <tags>
        <tag>发疯</tag>
        <tag>爸爸妈妈我出生了</tag>
      </tags>
  </entry>
  <entry>
    <title>算法岗知识汇总【CV】</title>
    <url>/2024/09/24/%E7%AE%97%E6%B3%95%E5%B2%97%E7%9F%A5%E8%AF%86%E6%B1%87%E6%80%BB%E3%80%90CV%E3%80%91/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>正在更新中……</p>
<p>秋招在即，用这篇博客记录一下算法岗求职过程中的一些必备知识汇总。</p>
<span id="more"></span>

<h2 id="视觉Backbone"><a href="#视觉Backbone" class="headerlink" title="视觉Backbone"></a>视觉Backbone</h2><h3 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h3><ul>
<li><p>CNN的感受野</p>
<blockquote>
<p>定义是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小</p>
</blockquote>
</li>
<li><p>CNN的参数量计算</p>
<blockquote>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>×</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>×</mo><mi>K</mi><mo>×</mo><mi>K</mi><mo>+</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{in}\times C_{out}\times K\times K + C_{out}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">in</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight">u</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，最后一项是偏置项。
</blockquote>
</li>
<li><p>1x1卷积的作用，与Linear的区别</p>
<blockquote>
<ul>
<li>1x1卷积相当于是在通道维度上做Linear，它不改变特征图的空间信息，同时能够将跨通道的信息进行整合，对通道维度进行升维降维。</li>
<li>Linear操作的是一维向量，没有空间信息。</li>
</ul>
<p>将[B,C,H,W] reshape成[B,H*W,C]后过Linear，与直接在[B,C,H,W]上做1x1卷积效果相同。</p>
</blockquote>
</li>
<li><p>Numpy手搓卷积</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">filter2d</span>(<span class="params">image, kernel</span>):</span><br><span class="line">    <span class="comment"># 获取输入图像和卷积核的维度</span></span><br><span class="line">    image_height, image_width = image.shape</span><br><span class="line">    kernel_height, kernel_width = kernel.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算滤波结果的维度</span></span><br><span class="line">    output_height = image_height - kernel_height + <span class="number">1</span></span><br><span class="line">    output_width = image_width - kernel_width + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化滤波结果数组</span></span><br><span class="line">    filtered_image = np.zeros((output_height, output_width))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 进行滤波操作（实际上是二维卷积操作）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(output_height):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(output_width):</span><br><span class="line">            <span class="comment"># 提取当前窗口的像素值</span></span><br><span class="line">            window = image[i:i+kernel_height, j:j+kernel_width]</span><br><span class="line">            <span class="comment"># 计算当前位置的卷积和</span></span><br><span class="line">            filtered_image[i, j] = np.<span class="built_in">sum</span>(window * kernel)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> filtered_image</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个图像和卷积核（滤波器）</span></span><br><span class="line">image = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                  [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">kernel = np.array([[<span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">0</span>, -<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 应用滤波器</span></span><br><span class="line">filtered_image = filter2d(image, kernel)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(filtered_image)</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="ViT"><a href="#ViT" class="headerlink" title="ViT"></a>ViT</h3><ul>
<li>ViT的结构描述<blockquote>
<div aligh="center"><img src="/asset/2/vit.png" class="lazyload" data-srcset="/asset/2/vit.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
<div aligh="center"><img src="/asset/2/vit_2.png" class="lazyload" data-srcset="/asset/2/vit_2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div></blockquote>
</li>
</ul>
<blockquote>
<ul>
<li>将图片patchify成P*P的patch，共N个patch。将每个patch进行flatten之后过一层线性层之后得到embedding。</li>
<li>与BERT类似，在patched embedding序列开头附加一个可学习的[class] token，来表示整个图片representation。</li>
<li>使用的位置编码为learnable 1D position embedding。</li>
<li>整体的结构为Transformer Encoder。</li>
<li>最后将[class] token的embedding过分类头。</li>
<li>微调时通常会使用更高分辨率，此时保持patch size不变，这样sequence length会变大，position embedding会不够用。文中采取的做法是进行2D插值拟合得到position embedding。</li>
</ul>
</blockquote>
<ul>
<li>手撕ViT的patchify<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PatchEmbed</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    2D Image to Patch Embedding</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img_size=<span class="number">224</span>, patch_size=<span class="number">16</span>, in_c=<span class="number">3</span>, embed_dim=<span class="number">768</span>, norm_layer=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        img_size = (img_size, img_size)</span><br><span class="line">        patch_size = (patch_size, patch_size)</span><br><span class="line">        <span class="variable language_">self</span>.img_size = img_size</span><br><span class="line">        <span class="variable language_">self</span>.patch_size = patch_size</span><br><span class="line">        <span class="variable language_">self</span>.grid_size = (img_size[<span class="number">0</span>] // patch_size[<span class="number">0</span>], img_size[<span class="number">1</span>] // patch_size[<span class="number">1</span>])</span><br><span class="line">        <span class="variable language_">self</span>.num_patches = <span class="variable language_">self</span>.grid_size[<span class="number">0</span>] * <span class="variable language_">self</span>.grid_size[<span class="number">1</span>]</span><br><span class="line"> </span><br><span class="line">        <span class="variable language_">self</span>.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)</span><br><span class="line">        <span class="variable language_">self</span>.norm = norm_layer(embed_dim) <span class="keyword">if</span> norm_layer <span class="keyword">else</span> nn.Identity()</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, C, H, W = x.shape</span><br><span class="line">        <span class="keyword">assert</span> H == <span class="variable language_">self</span>.img_size[<span class="number">0</span>] <span class="keyword">and</span> W == <span class="variable language_">self</span>.img_size[<span class="number">1</span>], \</span><br><span class="line">            <span class="string">f&quot;Input image size (<span class="subst">&#123;H&#125;</span>*<span class="subst">&#123;W&#125;</span>) doesn&#x27;t match model (<span class="subst">&#123;self.img_size[<span class="number">0</span>]&#125;</span>*<span class="subst">&#123;self.img_size[<span class="number">1</span>]&#125;</span>).&quot;</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># flatten: [B, C, H, W] -&gt; [B, C, HW]</span></span><br><span class="line">        <span class="comment"># transpose: [B, C, HW] -&gt; [B, HW, C]</span></span><br><span class="line">        x = <span class="variable language_">self</span>.proj(x).flatten(<span class="number">2</span>).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="目标检测"><a href="#目标检测" class="headerlink" title="目标检测"></a>目标检测</h2><ul>
<li><p>IOU计算及手写</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bbox_iou</span>(<span class="params">box1, box2</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Calculate the Intersection over Union (IoU) of two bounding boxes.</span></span><br><span class="line"><span class="string">    :param box1: (x1, y1, x2, y2) - coordinates of the first bounding box</span></span><br><span class="line"><span class="string">    :param box2: (x1, y1, x2, y2) - coordinates of the second bounding box</span></span><br><span class="line"><span class="string">    :return: IoU of the two bounding boxes</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Determine the coordinates of the intersection rectangle</span></span><br><span class="line">    x1_inter = <span class="built_in">max</span>(box1[<span class="number">0</span>], box2[<span class="number">0</span>])</span><br><span class="line">    y1_inter = <span class="built_in">max</span>(box1[<span class="number">1</span>], box2[<span class="number">1</span>])</span><br><span class="line">    x2_inter = <span class="built_in">min</span>(box1[<span class="number">2</span>], box2[<span class="number">2</span>])</span><br><span class="line">    y2_inter = <span class="built_in">min</span>(box1[<span class="number">3</span>], box2[<span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the area of intersection</span></span><br><span class="line">    width_inter = <span class="built_in">max</span>(<span class="number">0</span>, x2_inter - x1_inter)</span><br><span class="line">    height_inter = <span class="built_in">max</span>(<span class="number">0</span>, y2_inter - y1_inter)</span><br><span class="line">    area_inter = width_inter * height_inter</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the area of both bounding boxes</span></span><br><span class="line">    area_box1 = (box1[<span class="number">2</span>] - box1[<span class="number">0</span>]) * (box1[<span class="number">3</span>] - box1[<span class="number">1</span>])</span><br><span class="line">    area_box2 = (box2[<span class="number">2</span>] - box2[<span class="number">0</span>]) * (box2[<span class="number">3</span>] - box2[<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the intersection over union by taking the intersection</span></span><br><span class="line">    <span class="comment"># area and dividing it by the sum of both areas minus the intersection area</span></span><br><span class="line">    iou = area_inter / <span class="built_in">float</span>(area_box1 + area_box2 - area_inter)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> iou</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage:</span></span><br><span class="line">box1 = (<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">box2 = (<span class="number">2</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">iou = bbox_iou(box1, box2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;IoU: <span class="subst">&#123;iou&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>NMS描述及手写</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">nms</span>(<span class="params">boxes, scores, iou_threshold</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Perform Non-Maximum Suppression (NMS) on the given bounding boxes.</span></span><br><span class="line"><span class="string">    :param boxes: a list of bounding boxes (each box is [x1, y1, x2, y2])</span></span><br><span class="line"><span class="string">    :param scores: a list of scores for each bounding box</span></span><br><span class="line"><span class="string">    :param iou_threshold: a float representing the IoU threshold for NMS</span></span><br><span class="line"><span class="string">    :return: a list of indices of the selected bounding boxes</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># Get the indices of the boxes sorted by their scores in descending order</span></span><br><span class="line">    indices = <span class="built_in">sorted</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(scores)), key=<span class="keyword">lambda</span> i: scores[i], reverse=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># List to hold the indices of the selected boxes</span></span><br><span class="line">    selected_indices = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> indices:</span><br><span class="line">        <span class="comment"># Pick the box with the highest score and add its index to the list of selected indices</span></span><br><span class="line">        current_index = indices[<span class="number">0</span>]</span><br><span class="line">        selected_indices.append(current_index)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the IoU of the selected box with the rest of the boxes</span></span><br><span class="line">        ious = [bbox_iou(boxes[current_index], boxes[i]) <span class="keyword">for</span> i <span class="keyword">in</span> indices[<span class="number">1</span>:]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Remove the indices of the boxes that have a high IoU with the selected box</span></span><br><span class="line">        indices = [i <span class="keyword">for</span> i, io <span class="keyword">in</span> <span class="built_in">zip</span>(indices[<span class="number">1</span>:], ious) <span class="keyword">if</span> io &lt; iou_threshold]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> selected_indices</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example usage:</span></span><br><span class="line">boxes = [</span><br><span class="line">    [<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>],</span><br><span class="line">    [<span class="number">5</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>],</span><br><span class="line">    [<span class="number">10</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">12</span>]</span><br><span class="line">]</span><br><span class="line">scores = [<span class="number">0.9</span>, <span class="number">0.75</span>, <span class="number">0.6</span>, <span class="number">0.95</span>]</span><br><span class="line">iou_threshold = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">selected_boxes = nms(boxes, scores, iou_threshold)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Selected boxes: <span class="subst">&#123;selected_boxes&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h3><ul>
<li><p>Focal loss解决的问题</p>
<blockquote>
<p>类别样本不均衡或Hard examples学习不好。<br>普通CE对Well-classified的sample的loss依旧很大，并且通常这些sample很多(background)，这导致模型对hard example的梯度反传较小。</p>
</blockquote>
</li>
<li><p>Focal loss公式</p>
<blockquote>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>L</mi><mo stretchy="false">(</mo><msub><mi>p</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><msub><mi>α</mi><mi>t</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>t</mi></msub><msup><mo stretchy="false">)</mo><mi>γ</mi></msup><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><msub><mi>p</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">FL(p_t) = -\alpha_t(1-p_t)^\gamma log(p_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">F</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">−</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6644em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05556em;">γ</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>
<p>通常,<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span>取2时效果好;<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>在正样本中取0.25，负样本中取0.75。</p>
</blockquote>
</li>
<li><p>手撕Focal loss</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FocalLoss</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, gamma=<span class="number">0</span>, alpha=<span class="literal">None</span>, size_average=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(FocalLoss, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.gamma = gamma</span><br><span class="line">        <span class="variable language_">self</span>.alpha = alpha</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(alpha,(<span class="built_in">float</span>,<span class="built_in">int</span>,long)): <span class="variable language_">self</span>.alpha = torch.Tensor([alpha,<span class="number">1</span>-alpha])</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(alpha,<span class="built_in">list</span>): <span class="variable language_">self</span>.alpha = torch.Tensor(alpha)</span><br><span class="line">        <span class="variable language_">self</span>.size_average = size_average</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span>, target</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">input</span>.dim()&gt;<span class="number">2</span>:</span><br><span class="line">            <span class="built_in">input</span> = <span class="built_in">input</span>.view(<span class="built_in">input</span>.size(<span class="number">0</span>),<span class="built_in">input</span>.size(<span class="number">1</span>),-<span class="number">1</span>)  <span class="comment"># N,C,H,W =&gt; N,C,H*W</span></span><br><span class="line">            <span class="built_in">input</span> = <span class="built_in">input</span>.transpose(<span class="number">1</span>,<span class="number">2</span>)    <span class="comment"># N,C,H*W =&gt; N,H*W,C</span></span><br><span class="line">            <span class="built_in">input</span> = <span class="built_in">input</span>.contiguous().view(-<span class="number">1</span>,<span class="built_in">input</span>.size(<span class="number">2</span>))   <span class="comment"># N,H*W,C =&gt; N*H*W,C</span></span><br><span class="line">        target = target.view(-<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        logpt = F.log_softmax(<span class="built_in">input</span>)</span><br><span class="line">        logpt = logpt.gather(<span class="number">1</span>,target)</span><br><span class="line">        logpt = logpt.view(-<span class="number">1</span>)</span><br><span class="line">        pt = Variable(logpt.data.exp())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.alpha <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.alpha.<span class="built_in">type</span>()!=<span class="built_in">input</span>.data.<span class="built_in">type</span>():</span><br><span class="line">                <span class="variable language_">self</span>.alpha = <span class="variable language_">self</span>.alpha.type_as(<span class="built_in">input</span>.data)</span><br><span class="line">            at = <span class="variable language_">self</span>.alpha.gather(<span class="number">0</span>,target.data.view(-<span class="number">1</span>))</span><br><span class="line">            logpt = logpt * Variable(at)</span><br><span class="line"></span><br><span class="line">        loss = -<span class="number">1</span> * (<span class="number">1</span>-pt)**<span class="variable language_">self</span>.gamma * logpt</span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.size_average: <span class="keyword">return</span> loss.mean()</span><br><span class="line">        <span class="keyword">else</span>: <span class="keyword">return</span> loss.<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="语义分割"><a href="#语义分割" class="headerlink" title="语义分割"></a>语义分割</h2><h3 id="UNet"><a href="#UNet" class="headerlink" title="UNet"></a>UNet</h3><h3 id="SAM"><a href="#SAM" class="headerlink" title="SAM"></a>SAM</h3><h2 id="自监督预训练"><a href="#自监督预训练" class="headerlink" title="自监督预训练"></a>自监督预训练</h2><h3 id="DINO"><a href="#DINO" class="headerlink" title="DINO"></a>DINO</h3><ul>
<li><p>DINO的特性是什么？</p>
<blockquote>
<p>Self-distillation with no label.</p>
<ul>
<li>通过完全自监督的学习方式，ViT学得了图片的语义分割的信息！</li>
<li>对提取出的feature仅使用KNN分类器，就能在ImageNet上达到78.3%的top1准确率。</li>
</ul>
</blockquote>
</li>
<li><p>DINO的自监督学习方式</p>
<blockquote>
<div aligh="center"><img src="/asset/2/dino.png" class="lazyload" data-srcset="/asset/2/dino.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<div aligh="center"><img src="/asset/2/dino_code.png" class="lazyload" data-srcset="/asset/2/dino_code.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>包含teacher和student网络，两部分是完全同样的结构：ViT（patch 8或16）后接一层linear，输出K维的logit，然后经过一个带temperature的softmax。最终的优化目标是两部分分布的交叉熵。</p>
<p>对于teacher和student的输入不同：首先会将一张图片进行两种不同的RandomResizedCrop，得到两个分辨率较大的global view；然后会使用multi-crop得到多个较小分辨率的local views。<br>然后将local and global views输入给student，global views给teacher，然后分别计算每一对之间的交叉熵损失。这样可以鼓励模型学到local-to-global的对应关系。</p>
<p>对于teacher网络的更新，其不使用梯度更新（stop gradient），而是使用EMA（指数移动平均）的方式使用student的权重来更新。</p>
<p>为了避免collape（K维的feature集中在某一维或均匀分布在K维），对teacher网络的输出采取了centering和sharpening的策略。</p>
<ul>
<li>对于centering，会使用teacher输出的EMA维护一个center值，计算softmax值时减去该center。这样可以避免K维feature集中在某一维。</li>
<li>对于sharpening，实现方式是在softmax中使用一个较小的temperature，这样可以使得softmax之后的分布更加尖锐。<br>两种策略同时使用，会互相制衡，最后避免collape。</li>
</ul>
</blockquote>
</li>
<li><p>DINO中softmax的temperature的作用</p>
<blockquote>
<p>较小的temperature会将输出数值放大，差异更大，从而输出的softmax分布更加尖锐。</p>
<p>较大的temperature会使得softmax分布更加平滑。</p>
</blockquote>
</li>
<li><p>DINO中应用于下游任务中是哪个网络？</p>
<blockquote>
<p>学生网络负责从输入的图像中学习特征表示，而教师网络则提供一个稳定的目标，帮助学生网络学习。具体来说，学生网络接收所有预处理过的图像裁剪（包括局部和全局视图），而教师网络仅接收全局视图的裁剪图。学生网络的输出会模仿教师网络的输出，通过这种方式，学生网络可以学习到更好的特征表示.<br>因此在下游任务中使用的是学生网络。</p>
</blockquote>
</li>
</ul>
<h3 id="MAE"><a href="#MAE" class="headerlink" title="MAE"></a>MAE</h3><ul>
<li><p>MAE的主要特点</p>
<blockquote>
<ul>
<li>非对称的Encoder-Decoder结构：Encoder只encode没有被mask掉的tokens，而Decoder需要decode所有token。</li>
<li>极大的mask比例（75%）：相比于BERT的15%掩码概率，MAE使用了75%的掩码概率。这样的好处是既能增大task的难度，又能极大减少encoder的计算量。</li>
</ul>
</blockquote>
</li>
<li><p>MAE的模型结构与训练pipeline</p>
<blockquote>
<div aligh="center"><img src="/asset/2/mae.png" class="lazyload" data-srcset="/asset/2/mae.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ul>
<li>首先会将图片切成patch（一般是16 * 16），然后取75%的patch进行mask。</li>
<li>MAE Encoder：是ViT，但是输入的只有unmasked patches，被mask的patch不输入。</li>
<li>MAE Decoder：是ViT，输入的是完整的token 列表，包括：编码后的visible patches、masked patches。每一个mask token是一个共享的可学习的向量，同时会添加PE为mask token注入位置信息。（下游任务时，decoder不使用，因此decoder与encoder的结构可以是解耦的）</li>
<li>Linear：decoder出来的vector会过一个linear将维度转换到P * P * C，对应的是patch内的像素值。</li>
<li>Loss：使用MSE loss对mask的patch的像素值进行计算。与BERT相同，只对mask掉的部分进行loss计算。同时作者还实验了一种变种，预测一个patch内的normalized pixel value，这样可以涨点。</li>
</ul>
</blockquote>
</li>
<li><p>MAE为什么能很好地学到视觉特征？</p>
<blockquote>
<ul>
<li>mask比例很高（75%），对于encoder是个非常有挑战性的pretext task。</li>
<li>被mask掉的部分通过周边可见的patch来恢复信息，这导致了encoder学得的视觉特征本身就得包含语义信息。</li>
</ul>
</blockquote>
</li>
<li><p>MAE的encoder和decoder为什么是非对称的结构？</p>
<blockquote>
<ul>
<li>这是因为上下游任务的gap。在BERT中，预训练的输入中是包含mask的，这与下游微调任务的输入存在gap。因此在MAE中，为了与下游任务的输入对齐，在Encoder的输入中也不引入mask。</li>
<li>同时，encoder输入中只输入25%的patch，对模型训练速度大大提升（三倍以上）。</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="BEiT"><a href="#BEiT" class="headerlink" title="BEiT"></a>BEiT</h3><h3 id="MoCo-v1-v2-v3"><a href="#MoCo-v1-v2-v3" class="headerlink" title="MoCo v1&#x2F;v2&#x2F;v3"></a>MoCo v1&#x2F;v2&#x2F;v3</h3><h3 id="SimCLR"><a href="#SimCLR" class="headerlink" title="SimCLR"></a>SimCLR</h3><h3 id="SimMIM"><a href="#SimMIM" class="headerlink" title="SimMIM"></a>SimMIM</h3>]]></content>
      <categories>
        <category>算法岗</category>
        <category>八股文</category>
      </categories>
      <tags>
        <tag>算法岗</tag>
        <tag>八股文</tag>
      </tags>
  </entry>
  <entry>
    <title>算法岗知识汇总【机器学习】</title>
    <url>/2024/09/24/%E7%AE%97%E6%B3%95%E5%B2%97%E7%9F%A5%E8%AF%86%E6%B1%87%E6%80%BB%E3%80%90%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%91/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><ul>
<li><p>常用损失函数</p>
<blockquote>
<p>均方误差（MSE,L2）:<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mo stretchy="false">(</mo><mi>y</mi><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">L=(y-\hat{y})^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span> （回归任务常用，隐含的预设是数据误差符合高斯分布）<br>绝对误差（MAE,L1）:<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mi mathvariant="normal">∣</mi><mi>y</mi><mo>−</mo><mover accent="true"><mi>y</mi><mo>^</mo></mover><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">L=|y-\hat{y}|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.1944em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1944em;"><span></span></span></span></span></span><span class="mord">∣</span></span></span></span><br>二值交叉熵（BCE）：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mo>∑</mo><mi>i</mi></msub><mo>−</mo><mo stretchy="false">[</mo><msub><mi>y</mi><mi>i</mi></msub><mo>∗</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>∗</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">L=\frac{1}{N}\sum_i -[ y_i* log(p_i)+(1-y_i)*log(1-p_i)]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)]</span></span></span></span>;<br>交叉熵（CE）：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msub><mo>∑</mo><mi>i</mi></msub><msubsup><mo>∑</mo><mrow><mi>c</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mo>−</mo><msub><mi>y</mi><mrow><mi>i</mi><mi>c</mi></mrow></msub><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><msub><mi>p</mi><mrow><mi>i</mi><mi>c</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L=\frac{1}{N}\sum_i\sum\limits_{c=1}^M -y_{ic}log(p_{ic})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.4954em;vertical-align:-0.9671em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.162em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5283em;"><span style="top:-2.1329em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">∑</span></span></span><span style="top:-3.95em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9671em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">−</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>;（倾向于任务数据接近多项式分布）</p>
</blockquote>
</li>
<li><p>Cross Entropy的推导过程</p>
<blockquote>
<p>KL(p,q) &#x3D; H(p,q) - H(p)，因此优化交叉熵实际上是优化p和q分布之间的KL散度。</p>
</blockquote>
</li>
<li><p>回归任务中MAE和MSE的理解和区别</p>
<blockquote>
<p>MAE：</p>
<ul>
<li>优点：</li>
</ul>
<ol>
<li>直观易懂：MAE直接反映了平均预测误差，单位与原始数据一致，易于解释。</li>
<li>对异常值不敏感：由于没有平方运算，MAE对异常值的影响较小，更加稳健。</li>
<li>优化稳定：MAE损失函数的梯度平滑，使得优化算法收敛更加稳定</li>
</ol>
<ul>
<li>缺点:</li>
</ul>
<ol>
<li>对大误差不敏感：MAE对较大的误差没有特别的惩罚，因此在某些需要更严格控制大误差的应用中可能不适用。</li>
<li>不可微性：MAE在零点处不可微，这在一些优化算法中可能会引起问题，尽管通过技术手段可以缓解这一问题。</li>
</ol>
</blockquote>
</li>
</ul>
<blockquote>
<p>MSE:</p>
<ul>
<li>优点：</li>
</ul>
<ol>
<li>对大误差敏感：MSE通过平方项放大了大误差的影响，适用于需要严格控制大误差的应用</li>
<li>数学性质好：MSE的二次损失函数具有良好的数学性质，便于推导和计算，特别是在最小二乘法中应用广泛</li>
</ol>
<ul>
<li>缺点：</li>
</ul>
<ol>
<li>异常值影响大：由于对大误差的敏感性，异常值会显著影响MSE，使得模型对异常值过度关注。</li>
<li>解释性差：MSE的单位是原始数据单位的平方，不直观，需要转化为RMSE来辅助解释。</li>
</ol>
</blockquote>
<ul>
<li>L1和L2正则化的区别？它们都能防止过拟合吗？<blockquote>
<p>L1正则化：通过在损失函数中添加权重的L1范数（权重向量的绝对值之和）作为惩罚项。这种正则化倾向于产生稀疏权重矩阵，即将一些权重推向零，从而实现特征选择的效果。L1正则化有助于在众多特征中选择最重要的特征，减少模型的复杂度，并且提高模型的泛化能力。L1正则化的优化是非凸的，这可能导致局部最优解，但它的稀疏性使得模型更易于理解和解释。（更适合于特征选择）</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>L2正则化：L2正则化会使权重值变得较小，但不会直接导致权重稀疏，因此不具有特征选择的作用。L2正则化有助于控制模型的复杂度，防止过拟合，同时保持模型的平滑性。它对离群值更加稳健。L2正则化的优化是凸的，这意味着它总是能够找到全局最优解。（更适合保持模型参数的平滑性）</p>
</blockquote>
<ul>
<li><p>交叉熵伪代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">x</span>):</span><br><span class="line">	exps = np.exp(x - np.<span class="built_in">max</span>(x)) <span class="comment"># 防止上溢</span></span><br><span class="line">    <span class="keyword">return</span> exps / np.<span class="built_in">sum</span>(exps)</span><br><span class="line">	</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy_error</span>(<span class="params">p,y</span>):</span><br><span class="line">    delta=<span class="number">1e-7</span>       <span class="comment">#添加一个微小值可以防止负无限大(np.log(0))的发生。</span></span><br><span class="line">	p = softmax(p)   <span class="comment"># 通过 softmax 变为概率分布，并且sum(p) = 1</span></span><br><span class="line">    <span class="keyword">return</span> -np.<span class="built_in">sum</span>(y*np.log(p+delta))</span><br></pre></td></tr></table></figure></li>
<li><p>分类任务为什么常用交叉熵而不是MSE？</p>
<blockquote>
<ul>
<li>问题本质：交叉熵损失函数是为分类问题设计的，而均方误差是为回归问题设计的。分类问题的目标是预测一个离散的标签，而回归问题的目标是预测一个连续的值。交叉熵直接衡量的是预测概率分布与真实分布之间的差异。</li>
<li>梯度大小：交叉熵损失函数的梯度在预测错误时相对较大，这有助于模型在训练初期快速学习。而MSE的梯度随着预测值接近真实值而减小，这可能导致训练过程在后期变得缓慢。</li>
<li>优化问题：MSE在分类问题中是一个非凸优化，可能收敛到局部最优值；而交叉熵是凸优化，能够收敛到全局最优值。</li>
<li>归一化：交叉熵损失函数对类别进行了归一化处理，这意味着不同类别的预测误差对损失的贡献是相等的。而MSE可能会偏向于数值较大的类别。</li>
</ul>
</blockquote>
</li>
<li><p>如何选择损失函数？</p>
<blockquote>
<p>需要出于对数据分布的假设，不同的loss隐式地对数据分布有要求。例如L2隐含的是数据误差符合高斯分布。</p>
</blockquote>
</li>
</ul>
<h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><ul>
<li><p>TP,FP,TN,FN</p>
<blockquote>
<p>TP（True Positive）: 预测为正，实际为正<br>FP（False Positive）: 预测为正，实际为负<br>TN（True Negative）：预测为负，实际为负<br>FN（false negative）: 预测为负，实际为正</p>
</blockquote>
</li>
<li><p>Accuracy,Precision,Recall,F-Score</p>
<blockquote>
<p>Accuracy &#x3D; (TP+TN)&#x2F;N<br>Precision &#x3D; TP&#x2F;(TP+FP)<br>Recall &#x3D; TP&#x2F;(TP+FN)<br>F1-Score &#x3D; (2* Precision * Recall)&#x2F;(Precision+Recall)<br>上述指标用于评估二分类问题。</p>
</blockquote>
</li>
<li><p>P-R曲线</p>
<blockquote>
<p>P-R曲线：横轴是Recall，纵轴是Precision。</p>
</blockquote>
</li>
<li><p>ROC曲线</p>
<blockquote>
<p>ROC曲线：横轴是FPR&#x3D;FP&#x2F;(FP+TN),表示所有真实为负的样本中，被错误预测为正的比例；纵轴是TPR&#x3D;TP&#x2F;(TP+FN)，表示所有真实为正的样本中，被正确预测为正的比例。<br>如何画ROC曲线？</p>
<p>假设我们预测出（z,0.08）,(a,0.1),(b,0.2),(c,0.4),(d,0.5)，然后我们在预测结果中选择每一个样本的分值作为阈值，比如第一个数据（a,0.1）则分值大于等于0.1的都为正样本，小于0.1的为负样本，然后便根据这些样本算出一组FPR,与TPR值，得到ROC曲线上的一点，对所有测试用例做一遍操作，便可以绘制得到ROC曲线图。</p>
</blockquote>
</li>
<li><p>AUC的定义</p>
<blockquote>
<p>AUC(Area Under Curve)：</p>
<ul>
<li>第一个定义：ROC曲线下的面积大小，AUC越大，说明分类器越可能把真正的正样本排在前面，分类性能越好。</li>
<li>第二个定义：正样本被预测成正样本(TP)的得分大于负样本被预测成正样本(FP)的得分的概率。</li>
</ul>
</blockquote>
</li>
<li><p>AUC的计算方法</p>
<blockquote>
<ul>
<li>ROC曲线方法：</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">roc_point = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">        i = pred_prob[i]</span><br><span class="line">        TP = <span class="number">0</span>  <span class="comment"># 真阳样本数</span></span><br><span class="line">        FP = <span class="number">0</span>  <span class="comment"># 假阳样本数</span></span><br><span class="line">        TP_rate = <span class="number">0.</span>  <span class="comment"># 真阳率</span></span><br><span class="line">        FP_rate = <span class="number">0.</span>  <span class="comment"># 假阳率</span></span><br><span class="line">        pos_num = <span class="number">0</span>   <span class="comment"># 预测真样本数</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计数过程</span></span><br><span class="line">        <span class="keyword">for</span> ind, prob <span class="keyword">in</span> <span class="built_in">enumerate</span>(pred_prob):</span><br><span class="line">            <span class="keyword">if</span> prob&gt;i:</span><br><span class="line">                pos_num += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> prob&gt;i <span class="keyword">and</span> labels[ind]&gt;<span class="number">0.5</span>:</span><br><span class="line">                TP+=<span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> prob&gt;i <span class="keyword">and</span> labels[ind]&lt;<span class="number">0.5</span>:</span><br><span class="line">                FP+=<span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> pos_num!=<span class="number">0</span>:</span><br><span class="line">            TP_rate = TP / <span class="built_in">sum</span>(labels)</span><br><span class="line">            FP_rate = FP / (num-<span class="built_in">sum</span>(labels))</span><br><span class="line">        roc_point.append([FP_rate, TP_rate])  <span class="comment"># 记录ROC中的点</span></span><br><span class="line">    <span class="comment"># 画出ROC曲线</span></span><br><span class="line">    roc_point.sort(key=<span class="keyword">lambda</span> x: x[<span class="number">0</span>])</span><br><span class="line">    plt.plot(np.array(roc_point)[<span class="number">1</span>:, <span class="number">0</span>], np.array(roc_point)[<span class="number">1</span>: ,<span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">&quot;FPR&quot;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&quot;TPR&quot;</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算每个小长方形的面积，求和即为auc</span></span><br><span class="line">    lastx = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> roc_point:</span><br><span class="line">        auc1 += (x-lastx)*y  <span class="comment"># 底乘高</span></span><br><span class="line">        lastx = x</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;方法一 auc:&quot;</span>, auc1)</span><br></pre></td></tr></table></figure>
<blockquote>
<ul>
<li>排序方法<div aligh="center"><img src="/asset/2/auc.jpeg" class="lazyload" data-srcset="/asset/2/auc.jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></li>
</ul>
<p>其中\sum{rank}为正样本序号之和，M为正样本数，N为负样本数。（正样本这里有个特殊情况在于当正负样本概率相等时，可能排在前面也可能在后面，如果单纯将正样本排在前面或者后面计算，计算结果与前面的会有偏差。可以将两者样本序号取平均）</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 时间复杂度为O(nlogn)</span></span><br><span class="line">    new_data = [[p, l] <span class="keyword">for</span> p, l <span class="keyword">in</span> <span class="built_in">zip</span>(pred_prob, labels)]</span><br><span class="line">    new_data.sort(key=<span class="keyword">lambda</span> x:x[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求正样本rank之和</span></span><br><span class="line">    rank_sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> ind, [prob,label] <span class="keyword">in</span> <span class="built_in">enumerate</span>(new_data):</span><br><span class="line">        <span class="keyword">if</span> label&gt;<span class="number">0.5</span>:</span><br><span class="line">            rank_sum+=ind</span><br><span class="line">    auc3 = (rank_sum - <span class="built_in">len</span>(P_ind)*(<span class="number">1</span>+<span class="built_in">len</span>(P_ind))/<span class="number">2</span>) / (<span class="built_in">len</span>(P_ind)*<span class="built_in">len</span>(F_ind))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;方法二 auc:&quot;</span>, auc3)</span><br></pre></td></tr></table></figure>
</li>
<li><p>解释AUC的定义，它解决了什么问题，优缺点是什么</p>
</li>
</ul>
<blockquote>
<p>当AUC&#x3D;1时，分类器能够正确区分所有的正类点和负类点；当AUC&#x3D;0.5时，分类器无法区分正类点和负类点，即相当于随机猜测。<br>AUC的优点包括：</p>
<ul>
<li>AUC本身与模型预测score绝对值无关，只关注排序效果，衡量排序能力，适合排序类任务。</li>
<li>对正负样本均衡不敏感，在样本不均衡情况下也能够合理评估。</li>
<li>不需要手动设置阈值，是一种整体上的衡量方法。</li>
</ul>
<p>然而，AUC也有一些缺点：</p>
<ul>
<li>忽略了预测的概率值和模型的拟合程度。</li>
<li>反应信息较为笼统，无法反映召回率、精确率等实际业务关心指标。</li>
<li>没有给出模型误差的空间分布信息，只关心正负样本之间的排序，并不关心正负样本内部的排序，难以衡量样本对于实际程度的刻画能力。</li>
</ul>
</blockquote>
<ul>
<li>多分类问题评价指标<blockquote>
<ul>
<li>micro-F1<br>取值范围：（0，1）<br>权重倾向：每一个样本的权重都相同<br>适用环境：多分类不平衡，若数据极度不平衡会影响结果。<br>计算方式:<div aligh="center"><img src="/asset/2/micro_f1.png" class="lazyload" data-srcset="/asset/2/micro_f1.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></li>
</ul>
<p>由micro-F1计算方式可以得知，</p>
<div aligh="center"><img src="/asset/2/micro_f1_2.png" class="lazyload" data-srcset="/asset/2/micro_f1_2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></blockquote>
</li>
</ul>
<blockquote>
<ul>
<li>macro-F1<br>取值范围：（0,1）<br>权重倾向：每一类别的权重都相同；<br>适用环境：多分类问题，不受数据不平衡影响，容易受到高识别性（高recall，高precision）的类别影响。<br>计算方式：<div aligh="center"><img src="/asset/2/macro_f1.png" class="lazyload" data-srcset="/asset/2/macro_f1.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>weighted-F1<br>macro-F1的变种，将F1-score乘以该类的比例之后相加。</li>
</ul>
</blockquote>
<h2 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h2><blockquote>
<p>逻辑回归是一种二分类算法，目的是预测二元输出变量（比如0和1）。</p>
<p>其使用一个参数化函数计算给定输入变量的输出概率。</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>z</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>z</mi></mrow></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">g(z)=\frac{1}{1+e^{-z}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2484em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7027em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.04398em;">z</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，z是输入变量的线性组合，可以表示为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi><mo>=</mo><mi>b</mi><mo>+</mo><msub><mi>w</mi><mn>1</mn></msub><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><msub><mi>w</mi><mn>2</mn></msub><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo>+</mo><msub><mi>w</mi><mi>n</mi></msub><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">z=b+w_1x_1+w_2x_2+...+w_nx_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">...</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>

<p>其训练的损失函数为最大化对数似然函数，这里即BCE。</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>∑</mo><mrow><msub><mi>y</mi><mi>i</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mtext> </mtext><mi>g</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>y</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>g</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mrow><annotation encoding="application/x-tex">J(w) = -\frac{1}{n}\sum{y_i log\ g(z_i)+(1-y_i)log(1-g(z_i))}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.1901em;vertical-align:-0.345em;"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mspace"> </span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">))</span></span></span></span></span>

<p>训练完成后，我们可以使用模型来预测新的样本的类别标签。预测类别标签的方法是，将新样本的特征向量代入 sigmoid 函数，计算输出概率，如果概率大于0.5，则预测为正例，否则预测为负例。</p>
</blockquote>
<blockquote>
<p>Python实现如下：</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.</span>/(<span class="number">1</span>+np.exp(-x))</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegression</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, learning_rate=<span class="number">0.003</span>, iterations=<span class="number">100</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.learning_rate = learning_rate <span class="comment"># 学习率</span></span><br><span class="line">        <span class="variable language_">self</span>.iterations = iterations <span class="comment"># 迭代次数</span></span><br><span class="line">        <span class="variable language_">self</span>.weights = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.bias = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, X, y</span>):</span><br><span class="line">        <span class="comment"># X [N,d] , y [N]</span></span><br><span class="line">        <span class="comment"># 初始化参数</span></span><br><span class="line">        <span class="variable language_">self</span>.weights = np.random.randn(X.shape[<span class="number">1</span>])</span><br><span class="line">        <span class="variable language_">self</span>.bias = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 梯度下降</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="variable language_">self</span>.iterations):</span><br><span class="line">            <span class="comment"># 计算sigmoid函数的预测值, y_hat = w * x + b</span></span><br><span class="line">            y_hat = sigmoid(np.dot(X, <span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.bias)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 计算损失函数</span></span><br><span class="line">             loss = -np.mean(y*np.log(y_hat)+(<span class="number">1</span>-y)*np.log(<span class="number">1</span>-y_hat))</span><br><span class="line">            <span class="comment"># 计算梯度</span></span><br><span class="line">            dw = np.dot(X.T,(y_hat-y)) / X.shape[<span class="number">0</span>]</span><br><span class="line">            db = np.mean(y_hat-y)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 更新参数</span></span><br><span class="line">            <span class="variable language_">self</span>.weights -= <span class="variable language_">self</span>.learning_rate * dw</span><br><span class="line">            <span class="variable language_">self</span>.bias -= <span class="variable language_">self</span>.learning_rate * db</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 打印损失函数值</span></span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Loss after iteration <span class="subst">&#123;i&#125;</span>: <span class="subst">&#123;loss&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self, X</span>):</span><br><span class="line">        y_hat = sigmoid(np.dot(X, <span class="variable language_">self</span>.weights) + <span class="variable language_">self</span>.bias)</span><br><span class="line">        y_hat[y_hat &gt;= <span class="number">0.5</span>] = <span class="number">1</span></span><br><span class="line">        y_hat[y_hat &lt; <span class="number">0.5</span>] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> y_hat</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 精度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">score</span>(<span class="params">self, y_pred, y</span>):</span><br><span class="line">        accuracy = (y_pred == y).<span class="built_in">sum</span>() / <span class="built_in">len</span>(y)</span><br><span class="line">        <span class="keyword">return</span> accuracy</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h2><blockquote>
<p>支持向量机（supporr vector machine，SVM）是一种二类分类模型，该模型是定义在特征空间上的间隔最大的线性分类器。间隔最大使它有区别于感知机；支持向量机还包括核技巧，这使它成为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的最小化问题。使用Hinge loss训练。</p>
</blockquote>
<blockquote>
<p>当训练数据线性可分时，通过硬间隔最大化（hard margin maximization）学习一个线性的分类器，即线性可分支持向量机，又成为硬间隔支持向量机；<br>当训练数据近似线性可分时，通过软间隔最大化（soft margin maximization）也学习一个线性的分类器，即线性支持向量机，又称为软间隔支持向量机；<br>当训练数据线性不可分时，通过核技巧（kernel trick）及软间隔最大化，学习非线性支持向量机。 </p>
</blockquote>
<h2 id="K-means"><a href="#K-means" class="headerlink" title="K-means"></a>K-means</h2><ul>
<li><p>K-means算法逻辑</p>
<blockquote>
<p>K-means算法是一个实用的无监督聚类算法，其聚类逻辑依托欧式距离，当两个目标的距离越近，相似度越大。对于给定的样本集，按照样本之间的距离大小，将样本集划分为 K 个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。<br>主要步骤：</p>
<ol>
<li>选择初始化的 k 个样本作为初始聚类中心 D &#x3D; D1 , D2 , D3 , …, Dk 。</li>
<li>针对数据集中每个样本 xi ，计算它到 k 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中。</li>
<li>针对每个类别 Dj ，重新计算它的聚类中心 Dj 。（即属于该类的所有样本的质心）。</li>
<li>重复上面2和3两步的操作，直到达到设定的中止条件（迭代次数、最小误差变化等）。</li>
</ol>
</blockquote>
</li>
<li><p>手撕K-means</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 生成随机数据</span></span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">2</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 定义K值和迭代次数</span></span><br><span class="line">K = <span class="number">3</span></span><br><span class="line">max_iterations = <span class="number">100</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 随机初始化簇中心点</span></span><br><span class="line">centers = X[np.random.choice(X.shape[<span class="number">0</span>], K, replace=<span class="literal">False</span>)]</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 迭代更新簇中心点</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_iterations):</span><br><span class="line">    <span class="comment"># 计算每个数据点到每个簇中心点的欧氏距离</span></span><br><span class="line">    distances = np.linalg.norm(X[:, np.newaxis, :] - centers, axis=<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 分配每个数据点到最近的簇</span></span><br><span class="line">    labels = np.argmin(distances, axis=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新簇中心点为每个簇的平均值</span></span><br><span class="line">    new_centers = np.array([X[labels == k].mean(axis=<span class="number">0</span>) <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(K)])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 如果簇中心点不再改变，结束迭代</span></span><br><span class="line">    <span class="keyword">if</span> np.<span class="built_in">all</span>(centers == new_centers):</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    centers = new_centers</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h2><ul>
<li><p>KNN算法逻辑</p>
<blockquote>
<p>KNN是一种非参数有监督分类算法。K近邻（K-NN）算法计算不同数据特征值之间的距离进行分类。存在一个样本数据集合，也称作训练数据集，并且数据集中每个数据都存在标签，即我们知道每一个数据与所属分类的映射关系。接着输入没有标签的新数据后，在训练数据集中找到与该新数据最邻近的K个数据，然后提取这K个数据中占多数的标签作为新数据的标签（少数服从多数逻辑）。（训练可以使用KD树加速）<br>主要步骤：</p>
<ol>
<li>计算新数据与各个训练数据之间的距离。</li>
<li>选取距离最小的K个点。</li>
<li>确定前K个点所在类别的出现频率</li>
<li>返回前K个点中出现频率最高的类别作为新数据的预测分类。</li>
</ol>
</blockquote>
</li>
<li><p>手撕KNN</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">KNNClassifier</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,k=<span class="number">3</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.k = k</span><br><span class="line">        <span class="variable language_">self</span>.x_train = <span class="literal">None</span></span><br><span class="line">        <span class="variable language_">self</span>.y_train = <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self,x_train,y_train</span>):</span><br><span class="line">        <span class="variable language_">self</span>.x_train = x_train</span><br><span class="line">        <span class="variable language_">self</span>.y_train = y_train</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">self,x_predict</span>):</span><br><span class="line">        y_predict = [<span class="variable language_">self</span>._predict(x) <span class="keyword">for</span> x <span class="keyword">in</span> x_predict]</span><br><span class="line">        <span class="keyword">return</span> np.array(y_predict)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_predict</span>(<span class="params">self,x</span>):</span><br><span class="line">        distances = [np.sqrt(np.<span class="built_in">sum</span>((x_train - x)**<span class="number">2</span>)) <span class="keyword">for</span> x_train <span class="keyword">in</span> <span class="variable language_">self</span>.x_train]</span><br><span class="line">        nearest = np.argsort(distances)[:<span class="variable language_">self</span>.k]</span><br><span class="line">        top_k_y = [<span class="variable language_">self</span>.y_train[index] <span class="keyword">for</span> index <span class="keyword">in</span> nearest]</span><br><span class="line">        votes = np.bincount(top_k_y)</span><br><span class="line">        <span class="keyword">return</span> votes.argmax()</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="PCA"><a href="#PCA" class="headerlink" title="PCA"></a>PCA</h2><blockquote>
<p>对原始样本进行中心化处理，即零均值化.<br>求出样本的协方差矩阵。<br>求解协方差矩阵的特征值和特征向量。<br>将特征值由大到小排列，取出前 k 个特征值对应的特征向量。<br>将 n 维样本映射到 k 维，实现降维处理。</p>
</blockquote>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>常见数据集划分方法<blockquote>
<p>留出法（hold-out） 直接将数据集划分为两个互斥的集合，其中一个集合作为训练集，另一个作为测试集。在训练集上训练出模型后，用测试集来评估其测试误差，作为对泛化误差的估计。</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>k折交叉验证（k-fold cross validation） 通过分层抽样的方法，将数据集划分为k个大小相似的互斥子集。选择k-1个子集合并作为训练集，用于模型的训练，而剩下的一个子集则作为测试集，用于评估模型的性能。这个过程重复k次，每次选择不同的子集作为测试集，从而获得k组不同的训练&#x2F;测试集组合。这种方式可以对模型进行k次独立的训练和测试，最终得到一个更加稳健和可靠的性能评估结果</p>
</blockquote>
<blockquote>
<p>自助法（boostrapping） 通过采用有放回抽样的方法，我们每次从原始数据集D中随机选择一个样本，并将其复制到新的数据集D’中。这个过程重复进行m次，从而创建了一个包含$m$个样本的训练集D’。根据概率论的公式，这种有放回抽样的方式意味着每个样本在m次抽样中都不被选中的概率是(1-1&#x2F;m)^m。当m趋向于无穷大时，这个概率的极限值为36.8%。因此，可以预期大约有36.8%的原始样本不会出现在新数据集D’中，这些未出现在D’中的样本可以用来作为测试集，以评估模型的性能。</p>
</blockquote>
<ul>
<li>判别式模型和生成式模型的区别<blockquote>
<p>判别式模型<br>目标：直接学习输入数据 X 和标签 Y 之间的决策边界，即条件概率 P ( Y | X ) 。<br>任务：对未见数据X ，根据 P ( Y | X ) 可以求得标签 Y ，即可以直接判别出来未见数据的标签，主要用于分类和回归任务，关注如何区分不同类别。<br>例子：逻辑回归、支持向量机（SVM）、神经网络、随机森林等。</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>生成式模型<br>目标：学习输入数据 X 和标签 Y 的联合概率分布 P ( X , Y ) ，并通过它推导出条件概率 P ( Y | X ) 。<br>任务：不仅用于分类，还可以生成新的数据样本、建模数据的分布。<br>例子：扩散模型、高斯混合模型（GMM）、隐马尔可夫模型（HMM）、朴素贝叶斯、生成对抗网络（GAN）等。</p>
</blockquote>
<ul>
<li>基础信息论：熵，交叉熵，KL散度，JS散度，互信息<blockquote>
<p>熵：衡量了一个概率分布的随机性程度，或者说它包含的信息量的大小。<br>公式:<br>对于离散型随机变量：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>E</mi><mi>p</mi></msub><mo stretchy="false">[</mo><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>=</mo><mo>−</mo><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>p</mi><mi>i</mi></msub><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><msub><mi>p</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(p)=E_p[-log(p(x))]=-\sum_{i=1}^n p_i log(p_i)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))]</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.104em;vertical-align:-0.2997em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8043em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>;</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>交叉熵：定义于两个概率分布之上，反映了它们之间的差异程度<br>公式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mo separator="true">,</mo><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>E</mi><mi>p</mi></msub><mo stretchy="false">[</mo><mo>−</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>=</mo><mo>−</mo><msub><mo>∑</mo><mi>x</mi></msub><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">H(p,q)=E_p[- log(q(x))]=-\sum_{x}p(x) log(q(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0361em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord">−</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))]</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0497em;vertical-align:-0.2997em;"></span><span class="mord">−</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.0017em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span>;<br>交叉熵不具有对称性，H(p,q) !&#x3D; H(q,p);</p>
</blockquote>
<blockquote>
<p>KL散度：也叫相对熵，用于度量两个分布之间的差异。<br>公式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>E</mi><mi>p</mi></msub><mo stretchy="false">[</mo><mi>l</mi><mi>o</mi><mi>g</mi><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">]</mo><mo>=</mo><msub><mo>∑</mo><mi>x</mi></msub><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>l</mi><mi>n</mi><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">D_{KL}(p|q)=E_p[log\frac{p(x)}{q(x)}]=\sum_{x}p(x) ln \frac{p(x)}{q(x)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mopen">[</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.0017em;"><span style="top:-2.4003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2997em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">n</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>;<br>与交叉熵的关系：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>D</mi><mrow><mi>K</mi><mi>L</mi></mrow></msub><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∣</mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mo separator="true">,</mo><mi>q</mi><mo stretchy="false">)</mo><mo>−</mo><mi>H</mi><mo stretchy="false">(</mo><mi>p</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">D_{KL}(p|q)=H(p,q)-H(p)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">D</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="mord mathnormal mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mclose">)</span></span></span></span>;</p>
</blockquote>
<blockquote>
<p>JS散度：KL散度是不对称的，会因为不同的顺序造成不一样的训练结果。<br>公式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi><mi>S</mi><mo stretchy="false">(</mo><mi>P</mi><mi mathvariant="normal">∣</mi><mi>Q</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>P</mi><mi mathvariant="normal">∣</mi><mfrac><mrow><mi>P</mi><mo>+</mo><mi>Q</mi></mrow><mn>2</mn></mfrac><mo stretchy="false">)</mo><mo>+</mo><mfrac><mn>1</mn><mn>2</mn></mfrac><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>Q</mi><mi mathvariant="normal">∣</mi><mfrac><mrow><mi>P</mi><mo>+</mo><mi>Q</mi></mrow><mn>2</mn></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">JS(P|Q)=\frac{1}{2}KL(P|\frac{P+Q}{2})+\frac{1}{2}KL(Q|\frac{P+Q}{2})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord">∣</span><span class="mord mathnormal">Q</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2694em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord">∣</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9244em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.2694em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mord mathnormal">L</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mord">∣</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9244em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">P</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">Q</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>;</p>
</blockquote>
<blockquote>
<p>互信息：变量间相互依赖性的量度。<br>公式:<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mrow><mi>y</mi><mo>∈</mo><mi>Y</mi></mrow></munder><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi>X</mi></mrow></munder><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">I(X,Y)=\sum\limits_{y\in Y}\sum\limits_{x\in X}p(x,y)log(\frac{p(x,y)}{p(x)p(y)})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.1404em;vertical-align:-1.1304em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.75em;"><span style="top:-2.1057em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.22222em;">Y</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1304em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.75em;"><span style="top:-2.1057em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∈</span><span class="mord mathnormal mtight" style="margin-right:0.07847em;">X</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0217em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span><span class="mord mathnormal mtight">p</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mopen mtight">(</span><span class="mord mathnormal mtight">x</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">y</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>;</p>
</blockquote>
<ul>
<li><p>数据类别不平衡怎么办？</p>
<blockquote>
<ol>
<li>数据增强。</li>
<li>对少数类别数据做过采样，多数类别数据做欠采样。</li>
<li>损失函数的权重均衡。（不同类别的loss权重不一样，最佳参数需要手动调节）</li>
<li>采集更多少数类别的数据。</li>
<li>Focal loss</li>
</ol>
</blockquote>
</li>
<li><p>什么是过拟合？解决办法有哪些？</p>
<blockquote>
<p>过拟合：模型在训练集上拟合的很好，但是模型连噪声数据的特征都学习了，丧失了对测试集的泛化能力。<br>解决过拟合的方法：</p>
<ol>
<li>重新清洗数据，数据不纯会导致过拟合，此类情况需要重新清洗数据或重新选择数据。</li>
<li>增加训练样本数量。使用更多的训练数据是解决过拟合最有效的手段。我们可以通过一定的规则来扩充训练数据，比如在图像分类问题上，可以通过图像的平移、旋转、缩放、加噪声等方式扩充数据;也可以用GAN网络来合成大量的新训练数据。</li>
<li>降低模型复杂程度。适当降低模型复杂度可以避免模型拟合过多的噪声数据。在神经网络中减少网络层数、神经元个数等。</li>
<li>加入正则化方法，增大正则项系数。给模型的参数加上一定的正则约束，比如将权值的大小加入到损失函数中。</li>
<li>采用dropout方法，dropout方法就是在训练的时候让神经元以一定的概率失活。</li>
<li>提前截断（early stopping），减少迭代次数。</li>
<li>集成学习方法。集成学习是把多个模型集成在一起，来降低单一模型的过拟合风险，如Bagging方法。</li>
</ol>
</blockquote>
</li>
<li><p>常用正则化手段</p>
<blockquote>
<ol>
<li>L范数</li>
<li>Dropout</li>
<li>BatchNorm</li>
<li>Early Stopping</li>
<li>数据增强</li>
</ol>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>算法岗</category>
        <category>八股文</category>
      </categories>
      <tags>
        <tag>算法岗</tag>
        <tag>八股文</tag>
      </tags>
  </entry>
  <entry>
    <title>算法岗知识汇总【深度学习基础】</title>
    <url>/2024/09/24/%E7%AE%97%E6%B3%95%E5%B2%97%E7%9F%A5%E8%AF%86%E6%B1%87%E6%80%BB%E3%80%90%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80%E3%80%91/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>正在更新中……</p>
<p>秋招在即，用这篇博客记录一下算法岗求职过程中的一些必备知识汇总。</p>
<span id="more"></span>

<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><ul>
<li><p>激活函数作用</p>
<blockquote>
<p>如果不用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合，这种情况就是最原始的感知机（Perceptron）。使用激活函数能够给神经元引入非线性因素，使得神经网络可以任意逼近任何非线性函数，使深层神经网络表达能力更加强大，这样神经网络就可以应用到众多的非线性模型中</p>
</blockquote>
</li>
<li><p>激活函数分类</p>
<div aligh="center">
<img src="/asset/2/activation_cat.png" class="lazyload" data-srcset="/asset/2/activation_cat.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/>
</div>
</li>
<li><p>常见激活函数</p>
<blockquote>
<p>Sigmoid<br>数学表达式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac><mo>∈</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)=\frac{1}{1+e^{-x}}\in (0,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2484em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7027em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>;<br>导数表达式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>f</mi><msup><mrow></mrow><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f^{&#x27;}(x)=f(x)(1-f(x))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1925em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9425em;"><span style="top:-2.9425em;margin-right:0.05em;"><span class="pstrut" style="height:2.5795em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8278em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">))</span></span></span></span>;<br>函数图像：</p>
<div aligh="center"><img src="/asset/2/sigmoid.png" class="lazyload" data-srcset="/asset/2/sigmoid.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="50%"/></div>
缺点：容易造成梯度消失；消耗计算资源</blockquote>
</li>
</ul>
<blockquote>
<p>Tanh<br>数学表达式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>−</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow><mrow><msup><mi>e</mi><mi>x</mi></msup><mo>+</mo><msup><mi>e</mi><mrow><mo>−</mo><mi>x</mi></mrow></msup></mrow></mfrac><mo>∈</mo><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\in (-1,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3907em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9874em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.5935em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7027em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7385em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8477em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight">−</span><span class="mord mathnormal mtight">x</span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>;<br>函数图像：</p>
<div aligh="center"><img src="/asset/2/tanh.png" class="lazyload" data-srcset="/asset/2/tanh.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="50%"/></div>
缺点：与Sigmoid类似
</blockquote>
<blockquote>
<p>ReLU<br>数学表达式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mo>+</mo><mi mathvariant="normal">∞</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)=max(0,x)\in [0,+\infty)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">+</span><span class="mord">∞</span><span class="mclose">)</span></span></span></span>;<br>函数图像：</p>
<div aligh="center"><img src="/asset/2/relu.png" class="lazyload" data-srcset="/asset/2/relu.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="50%"/></div>
优点：相较于上面的改进：解决了梯度消失，当输入为正时，不会饱和；由于ReLU线性非饱和的性质，在SGD中能快速收敛；计算复杂度低。
缺点：与Sigmoid一样不是以0为中心的；Dead ReLU，当输入为负时，梯度为0。这个神经元及之后的神经元梯度永远为0，不再对任何数据有所响应，导致相应参数永远不会被更新。
</blockquote>
<blockquote>
<p>LeakyReLU<br>数学表达式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>α</mi><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)=max(\alpha x,x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">αx</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>;<br>函数图像：</p>
<div aligh="center"><img src="/asset/2/leakyrelu.png" class="lazyload" data-srcset="/asset/2/leakyrelu.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="50%"/></div>
优点：解决了Dead ReLu问题；线性非饱和；计算复杂度低。
缺点：a需要先验知识人工赋值。
</blockquote>
<blockquote>
<p>Softmax<br>数学表达式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup><mrow><mo>∑</mo><msup><mi>e</mi><msub><mi>x</mi><mi>i</mi></msub></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">Softmax(x)=\frac{e^{x_i}}{\sum e^{x_i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.431em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.911em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em;">∑</span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6401em;"><span style="top:-2.8326em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.6595em;"></span><span class="mord mathnormal mtight">i</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3147em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7385em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:0em;margin-right:0.1em;"><span class="pstrut" style="height:2.6595em;"></span><span class="mord mathnormal mtight">i</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3147em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>;<br>函数图像：</p>
<div aligh="center"><img src="/asset/2/softmax.png" class="lazyload" data-srcset="/asset/2/softmax.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="50%"/></div>
Softmax函数常在神经网络输出层充当激活函数，将输出层的值通过激活函数映射到0-1区间，将神经元输出构造成概率分布，用于多分类问题中，Softmax激活函数映射值越大，则真实类别可能性越大。
</blockquote>
<blockquote>
<p>GELU<br>数学表达式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>E</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mi>P</mi><mo stretchy="false">(</mo><mi>X</mi><mo>&lt;</mo><mo>=</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">GELU(x)=xP(X&lt;=x) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">GE</span><span class="mord mathnormal" style="margin-right:0.10903em;">LU</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em;">X</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&lt;=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>，其中P(X&lt;&#x3D;x)是高斯分布<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo stretchy="false">(</mo><mi>μ</mi><mo separator="true">,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">N(\mu,\sigma^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mopen">(</span><span class="mord mathnormal">μ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，一般取标准分布。<br>函数图像：</p>
<div aligh="center"><img src="/asset/2/gelu.png" class="lazyload" data-srcset="/asset/2/gelu.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="50%"/></div>
</blockquote>
<blockquote>
<p>Swish<br>数学表达式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mi>σ</mi><mo stretchy="false">(</mo><mi>β</mi><mi>x</mi><mo stretchy="false">)</mo><mtext> </mtext><mo>=</mo><mi>x</mi><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mo>−</mo><mi>β</mi><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">Swish(x)=x\sigma(\beta x) \ =x \frac{1}{1+exp(-\beta x)} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">Sw</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace"> </span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.3651em;vertical-align:-0.52em;"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">p</span><span class="mopen mtight">(</span><span class="mord mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span><span class="mord mathnormal mtight">x</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>,<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>σ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span></span></span></span>是sigmoid函数。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span>是超参数。当<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span>是1时，就是SiLU激活函数。<br>函数图像：<div aligh="center"><img src="/asset/2/swish.png" class="lazyload" data-srcset="/asset/2/swish.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="50%"/></div></p>
</blockquote>
<blockquote>
<p>GLU(Gated Linear Unit)<br>数学表达式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>G</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>W</mi><mo separator="true">,</mo><mi>V</mi><mo separator="true">,</mo><mi>b</mi><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>o</mi><mi>i</mi><mi>d</mi><mo stretchy="false">(</mo><mi>x</mi><mi>W</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>⊙</mo><mo stretchy="false">(</mo><mi>x</mi><mi>V</mi><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">GLU(x,W,V,b,c) = sigmoid(xW+b) \odot (xV+c) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">G</span><span class="mord mathnormal" style="margin-right:0.10903em;">LU</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">c</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">c</span><span class="mclose">)</span></span></span></span> , <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊙</mo></mrow><annotation encoding="application/x-tex">\odot</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6667em;vertical-align:-0.0833em;"></span><span class="mord">⊙</span></span></span></span>是矩阵的按元素乘, W,V,b,c为可学习参数。</p>
</blockquote>
<blockquote>
<p>SwiGLU<br>数学表达式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>w</mi><mi>i</mi><mi>G</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>W</mi><mo separator="true">,</mo><mi>V</mi><mo separator="true">,</mo><mi>b</mi><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mi>S</mi><mi>w</mi><mi>i</mi><mi>s</mi><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mi>W</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo>⊙</mo><mo stretchy="false">(</mo><mi>x</mi><mi>V</mi><mo>+</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">SwiGLU(x,W,V,b,c)=Swish(xW+b) \odot (xV+c) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">Sw</span><span class="mord mathnormal">i</span><span class="mord mathnormal">G</span><span class="mord mathnormal" style="margin-right:0.10903em;">LU</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">b</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">c</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02691em;">Sw</span><span class="mord mathnormal">i</span><span class="mord mathnormal">s</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⊙</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">c</span><span class="mclose">)</span></span></span></span>，将GLU中的sigmoid换成了Swish。</p>
<p>相对于ReLU的优势：</p>
<ol>
<li>平滑的转换：SwiGLU在0附近提供了更平滑的转换，这有助于更好的优化过程。</li>
<li>门控特性：SwiGLU继承了GLU的门控机制，可以根据输入情况决定哪些信息应该通过，哪些应该被过滤，这有助于提高模型的泛化能力，特别是在处理长序列、长距离依赖的文本时。</li>
<li>可学习参数：SwiGLU中的参数可以通过训练学习，使得模型可以根据不同任务和数据集动态调整这些参数，增强了模型的灵活性和适应性。</li>
<li>非线性能力：SwiGLU相比于ReLU，在负值区域也有响应，这克服了ReLU在负输入下输出始终为零的缺点，使得网络可以更有效地学习到有用的表示。(DeadReLU,有助于缓解梯度消失)<br>SwiGLU代码：</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedForward</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size: <span class="built_in">int</span>, intermediate_size: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">       	<span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.w1 = nn.Linear(hidden_size, intermediate_size, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.w2 = nn.Linear(intermediate_size, hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        <span class="variable language_">self</span>.w3 = nn.Linear(hidden_size, intermediate_size, bias=<span class="literal">False</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="comment"># x: (batch_size, seq_len, hidden_size)</span></span><br><span class="line">        <span class="comment"># w1(x) -&gt; (batch_size, seq_len, intermediate_size)</span></span><br><span class="line">        <span class="comment"># w3(x) -&gt; (batch_size, seq_len, intermediate_size)</span></span><br><span class="line">        <span class="comment"># w2(*) -&gt; (batch_size, seq_len, hidden_size)</span></span><br><span class="line">    	<span class="keyword">return</span> <span class="variable language_">self</span>.w2(F.silu(<span class="variable language_">self</span>.w1(x)) * <span class="variable language_">self</span>.w3(x))</span><br></pre></td></tr></table></figure>

<h2 id="RNN-GRU-LSTM"><a href="#RNN-GRU-LSTM" class="headerlink" title="RNN&#x2F;GRU&#x2F;LSTM"></a>RNN&#x2F;GRU&#x2F;LSTM</h2><h2 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h2><h3 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder,Decoder"></a>Encoder,Decoder</h3><ul>
<li><p>Transformer结构描述</p>
<blockquote>
<p>见下图</p>
</blockquote>
<div aligh="center">
<img src="/asset/2/transformer.png" class="lazyload" data-srcset="/asset/2/transformer.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="50%"/>
</div>
</li>
<li><p>简述Transformer中的FFN。</p>
<blockquote>
<p>使用了ReLU作为激活函数。</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mi>F</mi><mi>N</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo stretchy="false">)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">FFN(x) = max(0,xW_1+b_1)W_2+b_2 </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">FFN</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">x</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>
</blockquote>
</li>
<li><p>Decoder和Encoder是如何进行交互的？</p>
<blockquote>
<p>Cross attention。Decoder提供Q，Encoder提供K,V。</p>
</blockquote>
</li>
<li><p>Decoder和Encoder结构的差别</p>
<blockquote>
<p>Encoder的MHSA中需要对padding部分进行mask；Decoder部分的第一个MHSA是self-attention，并且这部分需要引入casual mask避免后面的序列看到前面的序列；Decoder部分的第二个MHA是Cross-attention，其中Q来自前一部分的输出，K,V来自Encoder的输出。</p>
</blockquote>
</li>
<li><p>Decoder在进行推理时的解码策略？</p>
<blockquote>
<ul>
<li>Random Sampling:按照概率分布随机选择一个单词。这种方法可以增加生成的多样性，但是可能会导致生成的文本不连贯和无意义.</li>
<li>Greedy Search:直接选择概率最高的单词。这种方法简单高效，但是可能会导致生成的文本过于单调和重复。</li>
<li>Beam Search:维护一个大小为 k 的候选序列集合，每一步从每个候选序列的概率分布中选择概率最高的 k 个单词，然后保留总概率最高的 k 个候选序列。这种方法可以平衡生成的质量和多样性，但是可能会导致生成的文本过于保守和不自然。</li>
<li>Top-k sampling:是对贪心策略的优化，它从排名前 k 的 token 中进行抽样，允许其他分数或概率较高的token 也有机会被选中。在很多情况下，这种抽样带来的随机性有助于提高生成质量。在每一步，只从概率最高的 k 个单词中进行随机采样，而不考虑其他低概率的单词。</li>
<li>Top-p sampling（也叫Nucleus sampling）:在每一步，只从累积概率超过某个阈值 p 的最小单词集合中进行随机采样，而不考虑其他低概率的单词。这种方法也被称为核采样（nucleus sampling），因为它只关注概率分布的核心部分，而忽略了尾部部分。</li>
</ul>
</blockquote>
</li>
<li><p>残差的作用</p>
<blockquote>
<p>与Resnet相同，解决梯度消失，防止过拟合，加速模型收敛。</p>
</blockquote>
</li>
<li><p>Transformer是如何做到并行的？</p>
<blockquote>
<p>在Encoder的并行化主要体现在Self-attention模块，可以并行处理整个序列，并得到整个输入序列经过Encoder端的输出，但RNN只能从前到后的串行执行。<br>在Decoder端，训练的时候使用Teacher-forcing训练方式，因此也可以并行；但推理的时候仍然是自回归的模式。</p>
</blockquote>
</li>
<li><p>RNN，CNN和Transformer的区别</p>
</li>
</ul>
<blockquote>
<p>   RNN（递归神经网络）：<br>       时间序列处理：RNN特别适用于处理序列数据，如时间序列、自然语言等。<br>       递归结构：RNN通过递归地应用相同的权重来处理序列中的每个元素，允许信息在序列中流动。<br>       参数共享：在序列的每个时间步上，RNN使用相同的权重矩阵。<br>       问题：RNN在处理长序列时可能会遇到梯度消失或梯度爆炸的问题，这限制了它们学习长期依赖关系的能力。</p>
</blockquote>
<blockquote>
<p>  CNN（卷积神经网络）：<br>       空间特征提取：CNN主要用于图像处理，通过卷积层提取图像的空间特征。<br>       局部连接：每个卷积神经元只与输入数据的一个局部区域相连接，这减少了参数的数量。<br>       参数共享：卷积核在整个输入数据上滑动，共享相同的权重。<br>       层次结构：CNN通常具有多个卷积层，每个层级可以捕捉不同级别的特征。<br>       应用：CNN在图像分类、目标检测和图像分割等领域非常成功。</p>
</blockquote>
<blockquote>
<p>   Transformer：<br>       自注意力机制：Transformer使用自注意力机制来处理序列数据，允许模型在编码每个元素时考虑到序列中的所有其他元素。<br>       并行处理：由于自注意力机制，Transformer可以并行处理序列中的所有元素，这大大提高了训练效率。<br>       无循环结构：与RNN不同，Transformer没有递归或循环结构，这使得它们在处理长序列时更加有效。<br>       多头注意力：Transformer通常使用多头注意力，这允许模型同时学习序列数据的多个表示。<br>       应用：Transformer在自然语言处理任务中非常流行，如机器翻译、文本摘要和问答系统。</p>
</blockquote>
<blockquote>
<p>总结来说，RNN适合处理序列数据，但可能在长序列上遇到训练问题；CNN擅长提取图像的空间特征，但在处理序列数据时可能不是最佳选择；而Transformer通过自注意力机制有效地处理序列数据，且能够并行处理，使其在自然语言处理任务中非常有效。每种架构都有其优势和局限性，选择哪一种取决于具体的应用场景和数据类型</p>
</blockquote>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><!-- <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow></mrow><annotation encoding="application/x-tex"></annotation></semantics></math></span><span class="katex-html" aria-hidden="true"></span></span> -->

<ul>
<li>Attention机制描述<div aligh="center">
<img src="/asset/2/attention.png" class="lazyload" data-srcset="/asset/2/attention.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/>
</div></li>
</ul>
<blockquote>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>t</mi><mi>t</mi><mi>e</mi><mi>n</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mi>S</mi><mi>o</mi><mi>f</mi><mi>t</mi><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt{d_k}})V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">A</span><span class="mord mathnormal">tt</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mopen">(</span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.6275em;vertical-align:-0.538em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">t</span><span class="mord mathnormal">ma</span><span class="mord mathnormal">x</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0895em;"><span style="top:-2.5864em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8622em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span><span style="top:-2.8222em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1778em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Q</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.9191em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span>
</blockquote>
<ul>
<li>Attention的复杂度<blockquote>
<p>单头注意力的计算复杂度：<br>假设输入序列长度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span>，输出序列长度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>，词向量的维度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span>。计算复杂度约为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>L</mi><mo>∗</mo><mi>M</mi><mo>∗</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(L* M *d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span>。<br>多头注意力的计算复杂度：<br>假设输入序列长度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>L</mi></mrow><annotation encoding="application/x-tex">L</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span></span></span></span>，输出序列长度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span>，词向量的维度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span>，有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">h</span></span></span></span>，每个头的维度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>h</mi></msub><mo>=</mo><mi>d</mi><mi mathvariant="normal">/</mi><mi>h</mi></mrow><annotation encoding="application/x-tex">d_h=d/h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">d</span><span class="mord">/</span><span class="mord mathnormal">h</span></span></span></span>。计算复杂度约为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>h</mi><mo>∗</mo><mi>L</mi><mo>∗</mo><mi>M</mi><mo>∗</mo><msub><mi>d</mi><mi>h</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(h* L* M* d_h)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal">L</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>。</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>尽管Multi-head attention引入了多个头，但由于每个头的维度减小，并且计算可以并行进行，其总计算复杂度与单头Attention相同，即O(L⋅M⋅d)。然而，实际应用中，由于并行计算和维度分割，Multi-head attention通常能够更有效地利用计算资源。</p>
</blockquote>
<ul>
<li><p>Attention中为什么除以sqrt(k)？</p>
<blockquote>
<p>在计算<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>时，假设Q和K的维度为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，其中每个元素期望值是0，方差为1，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup><mo separator="true">,</mo><mi>K</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>m</mi><mo>×</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">Q\in\R^{n\times d_k},K\in\R^{m\times d_k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0435em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">×</span><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3488em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1512em;"><span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span></span>，这使得<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>的期望为0，方差为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。因此，在计算softmax时，如果<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>比较大，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>的值也会很大，导致softmax输出非常尖锐的分布，会出现指数溢出或梯度消失的问题，难以训练。因此对<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">QK^T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0358em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span></span></span></span>进行缩放，是的每个元素的方差变为1，避免了上述问题。<br>综上，提升了模型的训练效果和稳定性</p>
</blockquote>
</li>
<li><p>在计算Attention score时，如何对Padding做mask？</p>
<blockquote>
<p>一般有两种方式：</p>
<ol>
<li>Padding mask：将填充位置对应的token设置为一个很大的负数（如负无穷），这样在进行softmax计算式，填充位置对应的权重就会趋近于0，这样计算注意力时就不会考虑填充位置的信息。</li>
<li>Masked softmax： 在softmax之前，将填充位置对应的token的score设置为一个很小的值，然后再进行softmax。</li>
</ol>
</blockquote>
</li>
<li><p>为什么要用Multi-head Attention？</p>
</li>
</ul>
<blockquote>
<ol>
<li>并行计算: 多头注意力机制允许模型同时关注输入序列的不同部分，每个注意力头可以独立计算，从而实现更高效的并行计算。这样能够加快模型的训练速度。</li>
<li>提升表征能力： 通过引入多个注意力头，模型可以学习到不同类型的注意力权重，从而捕捉输入序列中不同层次、不同方面的语义信息。这有助于提升模型对输入序列的表征能力。</li>
<li>降低过拟合风险：多头注意力机制使得模型可以综合不同角度的信息，从而提高泛化能力，降低过拟合的风险。</li>
<li>降低计算复杂度： 通过对每个头进行降维，使得每个头的参数量减少，进而降低计算复杂度。</li>
</ol>
</blockquote>
<ul>
<li>手搓Multi-head attention<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, heads, d_model</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // heads  <span class="comment"># 每个“头”对应的维度</span></span><br><span class="line">        <span class="variable language_">self</span>.h = heads  <span class="comment"># “头”的数量</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化线性层，用于生成Q，K，V</span></span><br><span class="line">        <span class="variable language_">self</span>.q_linear = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.k_linear = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.v_linear = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="comment"># 输出线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.out = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># q,k,v [...,d]</span></span><br><span class="line">        <span class="comment"># 计算点积，并通过 sqrt(d_k) 进行缩放</span></span><br><span class="line">        scores = torch.matmul(q, k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(<span class="variable language_">self</span>.d_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果有 mask，应用于 scores</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对 scores 应用 softmax</span></span><br><span class="line">        scores = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取输出</span></span><br><span class="line">        output = torch.matmul(scores, v)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># q,k,v:[B,T,d]</span></span><br><span class="line">        batch_size = q.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对 q，k，v 进行线性变换</span></span><br><span class="line">        q = <span class="variable language_">self</span>.q_linear(q).view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.h, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        k = <span class="variable language_">self</span>.k_linear(k).view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.h, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        v = <span class="variable language_">self</span>.v_linear(v).view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.h, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行多头注意力计算</span></span><br><span class="line">        scores = <span class="variable language_">self</span>.attention(q, k, v, mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将多个头的输出拼接回单个张量</span></span><br><span class="line">        concat = scores.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.d_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过输出线性层</span></span><br><span class="line">        output = <span class="variable language_">self</span>.out(concat)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><ul>
<li><p>Positional Encoding的作用</p>
<blockquote>
<p>因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional encoding来表示token在句子中的绝对位置信息。</p>
</blockquote>
</li>
<li><p>Transformer使用的位置编码</p>
<blockquote>
<p>Sinusoidal Positional Encoding。这是一种<strong>绝对</strong>位置编码。<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>+</mo><mi>k</mi></mrow></msub><mtext>和</mtext><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">PE_{pos+k}和PE_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">和</span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>的内积会随着相对位置的递增而减小，从而表征位置的相对距离，但由于距离的对称性，此方法虽然能够反应相对位置的距离，但是无法区分方向，即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>+</mo><mi>k</mi></mrow></msub><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub><mo>=</mo><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>−</mo><mi>k</mi></mrow></msub><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">PE_{pos+k}PE_{pos}=PE_{pos-k}PE_{pos}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0576em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">p</span><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>。见下图</p>
</blockquote>
<div aligh="center">
<img src="/asset/2/pe.png" class="lazyload" data-srcset="/asset/2/pe.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/>
</div>
</li>
<li><p>什么是大模型的外推性？</p>
<blockquote>
<p>外推性是指大模型在训练时和预测时的输入长度不一致，导致模型的泛化能力下降的问题。</p>
</blockquote>
</li>
<li><p>不同种Positional Encoding</p>
<blockquote>
<p>此处参考<a href="https://kexue.fm/archives/8130">苏神的文章</a><br>绝对位置编码：</p>
<ul>
<li>Learnable Positional Encoding：直接将位置编码当作可训练参数。BERT，GPT，ALBERT等模型用的就是这种。缺点是没有外推性，无法感知相对位置。</li>
<li>Sinusidal Positional Encoding：虽然pos+k可以被pos线性表示，这提供了表达相对位置信息的可能性，但不能表示方向。还具有远程衰减的性质。没有外推性。</li>
<li>Autoregressive：RNN就属于这种，它本身自带位置信息。</li>
</ul>
</blockquote>
</li>
</ul>
<blockquote>
<p>相对位置编码：相对位置并没有完整建模每个输入的位置信息，而是在算Attention的时候考虑当前位置与被Attention的位置的相对距离。这一部分需要继续学习《Self-attention with Relative Position Representation》等文章。</p>
<ul>
<li>显式的相对位置（Self-attention with Relative Position Representation）：对于第m和第n个位置的token，其相对位置可以表示为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi><mo stretchy="false">(</mo><mi>m</mi><mo>−</mo><mi>n</mi><mo separator="true">,</mo><msub><mi>r</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>r</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r=clip(m-n,r_{min},r_{max})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">m</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">n</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，即两个token之间的相对距离。因此，相比于绝对位置，相对位置只需要有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mrow><mi>m</mi><mi>a</mi><mi>x</mi></mrow></msub><mo>−</mo><msub><mi>r</mi><mrow><mi>m</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">r_{max}-r_{min}+1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">ma</span><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">min</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>个表征向量即可，即在计算两个token之间的attetnion score时，只需要在attention中注入相对位置表征向量即可。这样可以表征任意长度的句子：<div aligh="center"><img src="/asset/2/rel_pe.png" class="lazyload" data-srcset="/asset/2/rel_pe.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div></li>
</ul>
</blockquote>
<blockquote>
<p>旋转位置编码(RoPE)：通过绝对位置编码的方式实现了相对位置编码，对attention中的q、k向量注入了绝对位置信息，qk内积就会引入相对位置信息。（具体推导见苏神论文）见下图：</p>
<div aligh="center"><img src="/asset/2/rope.png" class="lazyload" data-srcset="/asset/2/rope.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
这里的\theta采取的和Transformer中一致，可以带来远程衰减的性质。
</blockquote>
<ul>
<li>手撕Sinusoidal PE<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()       </span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len, dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>).transpose(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#pe.requires_grad = False</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.pe[:x.size(<span class="number">0</span>), :]</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="BatchNorm-LayerNorm-Dropout-etc"><a href="#BatchNorm-LayerNorm-Dropout-etc" class="headerlink" title="BatchNorm,LayerNorm,Dropout,etc"></a>BatchNorm,LayerNorm,Dropout,etc</h3><ul>
<li>BatchNorm原理<blockquote>
<p>训练时前向传导见下图：</p>
</blockquote>
<div aligh="center">
<img src="/asset/2/bn.png" class="lazyload" data-srcset="/asset/2/bn.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/>
</div></li>
</ul>
<blockquote>
<p>包含可学习参数<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi><mo separator="true">,</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">\gamma,\beta </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span>，让网络可以学习恢复出原始数据的特征分布。同时也保存整个训练集的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>μ</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msubsup><mi>σ</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\mu_{train},\sigma^2_{train} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0728em;vertical-align:-0.2587em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">ain</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">ain</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span></span></span></span>，使用移动平均法更新。</p>
</blockquote>
<ul>
<li><p>BatchNorm的优劣</p>
<blockquote>
<p>优点：解决内部协变量偏移，加速模型收敛；增强模型稳定性，允许使用更高的学习率，提高模型泛化能力。<br>缺点：对batch size敏感（batch size较小时效果差,可用Group Normalization代替）；不适用于变长序列；在推理阶段额外计算。</p>
</blockquote>
</li>
<li><p>BatchNorm训练和推理时的区别</p>
<blockquote>
<p>我们在预测阶段，有可能只需要预测一个样本或很少的样本，没有像训练样本中那么多的数据，这样的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo separator="true">,</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\sigma^2,\mu </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">μ</span></span></span></span>要怎么计算呢？利用训练集训练好模型之后，其实每一层的BN层都保留下了每一个batch算出来的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>σ</mi><mn>2</mn></msup><mo separator="true">,</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\sigma^2,\mu </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0085em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">μ</span></span></span></span>（使用移动平均得到），利用整体训练集的无偏估计来估计测试集的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow><mn>2</mn></msubsup><mo separator="true">,</mo><msub><mi>μ</mi><mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\sigma^2_{test},\mu_{test} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">es</span><span class="mord mathnormal mtight">t</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">es</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。即<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>μ</mi><mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow></msub><mo>=</mo><mi>E</mi><mo stretchy="false">(</mo><msub><mi>μ</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo><mo separator="true">,</mo><msubsup><mi>σ</mi><mrow><mi>t</mi><mi>e</mi><mi>s</mi><mi>t</mi></mrow><mn>2</mn></msubsup><mo>=</mo><mfrac><mi>m</mi><mrow><mi>m</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mi>E</mi><mo stretchy="false">(</mo><msubsup><mi>σ</mi><mrow><mi>t</mi><mi>r</mi><mi>a</mi><mi>i</mi><mi>n</mi></mrow><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mu_{test}=E(\mu_{train}),\sigma^2_{test}=\frac{m}{m-1}E(\sigma^2_{train})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">es</span><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">ain</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight">es</span><span class="mord mathnormal mtight">t</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2174em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6954em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.4413em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span><span class="mord mathnormal mtight">ain</span></span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2587em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，然后再用学习到的参数进行BN。</p>
</blockquote>
</li>
<li><p>其他的Norm方法</p>
<blockquote>
<p>不同种Norm方法之间区别如下图：</p>
</blockquote>
<div aligh="center">
<img src="/asset/2/norm.png" class="lazyload" data-srcset="/asset/2/norm.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/>
</div></li>
</ul>
<blockquote>
<p>BatchNorm：batch 方向做归一化，算 N ∗ H ∗ W 的均值<br>LayerNorm：channel 方向做归一化，算 C ∗ H ∗ W 的均值<br>InstanceNorm：一个 channel 内做归一化，算 H ∗ W 的均值<br>GroupNorm：将 channel 方向分 group ，然后每个 group 内做归一化，算 ( C &#x2F; &#x2F; G ) ∗ H ∗ W 的均值</p>
</blockquote>
<ul>
<li>Transformer中用BatchNorm可以吗？<blockquote>
<p>LN是针对每个样本序列进行归一化，没有批量依赖，不会因为batchsize变化而变化，对一个序列的不同特征维度进行归一化。<br>CV使用BN是因为认为通道维度的信息对cv方面有重要意义，如果对通道维度也归一化会造成不同通道信息一定的损失。NLP认为句子长短不一，且各batch之间的信息没有什么关系，因此只考虑句子内信息的归一化。</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>LayerNorm是对每个样本的所有特征做归一化，BatchNorm是对一个batch样本内的每个特征做归一化。</p>
</blockquote>
<ul>
<li><p>手撕BatchNorm</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Batchnorm_simple_for_train</span>(<span class="params">x, gamma, beta, bn_param</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">param:x    : 输入数据，设shape(B,L)</span></span><br><span class="line"><span class="string">param:gama : 缩放因子  γ</span></span><br><span class="line"><span class="string">param:beta : 平移因子  β</span></span><br><span class="line"><span class="string">param:bn_param   : batchnorm所需要的一些参数</span></span><br><span class="line"><span class="string">	eps      : 接近0的数，防止分母出现0</span></span><br><span class="line"><span class="string">	momentum : 动量参数，一般为0.9， 0.99， 0.999</span></span><br><span class="line"><span class="string">	running_mean ：滑动平均的方式计算新的均值，训练时计算，为测试数据做准备</span></span><br><span class="line"><span class="string">	running_var  : 滑动平均的方式计算新的方差，训练时计算，为测试数据做准备</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">	running_mean = bn_param[<span class="string">&#x27;running_mean&#x27;</span>]  <span class="comment">#shape = [B]</span></span><br><span class="line">    running_var = bn_param[<span class="string">&#x27;running_var&#x27;</span>]    <span class="comment">#shape = [B]</span></span><br><span class="line">	results = <span class="number">0.</span> <span class="comment"># 建立一个新的变量</span></span><br><span class="line">    </span><br><span class="line">	x_mean=x.mean(axis=<span class="number">0</span>)  <span class="comment"># 计算x的均值</span></span><br><span class="line">    x_var=x.var(axis=<span class="number">0</span>)    <span class="comment"># 计算方差</span></span><br><span class="line">    x_normalized=(x-x_mean)/np.sqrt(x_var+eps)       <span class="comment"># 归一化</span></span><br><span class="line">    results = gamma * x_normalized + beta            <span class="comment"># 缩放平移</span></span><br><span class="line"></span><br><span class="line">    running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * x_mean</span><br><span class="line">    running_var = momentum * running_var + (<span class="number">1</span> - momentum) * x_var</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#记录新的值</span></span><br><span class="line">    bn_param[<span class="string">&#x27;running_mean&#x27;</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">&#x27;running_var&#x27;</span>] = running_var </span><br><span class="line">    </span><br><span class="line">	<span class="keyword">return</span> results , bn_param</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Batchnorm_simple_for_test</span>(<span class="params">x, gamma, beta, bn_param</span>):</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">param:x    : 输入数据，设shape(B,L)</span></span><br><span class="line"><span class="string">param:gama : 缩放因子  γ</span></span><br><span class="line"><span class="string">param:beta : 平移因子  β</span></span><br><span class="line"><span class="string">param:bn_param   : batchnorm所需要的一些参数</span></span><br><span class="line"><span class="string">	eps      : 接近0的数，防止分母出现0</span></span><br><span class="line"><span class="string">	momentum : 动量参数，一般为0.9， 0.99， 0.999</span></span><br><span class="line"><span class="string">	running_mean ：滑动平均的方式计算新的均值，训练时计算，为测试数据做准备</span></span><br><span class="line"><span class="string">	running_var  : 滑动平均的方式计算新的方差，训练时计算，为测试数据做准备</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">	running_mean = bn_param[<span class="string">&#x27;running_mean&#x27;</span>]  <span class="comment">#shape = [B]</span></span><br><span class="line">    running_var = bn_param[<span class="string">&#x27;running_var&#x27;</span>]    <span class="comment">#shape = [B]</span></span><br><span class="line">	results = <span class="number">0.</span> <span class="comment"># 建立一个新的变量</span></span><br><span class="line">   </span><br><span class="line">    x_normalized=(x-running_mean )/np.sqrt(running_var +eps)       <span class="comment"># 归一化</span></span><br><span class="line">    results = gamma * x_normalized + beta            <span class="comment"># 缩放平移</span></span><br><span class="line">    </span><br><span class="line">	<span class="keyword">return</span> results , bn_param</span><br></pre></td></tr></table></figure>
</li>
<li><p>手撕LayerNorm</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">eps = <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>:</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x, w, b</span>):</span><br><span class="line">        <span class="comment"># x is the input activations, of shape B,T,C</span></span><br><span class="line">        <span class="comment"># w are the weights, of shape C</span></span><br><span class="line">        <span class="comment"># b are the biases, of shape C</span></span><br><span class="line">        B, T, C = x.size()</span><br><span class="line">        <span class="comment"># calculate the mean</span></span><br><span class="line">        mean = x.<span class="built_in">sum</span>(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) / C <span class="comment"># B,T,1</span></span><br><span class="line">        <span class="comment"># calculate the variance</span></span><br><span class="line">        xshift = x - mean <span class="comment"># B,T,C</span></span><br><span class="line">        var = (xshift**<span class="number">2</span>).<span class="built_in">sum</span>(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) / C <span class="comment"># B,T,1</span></span><br><span class="line">        <span class="comment"># calculate the inverse standard deviation: **0.5 is sqrt, **-0.5 is 1/sqrt</span></span><br><span class="line">        rstd = (var + eps) ** -<span class="number">0.5</span> <span class="comment"># B,T,1</span></span><br><span class="line">        <span class="comment"># normalize the input activations</span></span><br><span class="line">        norm = xshift * rstd <span class="comment"># B,T,C</span></span><br><span class="line">        <span class="comment"># scale and shift the normalized activations at the end</span></span><br><span class="line">        out = norm * w + b <span class="comment"># B,T,C</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># return the output and the cache, of variables needed later during the backward pass</span></span><br><span class="line">        cache = (x, w, mean, rstd)</span><br><span class="line">        <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">dout, cache</span>):</span><br><span class="line">        x, w, mean, rstd = cache</span><br><span class="line">        <span class="comment"># recompute the norm (save memory at the cost of compute)</span></span><br><span class="line">        norm = (x - mean) * rstd</span><br><span class="line">        <span class="comment"># gradients for weights, bias</span></span><br><span class="line">        db = dout.<span class="built_in">sum</span>((<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        dw = (dout * norm).<span class="built_in">sum</span>((<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># gradients for input</span></span><br><span class="line">        dnorm = dout * w</span><br><span class="line">        dx = dnorm - dnorm.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) - norm * (dnorm * norm).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        dx *= rstd</span><br><span class="line">        <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure></li>
<li><p>Pre norm和Post norm的区别</p>
<blockquote>
<div aligh="center"><img src="/asset/2/pre_post.png" class="lazyload" data-srcset="/asset/2/pre_post.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
Pre-norm训练更稳定，更容易收敛。
Post-norm性能更好，但是需要warm up，且对超参数敏感，训练初期可能会有梯度消失或梯度爆炸，比较难训练。
</blockquote>
</li>
<li><p>Dropout在训练和推理时的区别</p>
<blockquote>
<p>训练时，随机屏蔽一部分神经元以防止过拟合；测试时，需要用完整的模型进行预测，因此禁用dropout。（可以通过model.eval()）<br>使用dropout需要对神经元的输出重新缩放：</p>
<ul>
<li>假设dropout的保留率为p，在训练期间，一个神经元以概率p保留，其输出会被除以p来保持期望不变。</li>
<li>在测试期间，所有神经元保留，保持原始输出即可。</li>
</ul>
</blockquote>
</li>
<li><p>Dropout作用</p>
<blockquote>
<p>是一种常用的正则化技术，训练时随机丢弃神经元，主要用于防止过拟合。<br>位置：Embedding之后；MHSA之后的Add&amp;Norm之前；FFN的激活函数之后，Add&amp;Norm之前；Decoder的最终输出层之前。</p>
</blockquote>
</li>
</ul>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><ul>
<li><p>BGD优化器</p>
<blockquote>
<p>BGD采用整个训练集的数据来计算loss对参数的梯度。<br>优点：如果是凸优化一定能取得全局最优解，如果是非凸优化可以取得局部最优解<br>缺点：在一次迭代中，对整个数据集计算梯度，计算起来非常慢，遇到大型数据集会非常棘手。</p>
</blockquote>
</li>
<li><p>SGD优化器</p>
<blockquote>
<p>SGD 可以避免 BGD 因为大数据集而造成的冗余计算，比如 BGD 会对相似的数据进行重复计算。SGD 则是每次只选择一个样本的数据来进行更新梯度。<br>优点：由于一次只用一个数据，因此梯度更新很快；当然也可以进行在线学习（不用收齐所有数据）。<br>缺点：因为震荡，很难收敛于一个精准的极小值。</p>
</blockquote>
</li>
<li><p>MBGD优化器</p>
<blockquote>
<p>小批量随机梯度下降可以看作是 SGD 和 BGD 的中间选择，每次选择数量为 n 的数据进行计算，既节约的每次更新的计算时间和成本，也减少了 SGD 的震荡，使得收敛更加快速和稳定。<br>缺点：选择合适的学习率仍然是一个玄学；对于非凸问题极易陷入局部最优（鞍点）。</p>
</blockquote>
</li>
<li><p>Momentum优化器</p>
<blockquote>
<p>公式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>v</mi><mi>t</mi></msub><mo>=</mo><mi>γ</mi><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mi>η</mi><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>θ</mi><mo>=</mo><mi>θ</mi><mo>−</mo><msub><mi>v</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">v_t=\gamma v_{t-1}+\eta \nabla_{\theta}J(\theta), \theta=\theta-v_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7917em;vertical-align:-0.2083em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">η</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7778em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，一般<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span></span></span></span>取0.9.<br>优点： 加速收敛。</p>
</blockquote>
</li>
<li><p>Adagrad优化器</p>
<blockquote>
<p>公式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo>=</mo><msub><mi>θ</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo>−</mo><mfrac><mi>η</mi><msqrt><mrow><msub><mi>G</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi><mi>i</mi></mrow></msub><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\theta_{t+1,i}=\theta_{t,i}-\frac{\eta}{\sqrt{G_{t,ii}+\epsilon}}\nabla_{\theta}J(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9805em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.5796em;vertical-align:-0.8296em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7475em;"><span style="top:-2.4987em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9876em;"><span class="svg-align" style="top:-3.4286em;"><span class="pstrut" style="height:3.4286em;"></span><span class="mord mtight" style="padding-left:1.19em;"><span class="mord mtight"><span class="mord mathnormal mtight">G</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mpunct mtight">,</span><span class="mord mathnormal mtight">ii</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em;"><span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">ϵ</span></span></span><span style="top:-2.9596em;"><span class="pstrut" style="height:3.4286em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.5429em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.5429em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4689em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">η</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8296em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span><br>优点：减少了学习率的手动调节<br>缺点： 学习率会收缩并变得非常小。</p>
</blockquote>
</li>
<li><p>RMSprop优化器</p>
</li>
</ul>
<blockquote>
<p>为了解决AdaGrad学习率急剧下降问题的。<br>公式： <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">[</mo><msup><mi>g</mi><mn>2</mn></msup><msub><mo stretchy="false">]</mo><mi>t</mi></msub><mo>=</mo><mn>0.9</mn><mi>E</mi><mo stretchy="false">[</mo><msup><mi>g</mi><mn>2</mn></msup><msub><mo stretchy="false">]</mo><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mn>0.1</mn><msubsup><mi>g</mi><mi>t</mi><mn>2</mn></msubsup><mo separator="true">,</mo><mspace linebreak="newline"></mspace><msub><mi>θ</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>θ</mi><mi>t</mi></msub><mo>−</mo><mfrac><mi>η</mi><msqrt><mrow><mi>E</mi><mo stretchy="false">[</mo><msup><mi>g</mi><mn>2</mn></msup><mo stretchy="false">]</mo><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E[g^2]_t=0.9E[g^2]_{t-1}+0.1g_t^2,\\ \theta_{t+1}=\theta_{t}-\frac{\eta}{\sqrt{E[g^2]+\epsilon}}\nabla_{\theta}J(\theta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord">0.9</span><span class="mord mathnormal" style="margin-right:0.05764em;">E</span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">]</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord">0.1</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mpunct">,</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.5796em;vertical-align:-0.8296em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7475em;"><span style="top:-2.4642em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0369em;"><span class="svg-align" style="top:-3.4286em;"><span class="pstrut" style="height:3.4286em;"></span><span class="mord mtight" style="padding-left:1.19em;"><span class="mord mathnormal mtight" style="margin-right:0.05764em;">E</span><span class="mopen mtight">[</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose mtight">]</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">ϵ</span></span></span><span style="top:-3.0089em;"><span class="pstrut" style="height:3.4286em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.5429em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.5429em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4197em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">η</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8296em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="mclose">)</span></span></span></span></p>
</blockquote>
<ul>
<li><p>Adam优化器</p>
<blockquote>
<p>是Momentum和RMSprop的结合体，需要保存梯度和梯度平方的指数加权平均<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>v</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">m_t,v_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。<br>公式：</p>
<ol>
<li><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>t</mi></msub><mo>=</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>m</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>β</mi><mn>1</mn></msub><mo stretchy="false">)</mo><msub><mi>g</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>v</mi><mi>t</mi></msub><mo>=</mo><msub><mi>β</mi><mn>2</mn></msub><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><msub><mi>β</mi><mn>2</mn></msub><mo stretchy="false">)</mo><msubsup><mi>g</mi><mi>t</mi><mn>2</mn></msubsup><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t,v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2.</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0641em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0528em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mord">.</span></span></span></span></li>
<li>由于初始时刻没有什么可平均的，因此进行偏差修正，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><msub><mi>m</mi><mi>t</mi></msub><mo>^</mo></mover><mo>=</mo><mfrac><msub><mi>m</mi><mi>t</mi></msub><mrow><mn>1</mn><mo>−</mo><msubsup><mi>β</mi><mn>1</mn><mi>t</mi></msubsup></mrow></mfrac><mo separator="true">,</mo><mover accent="true"><msub><mi>v</mi><mi>t</mi></msub><mo>^</mo></mover><mo>=</mo><mfrac><msub><mi>v</mi><mi>t</mi></msub><mrow><mn>1</mn><mo>−</mo><msubsup><mi>β</mi><mn>2</mn><mi>t</mi></msubsup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\hat{m_t}=\frac{m_t}{1-\beta_1^t},\hat{v_t}=\frac{v_t}{1-\beta_2^t}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2885em;vertical-align:-0.577em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7115em;"><span style="top:-2.6411em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7841em;"><span style="top:-2.1885em;margin-left:-0.0528em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">1</span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.577em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2885em;vertical-align:-0.577em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7115em;"><span style="top:-2.6411em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7841em;"><span style="top:-2.1885em;margin-left:-0.0528em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span><span style="top:-2.8448em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3115em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.577em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>.</li>
<li>最后更新权重<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>t</mi></msub><mo>=</mo><msub><mi>θ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>−</mo><mfrac><mi>η</mi><msqrt><mrow><mover accent="true"><msub><mi>v</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>^</mo></mover><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><mover accent="true"><msub><mi>m</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\theta_{t}=\theta_{t-1}-\frac{\eta}{\sqrt{\hat{v_{t-1}}+\epsilon}}\hat{m_{t-1}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.9028em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.5771em;vertical-align:-0.8296em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.7475em;"><span style="top:-2.467em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0329em;"><span class="svg-align" style="top:-3.4286em;"><span class="pstrut" style="height:3.4286em;"></span><span class="mord mtight" style="padding-left:1.19em;"><span class="mord accent mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3173em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2025em;"><span></span></span></span></span></span></span></span><span style="top:-2.7em;"><span class="pstrut" style="height:2.7em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord mtight">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2025em;"><span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">ϵ</span></span></span><span style="top:-3.0049em;"><span class="pstrut" style="height:3.4286em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.5429em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.5429em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4237em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4461em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">η</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.8296em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span></span></span>。</li>
</ol>
</blockquote>
</li>
<li><p>AdamW优化器</p>
<blockquote>
<p>在Adam基础上加了L2正则化项，即optimizer参数中的weight_decay项。</p>
</blockquote>
</li>
</ul>
<h2 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h2><ul>
<li><p>梯度消失和梯度爆炸及解决办法</p>
<blockquote>
<p>梯度消失：梯度趋近于零，网络权重无法更新或更新的很微小，网络训练再久也不会有效果<br>原因：激活函数偏导过小，梯度连乘导致很低。<br>梯度爆炸：梯度呈指数级增长，变的非常大，然后导致网络权重的大幅更新，使网络变得不稳定.<br>原因：模型初始化权重可能很大。连乘导致爆炸。<br>解决方法：</p>
<ul>
<li>梯度截断、梯度正则；</li>
<li>使用ReLU、LeakyReLU等激活函数；</li>
<li>引入BN层；</li>
<li>使用残差结构。</li>
</ul>
</blockquote>
</li>
<li><p>什么是warm up？</p>
<blockquote>
<p>模型训练开始时使用非常小的学习率，再逐渐增大。</p>
</blockquote>
</li>
<li><p>zero shot，few shot区别</p>
<blockquote>
<p>Zero-shot: 利用训练集数据训练模型，使得模型能够对测试集的对象进行分类，但是训练集类别和测试集类别之间没有交集；期间需要借助类别的描述，来建立训练集和测试集之间的联系，从而使得模型有效。<br>Few-shot: 旨在利用极少量的样本来训练模型，从而在新的任务中表现出良好的性能。这通常涉及到模型在预训练阶段获得大量的背景知识，然后在只提供几个新样本的情况下快速适应新任务。</p>
</blockquote>
</li>
<li><p>Pytorch中nn.eval函数和训练的区别</p>
<blockquote>
<div aligh="center"><img src="/asset/2/nneval.png" class="lazyload" data-srcset="/asset/2/nneval.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
</li>
<li><p>权重初始化的方法</p>
<blockquote>
<ul>
<li>常数初始化： 把权值或者偏置初始化为一个常数，例如设置为0，偏置初始化为0较为常见，权重很少会初始化为0。</li>
<li>随机分布初始化：可以使用均匀分布或高斯分布初始化(uniform)。</li>
<li>xavier初始化：目的是为了使得模型各层的激活值和梯度在传播过程中的方差保持一致。在Tanh中表现好，在ReLU中表现差。</li>
<li>Kaiming初始化：针对于Relu的初始化方法。本质上是高斯分布初始化，与上述高斯分布初始化有所不同，其是个满足均值为0，方差为2&#x2F;n的高斯分布。</li>
</ul>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>算法岗</category>
        <category>八股文</category>
      </categories>
      <tags>
        <tag>算法岗</tag>
        <tag>八股文</tag>
      </tags>
  </entry>
  <entry>
    <title>算法岗知识汇总【图像处理】</title>
    <url>/2024/09/24/%E7%AE%97%E6%B3%95%E5%B2%97%E7%9F%A5%E8%AF%86%E6%B1%87%E6%80%BB%E3%80%90%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E3%80%91/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>正在更新中……</p>
<p>秋招在即，用这篇博客记录一下算法岗求职过程中的一些必备知识汇总。</p>
<span id="more"></span>
<h2 id="颜色空间"><a href="#颜色空间" class="headerlink" title="颜色空间"></a>颜色空间</h2><h3 id="RGB"><a href="#RGB" class="headerlink" title="RGB"></a>RGB</h3><h3 id="HSI"><a href="#HSI" class="headerlink" title="HSI"></a>HSI</h3><h3 id="CMYK"><a href="#CMYK" class="headerlink" title="CMYK"></a>CMYK</h3><h3 id="YUV"><a href="#YUV" class="headerlink" title="YUV"></a>YUV</h3><h2 id="图像滤波"><a href="#图像滤波" class="headerlink" title="图像滤波"></a>图像滤波</h2><h3 id="高斯滤波"><a href="#高斯滤波" class="headerlink" title="高斯滤波"></a>高斯滤波</h3><h3 id="均值滤波"><a href="#均值滤波" class="headerlink" title="均值滤波"></a>均值滤波</h3><h3 id="中值滤波"><a href="#中值滤波" class="headerlink" title="中值滤波"></a>中值滤波</h3><h3 id="双边滤波"><a href="#双边滤波" class="headerlink" title="双边滤波"></a>双边滤波</h3><h2 id="图像插值"><a href="#图像插值" class="headerlink" title="图像插值"></a>图像插值</h2><div aligh="center"><img src="/asset/2/interpolation.png" class="lazyload" data-srcset="/asset/2/interpolation.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<h3 id="最近邻插值"><a href="#最近邻插值" class="headerlink" title="最近邻插值"></a>最近邻插值</h3><blockquote>
<p>在待求像素的四邻域中，将距离待求像素最近的邻像素的灰度值赋给待求像素。</p>
</blockquote>
<h3 id="双线性插值"><a href="#双线性插值" class="headerlink" title="双线性插值"></a>双线性插值</h3><blockquote>
<p>设f(i,j)为在(i,j)位置的像素，现在想求f(i+u,j+v)的像素，其中0&lt; u &lt;1,0&lt; v &lt;1。</p>
<p>对应的公式为</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>i</mi><mo>+</mo><mi>u</mi><mo separator="true">,</mo><mi>j</mi><mo>+</mo><mi>v</mi><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>u</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>v</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>f</mi><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>u</mi><mo stretchy="false">)</mo><mi>v</mi><mo>⋅</mo><mi>f</mi><mo stretchy="false">(</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>+</mo><mi>u</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>v</mi><mo stretchy="false">)</mo><mo>⋅</mo><mi>f</mi><mo stretchy="false">(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">)</mo><mo>+</mo><mi>u</mi><mi>v</mi><mo>⋅</mo><mi>f</mi><mo stretchy="false">(</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo separator="true">,</mo><mi>j</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(i+u,j+v)=(1-u)(1-v)\cdot f(i,j) + (1-u)v \cdot f(i,j+1) + u(1-v)\cdot f(i+1,j)+uv \cdot f(i+1,j+1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">u</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">u</span><span class="mclose">)</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">u</span><span class="mclose">)</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">u</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.4445em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">uv</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">⋅</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">i</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.854em;vertical-align:-0.1944em;"></span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.05724em;">j</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>
</blockquote>
<h3 id="Bicubic插值"><a href="#Bicubic插值" class="headerlink" title="Bicubic插值"></a>Bicubic插值</h3><h2 id="直线检测"><a href="#直线检测" class="headerlink" title="直线检测"></a>直线检测</h2><h3 id="Hough变换"><a href="#Hough变换" class="headerlink" title="Hough变换"></a>Hough变换</h3><h2 id="边缘检测"><a href="#边缘检测" class="headerlink" title="边缘检测"></a>边缘检测</h2><h3 id="Canny算法"><a href="#Canny算法" class="headerlink" title="Canny算法"></a>Canny算法</h3><h3 id="Sobel算法"><a href="#Sobel算法" class="headerlink" title="Sobel算法"></a>Sobel算法</h3><h2 id="直方图均衡化"><a href="#直方图均衡化" class="headerlink" title="直方图均衡化"></a>直方图均衡化</h2><h2 id="傅里叶变换"><a href="#傅里叶变换" class="headerlink" title="傅里叶变换"></a>傅里叶变换</h2><h3 id="图像中的低频信息与高频信息"><a href="#图像中的低频信息与高频信息" class="headerlink" title="图像中的低频信息与高频信息"></a>图像中的低频信息与高频信息</h3><h3 id="低通滤波与高通滤波"><a href="#低通滤波与高通滤波" class="headerlink" title="低通滤波与高通滤波"></a>低通滤波与高通滤波</h3><h2 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h2><h3 id="SIFT"><a href="#SIFT" class="headerlink" title="SIFT"></a>SIFT</h3><h3 id="SURF"><a href="#SURF" class="headerlink" title="SURF"></a>SURF</h3><h3 id="HOG"><a href="#HOG" class="headerlink" title="HOG"></a>HOG</h3><h2 id="腐蚀和膨胀"><a href="#腐蚀和膨胀" class="headerlink" title="腐蚀和膨胀"></a>腐蚀和膨胀</h2>]]></content>
      <categories>
        <category>算法岗</category>
        <category>八股文</category>
      </categories>
      <tags>
        <tag>算法岗</tag>
        <tag>八股文</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——Controllable系列</title>
    <url>/2024/10/31/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Controllable%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="ControlNet"><a href="#ControlNet" class="headerlink" title="ControlNet"></a>ControlNet</h2><ul>
<li><p>ControlNet可以添加的控制条件类型</p>
<blockquote>
<p>Canny Edge, Hough lines, User scrobbles, Human Keypoints, Segmentation maps, shape normals ,depth.</p>
<p>多个条件可以叠加，将controlnet的结果相加即可。</p>
</blockquote>
</li>
<li><p>ControlNet的结构</p>
<blockquote>
<div aligh="center"><img src="/asset/2/controlnet.png" class="lazyload" data-srcset="/asset/2/controlnet.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>会将SD的Unet的Encoder和Middle做trainable copy，并且条件输入时和输出时会过一个Zero-initialized conv。然后在Skip connection时，与原网络的部分相加后，与Decoder的concat。</p>
</blockquote>
</li>
<li><p>Zero-conv的结构？为什么要零初始化？</p>
<blockquote>
<p>Zero-conv是w和b初始化为0的1*1卷积层。使用零初始化的目的是：在训练初期时，trainable copy不会对原始模型产生有害的噪声。</p>
</blockquote>
</li>
<li><p>ControlNet推理时的条件控制</p>
<blockquote>
<p>CFG resolution weighting：使用CFG推理时，会同时计算cond和uncond。如果同时将conditioning image加到uncond和cond，会完全忽视CFG guidance；如果只加到cond，将会使得guidance太强。</p>
<p>提出的方法是：将conditioning image只加到cond，但是每次在skip connectionn加的时候会根据分辨率添加一个权重w&#x3D;64&#x2F;h，h为分辨率大小。通过这样，可以减轻guidance的强度。</p>
</blockquote>
</li>
</ul>
<h2 id="T2I-Adapter"><a href="#T2I-Adapter" class="headerlink" title="T2I-Adapter"></a>T2I-Adapter</h2><ul>
<li><p>T2I-Adapter可以添加的控制条件类型</p>
<blockquote>
<p>Sketch map, Semantic segmentation map, Keypoints , color , depth maps</p>
</blockquote>
</li>
<li><p>T2I-Adapter的模型结构，以及它作用的位置</p>
<blockquote>
<div aligh="center"><img src="/asset/2/t2i_adapter.png" class="lazyload" data-srcset="/asset/2/t2i_adapter.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>Adapter结构由四个特征提取block和三个downsample block组成，输入分辨率为512 * 512。初始时，将condition map使用pixel unshuffle降采样到64 * 64。然后在每一层scale会输出一个feature，对应SD encoder中的分辨率大小。这四个feature会分别加到SD的UNet encoder的中间feature上。</p>
<p>起作用的位置是Unet的encoder层，会在每个scale与中间feature相加。</p>
</blockquote>
</li>
<li><p>T2I-Adapter的多条件控制</p>
<blockquote>
<p>无需额外训练，只需要将多个条件的feature加权相加即可。</p>
</blockquote>
</li>
<li><p>T2I-Adapter的采样策略</p>
<blockquote>
<p>实验中发现，将time embedding输入adapter对guidance能力有帮助，但如果每个去噪步都加guidance会过于繁琐。</p>
<p>文中最后发现，在去噪的中后阶段guidance几乎不起作用，大部分内容都是在早期起作用。因此在训练时，当采集的t是中后期的t，则忽略guidance；如果是早期的，则正常加guidance。推理使用DDIM采样时，也采用non-uniform sampling。即增大t落在早期阶段的概率（这里采用了cubic function）</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——Editing系列</title>
    <url>/2024/10/31/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Editing%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="Prompt-to-Prompt"><a href="#Prompt-to-Prompt" class="headerlink" title="Prompt-to-Prompt"></a>Prompt-to-Prompt</h2><ul>
<li><p>Prompt-to-Prompt做的事情</p>
<blockquote>
<p>无需mask，仅通过文字prompt，对生成图片进行编辑，且能够保持除编辑区域之外的一致性。该过程中无需任何训练、微调，只需要调整Attention map。</p>
</blockquote>
</li>
<li><p>Prompt-to-Prompt的动机</p>
<blockquote>
<p>作者发现生成图片的空间布局和几何形状都是由内部的cross-attention层的attention map决定,并在time step的早期这个对应关系就已形成。</p>
<div aligh="center"><img src="/asset/2/p2p.png" class="lazyload" data-srcset="/asset/2/p2p.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
<li><p>Prompt-to-Prompt的控制方式</p>
<blockquote>
<div aligh="center"><img src="/asset/2/p2p_alg.png" class="lazyload" data-srcset="/asset/2/p2p_alg.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>每个时间步t分别计算原始prmopt的attention map M和新的prompt的attention map M*，并用特定规则替换后生成。</p>
</blockquote>
</li>
</ul>
<blockquote>
<ul>
<li>Local Editing<br>如果只想修改部分区域，会在两个attention map中根据attention score进行mask，然后对两部分的并集部分进行attention injection。</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li>Word Swap<div aligh="center"><img src="/asset/2/p2p_word_swap.png" class="lazyload" data-srcset="/asset/2/p2p_word_swap.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></li>
</ul>
<div aligh="center"><img src="/asset/2/p2p_word_swap_2.jpg" class="lazyload" data-srcset="/asset/2/p2p_word_swap_2.jpg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>这个编辑类型是指将原始prompt中的某个token用新的token进行替换。当时间步小于tau时不做替换，否则用原始prompt的attention map做替换。（当两个词的长度不同时，可以对少的进行复制）引入tau的目的是：有一些编辑对图像的几何改变会很大，可以通过引入控制时机tau来缓和.(图片的几何和结构在去噪早期定型，因此在早期使用attention injection来影响编辑后的图片)</p>
</blockquote>
<blockquote>
<ul>
<li>Adding a New Phrase<div aligh="center"><img src="/asset/2/p2p_add.jpg" class="lazyload" data-srcset="/asset/2/p2p_add.jpg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></li>
</ul>
<p>在原始prompt中新增一些token。</p>
</blockquote>
<blockquote>
<ul>
<li>Attention Re-weighting<div aligh="center"><img src="/asset/2/p2p_arw.png" class="lazyload" data-srcset="/asset/2/p2p_arw.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></li>
</ul>
<p>可以精细控制prompt中每个token的控制强度。</p>
</blockquote>
<blockquote>
<p>Prompt-to-Prompt也可以对Real image进行编辑，方式是先做DDIM inversion，然后与上面的方式相同。</p>
</blockquote>
<h2 id="InstructPix2Pix"><a href="#InstructPix2Pix" class="headerlink" title="InstructPix2Pix"></a>InstructPix2Pix</h2><h2 id="Blended-Latent-Diffusion"><a href="#Blended-Latent-Diffusion" class="headerlink" title="Blended Latent Diffusion"></a>Blended Latent Diffusion</h2>]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——Personalization系列</title>
    <url>/2024/10/31/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Personalization%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="Textual-Inversion"><a href="#Textual-Inversion" class="headerlink" title="Textual Inversion"></a>Textual Inversion</h2><ul>
<li><p>Texual Inversion做的事情</p>
<blockquote>
<p>给定3-5张某个主体的图片，冻结T2I模型，通过优化一个pseudo-word的embedding来使得模型学会生成该主体。在2张V100上优化了5000个steps。</p>
</blockquote>
</li>
<li><p>Textual Inversion的优化流程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/textual_inversion.png" class="lazyload" data-srcset="/asset/2/textual_inversion.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>将placeholder的embedding vector设置为可学习的embedding（使用其coarse description的token进行初始化），将输入图片的prompt设置为”A photo of S*”,”A rendition of S*”等。然后使用LDM的reconstruction loss（去噪损失）对该embedding进行优化。（注意，该优化过程中，除了此可学习embedding，其他所有参数冻结）</p>
</blockquote>
</li>
</ul>
<h2 id="DreamBooth"><a href="#DreamBooth" class="headerlink" title="DreamBooth"></a>DreamBooth</h2><ul>
<li><p>DreamBooth做的事情</p>
<blockquote>
<p>给定3-5张某个主体的图片，通过微调T2I模型，使得模型能够生成该主体的图片，同时保持原有类别的生成能力。在单张A100上微调SD约需要5min。</p>
</blockquote>
</li>
<li><p>DreamBooth的微调流程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/dreambooth.png" class="lazyload" data-srcset="/asset/2/dreambooth.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ol>
<li><p>对于输入主体的照片，为其打上标签“a [identifier] [class noun]”，其中identifier是词汇表中的rare tokens，class是一个粗粒度的描述主体类别的词。</p>
</li>
<li><p>接下来微调SD的所有层（实际应用起来，同时微调文本编码器效果会更好），但是这样会造成language drift（忘记原来的类别如何生成），并且生成多样性降低。采取的办法是除了reconstruction loss之外，还使用了class-specific preservation loss。具体做法是：从原始SD中生成一些“A [class noun]”的样本，然后将这些sample加入训练数据来同时微调SD。通过这个loss可以避免few-shot微调之后忘记原本属于该class的信息。</p>
</li>
</ol>
</blockquote>
</li>
</ul>
<h2 id="Custom-Diffusion"><a href="#Custom-Diffusion" class="headerlink" title="Custom Diffusion"></a>Custom Diffusion</h2><ul>
<li><p>Custom Diffusion做的事情</p>
<blockquote>
<p>给定4-5张主体图片，对模型进行较少参数的微调使得SD能够生成对应主体的图片。同时能够支持多主体的定制化生成。</p>
</blockquote>
</li>
<li><p>Custom Diffusion对于单概念的微调</p>
<blockquote>
<div aligh="center"><img src="/asset/2/custom_diffusion.png" class="lazyload" data-srcset="/asset/2/custom_diffusion.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>Custom Diffusion对于模型的微调有三个方面：</p>
<ol>
<li>对于文本编码，会和Textual Inversion一样引入一个新token与对应的mebedding，同时在该token后加上通用类的名字。训练时，会同时优化这个embedding。</li>
<li>对于UNet部分，只微调每个Cross attention中的KV矩阵，这些参数量只占了整个UNet的5%。</li>
<li>对于language drift问题的解决，从LAION-400M中选取了与text prompt高相似度的图片，作为正则集（与DreamBooth类似）。</li>
</ol>
</blockquote>
</li>
<li><p>Custom Diffusion对于多概念的组合</p>
<blockquote>
<ol>
<li>联合训练。将两种概念的数据集直接进行联合训练。</li>
<li>合并优化。在已有两个单概念的模型之后，可以通过优化合成目标的方式，仅优化KV矩阵，得到优化之后的组合概念模型。</li>
</ol>
</blockquote>
</li>
<li><p>Custom Diffusion与Textual Inversion&#x2F;DreamBooth的关系</p>
<blockquote>
<ul>
<li>Custom Diffusion与Textual Inversion类似，会在文本嵌入中加入一个可学习的embedding。</li>
<li>Custom Diffusion与DreamBooth类似，会有正则集来防止language drift，也会微调UNet。区别在于，Custom Diffusion只微调Cross attention中得KV矩阵，而DreamBooth微调所有；Custom Diffusion得正则集是直接选取得，而DreamBooth的是模型提前生成的。</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="BLIP-Diffusion"><a href="#BLIP-Diffusion" class="headerlink" title="BLIP-Diffusion"></a>BLIP-Diffusion</h2><ul>
<li><p>BLIP-Diffusion做的事情？</p>
<blockquote>
<p>给定一张照片，无需finetune得到subject representation，然后用于Personalization；同时，也支持对subject进行few-shot的finetune。</p>
</blockquote>
</li>
<li><p>BLIP-Diffusion的做法</p>
<blockquote>
<p>使用了两阶段的预训练策略，第一阶段使得模型产生text-aligned image representation；第二阶段是subject representation learning的过程。</p>
<div aligh="center"><img src="/asset/2/blip_diffusion.png" class="lazyload" data-srcset="/asset/2/blip_diffusion.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></blockquote>
</li>
</ul>
<blockquote>
<p>Stage 1：Multimodal Representation Learning with BLIP-2<br>第一阶段与BLIP2的预训练一样，使用多目标损失来训练Q-former，区别在于使用的可学习query个数只有16个。</p>
</blockquote>
<blockquote>
<p>Stage 2: Subject Representation Learning</p>
<p>将Q-former的结果经过FFN后，附加在text embedding的最后作为visual prompt。同时，使用的文本prompt template为”[text prompt] ,the [subject text] is [subject prompt]”</p>
<p>同时，为了将subject的学习与background的学习解耦，对于训练数据集的构建，是将图片中的subject保留，背景使用随机背景，然后将改变之后的图输入，原图作为目标。</p>
<p>训练时，冻结image encoder,然后同时训练Q-former、SD的text encoder和U-Net。</p>
</blockquote>
<h2 id="FastComposer"><a href="#FastComposer" class="headerlink" title="FastComposer"></a>FastComposer</h2><ul>
<li><p>FastComposer做的事情？</p>
<blockquote>
<p>给定一张照片，无需test time的微调，能够生成定制化的图片，同时保持一定的文本可编辑性。与Fine-tuning-based方法相比，速度更快且不需要额外存储。同时，FastComposer支持多主体的图像生成。</p>
</blockquote>
</li>
<li><p>FastComposer提出的challenge</p>
<blockquote>
<ul>
<li>之前的Personalization方法大多基于微调，耗时。</li>
<li>多主体生成过程中的Identity Blending问题：当文本中出现多个个体的，他们的ID会因为Attention中注意到text的每个token而出现混杂。</li>
<li>在进行Personalization后，可能出现模型不遵循文本prompt的现象。</li>
</ul>
</blockquote>
</li>
<li><p>FastComposer中对Cross-attention的解释</p>
<blockquote>
<div aligh="center"><img src="/asset/2/fast_ca.png" class="lazyload" data-srcset="/asset/2/fast_ca.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>假设A为cross attention得到的Attention score（即softmax(qk)），则A[i,j,k]表示从k-th token到(i,j)位置的latent pixel的信息流。当某个像素更注意到某个token时，该值应该更大。</p>
</blockquote>
</li>
<li><p>FastComposer的原理</p>
<blockquote>
<div aligh="center"><img src="/asset/2/fast_composer.png" class="lazyload" data-srcset="/asset/2/fast_composer.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></blockquote>
</li>
</ul>
<blockquote>
<p><strong>Tuning-Free Subject-Driven Image Generation</strong></p>
<div aligh="center"><img src="/asset/2/text_embed_aug.png" class="lazyload" data-srcset="/asset/2/text_embed_aug.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>为了达到Tuning-free的目的，这里采用的是augmented的text embedding。<br>具体来说，给定文本Prompt，Reference Image，指定Subject在prompt中的token id，首先将文本和Reference Image分别过CLIP encoders。对于包含主体词的word embedding，将它的embedding和visual feature相连后输入给MLP，得到增强后的文本 embedding。<br>……………………………………..<br>训练时，使用denoising loss训练Image encoder的最后两层，MLP模块以及U-Net。<br>为了训练这一部分，作者创建了一个subject-augmented的图文对数据集，会将主体与对应文本的token对应。同时，训练的时候会将主体的背景使用随机噪声，防止模型对于主体背景的过拟合。</p>
</blockquote>
<blockquote>
<p><strong>Localizing Cross-Attention Maps with Subject Segmentation Masks</strong><br>作者认为，正确的Cross attention map，应该让对应主体的token在主体的instance seg mask处分数高，而不是散布在整张图片。因此，在训练的时候会在Cross attention这里加上一个Regularization项。该项只在训练时使用，推理时不使用。<br>……………………………………..<br>使用的方法是使用reference主体的seg mask来定位cross attention map。A_i&#x3D;[:,:,i]表示第i个subject token的cross attention map,m_j表示第j个subject token。使用m_j来监督A_ij，使用Balanced L1 loss。</p>
<div aligh="center"><img src="/asset/2/loc_loss.png" class="lazyload" data-srcset="/asset/2/loc_loss.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>最终，FastComposer的训练目标由denoising loss和localization loss组成。其中，localization loss只对Unet的中间5个block使用，因为它们包含更多的语义信息。</p>
</blockquote>
<blockquote>
<p><strong>Delayed Subject Conditioning</strong><br>直接使用augmented text representation可能导致图片很像subject但忽视文字引导。这是因为图片的layout是在去噪早期形成的，而一直的subject augmentation会导致模型忽视文本instruction。<br>…………………………………..<br>因此采用延迟主体条件，即只在去噪过程的后期使用augmented text embedding，而在前期layout形成过程中，只是用text-only prompt。这在identity preservation和editability间达到了平衡。</p>
<div aligh="center"><img src="/asset/2/delayed_cond.png" class="lazyload" data-srcset="/asset/2/delayed_cond.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>其中c指原始的text embedding，c’指使用reference image增强过的text embedding。\alpha通常取[0.6,0.8]</p>
</blockquote>
<ul>
<li>FastComposer的流程。<blockquote>
<p>训练过程：使用主体增强的图文对数据集。</p>
<ol>
<li>文本prompt过Text encoder得到text embedding；每个Subject的图片部分过Image Encoder得到Subject image embedding。</li>
<li>将Subject embedding与对应token的embedding相连后过MLP得到增强后的text embedding。</li>
<li>过U-Net，且对中间5个block的cross attention map会使用Localazation loss监督。</li>
<li>对去噪后的图像使用denoising loss。</li>
</ol>
</blockquote>
</li>
</ul>
<blockquote>
<p>推理过程：用户输入主体referene照片，以及文本prompt（主体分先后顺序，使用man img来进行标记subject token）。</p>
<ol>
<li>文本prompt过text encoder得到text embedding；图片过Image encoder得到Subject embedding。</li>
<li>Subject embedding与Subject token embedding相连后过MLP得到增强的text embedding。</li>
<li>输入UNet进行去噪。</li>
<li>若使用Delayed Subject Conditioning，则去噪前期不增强text embedding，使用原始text embedding。</li>
</ol>
</blockquote>
<h2 id="IP-Adapter"><a href="#IP-Adapter" class="headerlink" title="IP-Adapter"></a>IP-Adapter</h2><ul>
<li><p>IP-Adapter做了什么事情？</p>
<blockquote>
<p>IP（Image Prompt），即生成图片时使用图片作为prompt。IP-Adapter可以在冻结原模型的情况下，为模型注入Image prompt，使得生成图片拥有image prompt的内容和风格；并且同时能够保留text prompt的能力。模型轻量化，仅包含22M参数。同时，IP-Adapter也与其他adpater兼容，例如ControlNet。</p>
<p>在推理时，给定一张图片，不需要对模型进行test time微调，能生成与图片内容一致（但ID可能不一致）或者风格的图片。</p>
</blockquote>
</li>
<li><p>IP-Adapter的模型结构</p>
<blockquote>
<div aligh="center"><img src="/asset/2/ip_adapter.png" class="lazyload" data-srcset="/asset/2/ip_adapter.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ol>
<li>包含了一个冻结的CLIP image encoder，一个可训练的linear和LN，提取image promt的feature并将维度调整至与text feature的维度一样。</li>
<li>在SD的Unet的每个cross attention层都会额外添加一个cross attention。这个cross attention的query以及W_q都与text的cross attention一致，不同的部分是kv输入的image prompt，以及W_k,W_v也不同。因此在这一部分，可训练的参数是W_k,W_v。然后将image cross-attention和text cross-attention相加，继续传回Unet。</li>
</ol>
</blockquote>
</li>
<li><p>IP-Adapter的训练与推理</p>
<blockquote>
<p>训练：训练数据使用了大规模图文对数据。可训练模块只有上面所述的模块（Linear，LN,UNet所有Cross attention中的KV矩阵），其中SD始终保持冻结。为了支持CFG，以0.05概率丢掉文本，以0.05概率同时丢掉文本和image prompt。</p>
<p>推理：在text cross-attention和image cross-attention相加时，可以以一定的权重加上image feature。当仅有image prompt时，权重设为1。使用50步的DDIM采样器，guidance scale设为7.5。</p>
</blockquote>
</li>
</ul>
<h2 id="PhotoMaker"><a href="#PhotoMaker" class="headerlink" title="PhotoMaker"></a>PhotoMaker</h2><ul>
<li><p>PhotoMaker做的事情</p>
<blockquote>
<p>给定几张照片，在test time无需微调，生成高保真ID和具有可编辑性的图片。同时，还可以支持ID之间的混合。</p>
</blockquote>
</li>
<li><p>PhotoMaker的做法</p>
<blockquote>
<div aligh="center"><img src="/asset/2/photomaker.png" class="lazyload" data-srcset="/asset/2/photomaker.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ul>
<li><strong>Stacked ID Embedding</strong></li>
</ul>
<p>与FastComposer类似，会将背景加上噪声，减少噪声的影响。将每张图片过CLIP后过Porjection得到stacked ID embedding。这一过程中，可训练参数是CLIP encoder的最后几层以及Projection layer。使用Masked Diffusion loss。</p>
<ul>
<li><strong>Stacking</strong></li>
</ul>
<p>将上面的stacked ID embedding与class word（man，woman）的word embedding过两层MLP得到fused embedding。然后将class word的embedding替换为fused embedding。（注意到这里会将ID embedding和word融合，因此可以用boy和man来生成不同age的）。</p>
<ul>
<li><strong>Merging</strong></li>
</ul>
<p>为了达到更好的ID customization，会在每个cross attention层加上额外的LoRA权重。</p>
</blockquote>
</li>
<li><p>PhotoMaker数据集的构建流程</p>
<blockquote>
<ol>
<li>Image Downloading</li>
<li>Face detection&amp;Filtering</li>
<li>ID Verification</li>
<li>Cropping &amp; Segmentation</li>
<li>Captioning &amp; Marking</li>
</ol>
</blockquote>
</li>
</ul>
<h2 id="InstantID"><a href="#InstantID" class="headerlink" title="InstantID"></a>InstantID</h2><ul>
<li><p>InstantID做的事情</p>
<blockquote>
<p>给定一张照片，在test time无需微调，生成高保真ID和具有可编辑性的图片。同时，不需要训练SD的参数，这使得与其他模块如LoRA，ControlNet等兼容。</p>
</blockquote>
</li>
<li><p>InstantID的做法</p>
<blockquote>
<div aligh="center"><img src="/asset/2/instantid.png" class="lazyload" data-srcset="/asset/2/instantid.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ul>
<li>ID Embedding：IP-Adapter等方法对于图像特征的提取使用CLIP，但这是比较广泛的语义信息，不适合人脸。因此这里使用Face Encoder来提取Face Embedding。</li>
<li>Image Adapter：与IP-Adapter类似，会有一个decoupled cross attention，区别在于这里输入的是ID Embedding。</li>
<li>IdentityNet：利用一个类似ControlNet的结构，输入Input是五个脸部的landmarks（可以使用landmarks控制生成脸部姿态），条件为ID embedding，给生成过程添加空间控制，可以达到高保真的目的。</li>
</ul>
<p>综上，简单来说就是IP-Adapter+Face controlNet。</p>
<p>训练时，只训练Image Adapter、IndentityNet部分，其余部分冻结。使用的是denoising loss。</p>
<p>推理时，可以控制adapter和identitynet的强度（见IP-Adapter）。</p>
</blockquote>
</li>
</ul>
<h2 id="StoryMaker"><a href="#StoryMaker" class="headerlink" title="StoryMaker"></a>StoryMaker</h2><ul>
<li><p>StoryMaker做的事情</p>
<blockquote>
<p>给定一张单个主体或多个主体的照片，无需微调生成能够保持ID的图片，同时保持其衣服、发型的一致性。(Holistic consistency)</p>
</blockquote>
</li>
<li><p>StoryMaker的做法</p>
<blockquote>
<div aligh="center"><img src="/asset/2/storymaker.png" class="lazyload" data-srcset="/asset/2/storymaker.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></blockquote>
</li>
</ul>
<blockquote>
<ul>
<li>Reference Extraction</li>
</ul>
<p>对于输入的reference，会分为脸部和身体进行提取。</p>
<p>对于脸部，使用ArcFace检测脸部并提取facial embeddings。对于衣服、头发等，将characeter切割出来，然后使用CLIP vision encoder进行特征提取。</p>
</blockquote>
<blockquote>
<ul>
<li>Reference Information Refinement by Positional-aware Perceiver Resampler</li>
</ul>
<p>对于两部分feature，会分别有两个resampler进行特征变换（即Linear+LN）。其中脸部的resampler使用IP-Adapter-FaceID初始化，身体的使用IP-Adapter初始化。</p>
<p>之后将两部分concat并使用Position ebedding，以区分不同的characters。同时，为了区分背景，会引入一个可学习的background embedding，concatt到最终的embedding上。具体见图上。</p>
</blockquote>
<blockquote>
<ul>
<li>Decoupled Cross-attention</li>
</ul>
<p>与IP-Adapter一样，会使用decoupled cross-attention将feature注入到Unet中。</p>
</blockquote>
<blockquote>
<ul>
<li>Traing with LoRA</li>
</ul>
<p>为了增强ID的一致性、保真性和质量，和IP-Adapter-FaceID一样，在每个cross attention层会加上额外的LoRA权重进行训练。</p>
</blockquote>
<blockquote>
<ul>
<li>Pose Decoupling from Character Images</li>
</ul>
<p>单独在character图片上训练可能导致网络过拟合到reference的pose。因此将pose从图片中解耦，在训练时候使用Pose-Controlnet，将图片的pose作为条件输入。推理时，可以使用controlnet，也可以不使用。</p>
</blockquote>
<blockquote>
<ul>
<li>Loss Constraints on Cross-attention Maps with Masks</li>
</ul>
<p>为了防止多个主体和背景之间交错现象，使用mask来对cross attention map进行正则化（与FastComposer类似）。这里的做法是对每个主体以及background分别计算cross-attention map loss。（来自Break-a-scene的cross-attention loss）</p>
<div aligh="center"><img src="/asset/2/camap_loss.png" class="lazyload" data-srcset="/asset/2/camap_loss.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>最终训练的loss是denoising loss和Cross attention map loss。</p>
</blockquote>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——DALLE(unCLIP)系列</title>
    <url>/2024/10/31/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94DALLE(unCLIP)%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="DALL-E-1"><a href="#DALL-E-1" class="headerlink" title="DALL-E 1"></a>DALL-E 1</h2><ul>
<li>DALL-E 1的模型架构<blockquote>
<div aligh="center"><img src="/asset/2/dalle.png" class="lazyload" data-srcset="/asset/2/dalle.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>第一阶段，训练一个VQVAE。第二阶段，训练一个自回归的prior模型，这里用的是decoder-only的Transformer（与VQGAN很像，但是这里将文本作为prefix预置条件，实现了Text-conditional生成）</p>
</blockquote>
</li>
</ul>
<h2 id="DALL-E-2-unCLIP"><a href="#DALL-E-2-unCLIP" class="headerlink" title="DALL-E 2(unCLIP)"></a>DALL-E 2(unCLIP)</h2><ul>
<li><p>DALL-E 2的模型结构</p>
<blockquote>
<div aligh="center"><img src="/asset/2/dalle2.png" class="lazyload" data-srcset="/asset/2/dalle2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
共由三部分组成，预训练的CLIP，prior模型，decoder模型。之所以叫unCLIP，是因为这里是和CLIP反过来的，将Image embedding还原成图片。
</blockquote>
</li>
<li><p>DALL-E 2训练过程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/dalle2_train.png" class="lazyload" data-srcset="/asset/2/dalle2_train.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ol>
<li>在图文对上预训练CLIP模型，之后一直是冻结的。</li>
<li>训练Prior模型，由文本embedding还原得到对应的图片embedding。实现上使用了两种结构：（1）自回归Prior。（2）扩散模型prior。最终选用了扩散模型Prior。</li>
<li>训练Decoder模型，由CLIP图片embedding解码为真实的图片（因此叫unCLIP）。生成分辨率为64*64，为了生成高清大图，还训练了两个扩散模型的超分模型，上采样到 256 * 256和1024 * 1024.</li>
</ol>
</blockquote>
</li>
</ul>
<h2 id="DALL-E-3"><a href="#DALL-E-3" class="headerlink" title="DALL-E 3"></a>DALL-E 3</h2><ul>
<li>DALL-E 3主要贡献<blockquote>
<p>对图文数据集recaptioning，即训练一个 captioner 模型，来为图片生成更完整的文本描述。用这种合成的（synthetic）图像文本对数据集，来训练文生图模型，prompt following 能力显著提升。</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——Diffusion及其加速采样系列</title>
    <url>/2024/10/31/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Diffusion%E5%8F%8A%E5%85%B6%E5%8A%A0%E9%80%9F%E9%87%87%E6%A0%B7%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="DDPM"><a href="#DDPM" class="headerlink" title="DDPM"></a>DDPM</h2><blockquote>
<div aligh="center"><img src="/asset/2/ddpm_process.png" class="lazyload" data-srcset="/asset/2/ddpm_process.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
<ul>
<li><p>DDPM的前向加噪过程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/forward_diffusion.jpg" class="lazyload" data-srcset="/asset/2/forward_diffusion.jpg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
</li>
<li><p>DDPM的反向去噪过程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/denoise.jpg" class="lazyload" data-srcset="/asset/2/denoise.jpg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
</li>
<li><p>DDPM的训练过程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/ddpm_train.png" class="lazyload" data-srcset="/asset/2/ddpm_train.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>随机采样时间步t和前向噪声epsilon，用神经网络拟合该epsilon。</p>
</blockquote>
</li>
<li><p>DDPM的采样过程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/ddpm_sample.png" class="lazyload" data-srcset="/asset/2/ddpm_sample.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>首先从高斯分布采样初始噪声xt，然后每一步预测出epsilon来还原x_{t-1}。由于x_{t-1}是有均值方差的高斯分布，因此<br>预测出epsilon还原出均值后，还需要从标准正态分布中随机采样一个z来加上方差。</p>
</blockquote>
</li>
<li><p>DDPM的生成是随机性过程吗？</p>
<blockquote>
<p>是的。在DDPM的反向过程中，每一步都会采样一个噪声，然后利用这个噪声和前一步的输出来生成下一步的图像。这个过程是随机的，因为每一步都会引入新的随机性。</p>
</blockquote>
</li>
</ul>
<h2 id="DDIM"><a href="#DDIM" class="headerlink" title="DDIM"></a>DDIM</h2><ul>
<li><p>DDIM的采样过程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/ddim.jpg" class="lazyload" data-srcset="/asset/2/ddim.jpg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>DDIM的采样过程是非马尔可夫的，且由于eta设置为0，每一步去噪无需重采样高斯噪声（除了第一步采样x_t），因此采样过程是一个确定性过程。</p>
<p>单独DIIM并没有加速，加速使用respacing（按照一定间隔减小采样步数）。只不过respacing对DDPM使用会大幅降低生成效果，但是DDIM不会。所以DDIM+respacing又快又好。</p>
</blockquote>
</li>
<li><p>DDIM的Inversion</p>
<blockquote>
<p>DDIM除了可以确定性去噪外，还可以确定性加噪，从而实现Image editing。</p>
</blockquote>
</li>
<li><p>DDPM与DDIM的区别</p>
<blockquote>
<p>DDPM：固定sample方差为beta或者\hat{beta}。<br>DDIM：固定方差为0，配合respacing实现加速。</p>
</blockquote>
</li>
</ul>
<h2 id="V-Prediction"><a href="#V-Prediction" class="headerlink" title="V-Prediction"></a>V-Prediction</h2><ul>
<li>V-prediction的原理<blockquote>
<p>DDPM中，加噪过程公式为</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><msqrt><mover accent="true"><msub><mi>α</mi><mi>t</mi></msub><mo stretchy="true">‾</mo></mover></msqrt><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><msqrt><mrow><mn>1</mn><mo>−</mo><mover accent="true"><msub><mi>α</mi><mi>t</mi></msub><mo stretchy="true">‾</mo></mover></mrow></msqrt><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">x_t=\sqrt{\overline{\alpha_t}}x_0+\sqrt{1-\overline{\alpha_t}}\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.2147em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8253em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6306em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.5506em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-2.7853em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2147em;"><span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.2078em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8322em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6306em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.5506em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-2.7922em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2078em;"><span></span></span></span></span></span><span class="mord mathnormal">ϵ</span></span></span></span>。

<p>在V-prediction的表示中，该公式被写成</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><msub><mi>α</mi><mi>t</mi></msub><mi>x</mi><mo>+</mo><msub><mi>σ</mi><mi>t</mi></msub><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">x_t=\alpha_tx+\sigma_t\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">ϵ</span></span></span></span>。

<p>这里的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>相当于DDPM中的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mover accent="true"><msub><mi>α</mi><mi>t</mi></msub><mo stretchy="true">‾</mo></mover></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{\overline{\alpha_t}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.2147em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8253em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6306em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.5506em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-2.7853em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2147em;"><span></span></span></span></span></span></span></span></span>，x相对于DDPM中的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>σ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\sigma_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>相当于DDPM中的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mrow><mn>1</mn><mo>−</mo><mover accent="true"><msub><mi>α</mi><mi>t</mi></msub><mo stretchy="true">‾</mo></mover></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{1-\overline{\alpha_t}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.04em;vertical-align:-0.2078em;"></span><span class="mord sqrt"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8322em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord" style="padding-left:0.833em;"><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mord overline"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6306em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.5506em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-2.7922em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2078em;"><span></span></span></span></span></span></span></span></span>。</p>
<p>易知在单位圆上。速度v定义为这条半径的切线方向，也就是其导数：</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo>=</mo><msub><mi>α</mi><mi>t</mi></msub><mi>ϵ</mi><mo>−</mo><msub><mi>σ</mi><mi>t</mi></msub><mi>x</mi></mrow><annotation encoding="application/x-tex">v=\alpha_t\epsilon-\sigma_tx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">ϵ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal">x</span></span></span></span>。

<p>对v的公式和<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>的公式联立，利用<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>α</mi><mi>t</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\alpha_t^2+\sigma_t^2=1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1.0611em;vertical-align:-0.247em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-2.453em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.247em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span>，可得到预测的原图的表达式:</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>=</mo><msub><mi>α</mi><mi>t</mi></msub><msub><mi>z</mi><mi>t</mi></msub><mo>−</mo><msub><mi>σ</mi><mi>t</mi></msub><mover accent="true"><msub><mi>v</mi><mi>θ</mi></msub><mo>^</mo></mover><mo stretchy="false">(</mo><msub><mi>z</mi><mi>t</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\hat{x}=\alpha_tz_t-\sigma_t\hat{v_\theta}(z_t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2222em;"><span class="mord">^</span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，

<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2222em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>为预测的原图，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">z_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.04398em;">z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.044em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为当前时间步的噪声，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mi>t</mi></msub><mtext>和</mtext><msub><mi>σ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_t和\sigma_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0037em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord cjk_fallback">和</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">σ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>参见上面定义，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><msub><mi>v</mi><mi>θ</mi></msub><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{v_\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.25em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span>为去噪网络。</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>在训练阶段，依然使用DDPM给原图加噪，不过模型预测目标是v，并用公式计算出的结果监督。</p>
</blockquote>
<blockquote>
<p>在推理阶段，模型预测出速度v，可以用上边的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.6944em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">x</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.2222em;"><span class="mord">^</span></span></span></span></span></span></span></span></span></span>计算出预测的原图，和xt加权求出xt-1再继续去噪。</p>
</blockquote>
<blockquote>
<p>从v的表达式来看，它是原图x0和噪声的加权求和，因此可以把对速度的预测看作介于直接预测原图和直接预测噪声中间的一个预测目标。</p>
</blockquote>
<h2 id="Score-based"><a href="#Score-based" class="headerlink" title="Score-based"></a>Score-based</h2><h2 id="DPMSolver-DPMSolver"><a href="#DPMSolver-DPMSolver" class="headerlink" title="DPMSolver,DPMSolver++"></a>DPMSolver,DPMSolver++</h2><h2 id="PNDM"><a href="#PNDM" class="headerlink" title="PNDM"></a>PNDM</h2><h2 id="Euler"><a href="#Euler" class="headerlink" title="Euler"></a>Euler</h2><h2 id="UniPC"><a href="#UniPC" class="headerlink" title="UniPC"></a>UniPC</h2>]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——SD 1.x,SD 2.x,SDXL系列</title>
    <url>/2024/10/31/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94SD%201.x,SD%202.x,SDXL%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="SD-1-x"><a href="#SD-1-x" class="headerlink" title="SD 1.x"></a>SD 1.x</h2><ul>
<li><p>SD中的VAE。</p>
<blockquote>
<div aligh="center"><img src="/asset/2/sd_vae.jpeg" class="lazyload" data-srcset="/asset/2/sd_vae.jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
重构损失：L1 loss,Perceptual loss，对抗损失：Patch-based GAN loss,正则化：KL loss.

<p>为了避免latent太过无序，使用正则项对latent space进行规范。原文中使用了两种：（1）KL-reg，但权重设置得很小（过强的正则化会导致生成的图像模糊）；（2）VQ-reg，使用很大的codebook。</p>
<p>SD中的VAE不具有生成功能，主要起的是压缩作用。</p>
<p>H * W * 3 –&gt; H&#x2F;8 * W&#x2F;8 * 4</p>
</blockquote>
</li>
<li><p>SD中VAE与普通VAE的区别</p>
<blockquote>
<div aligh="center"><img src="/asset/2/ldm_vae.png" class="lazyload" data-srcset="/asset/2/ldm_vae.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
这里的KL只是起了规范latent的作用。而且原文中设置的KL设置的极小。（ELBO中过强的正则化会导致生成的图像模糊）
</blockquote>
</li>
<li><p>SD中scale_factor的作用</p>
<blockquote>
<p>见<a href="https://github.com/huggingface/diffusers/issues/437">该issue</a></p>
<p>虽然VAE模型使用了KL正则化，但是由于KL正则化的权重系数非常小，实际生成的Latent特征的标准差依旧存在比较大的情况，因此使用一种rescaling方法强化正则效果。在VAE得到latent之后，需要将latent乘上一个scaling_factor，再送入Unet。这个的作用是：不同Autoencoder训练得到的latent space可能不同，使用scaling_factor将其归一到Unit Variance上，这对于UNet的学习有好处。解码时，将Unet去噪得到的latent除以该scaling_factor。</p>
<p>scaling_factor的计算，是使用数据集中第一个batch数据中Latent特征的标准差。</p>
</blockquote>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># encode and decode the image latents with vae</span></span><br><span class="line"><span class="comment"># encode</span></span><br><span class="line">latents = vae.encode(image).latent_dist.sample()</span><br><span class="line">latents = latents * <span class="number">0.18215</span> <span class="comment"># N（0，1）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 送入Unet，在N(0,1)空间去噪</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># decode </span></span><br><span class="line">latents = <span class="number">1</span> / <span class="number">0.18215</span> * latents</span><br><span class="line">image = vae.decode(latents).sample</span><br></pre></td></tr></table></figure>

<ul>
<li>SD的U-Net结构及与DDPM中U-Net的区别。<blockquote>
<div aligh="center"><img src="/asset/2/sd_unet.jpeg" class="lazyload" data-srcset="/asset/2/sd_unet.jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
其中时间步t在ResBlock中过线性层后与x相加；文本c在SpatialTransformer的cross attention中与flatten的x交互。Unet中的连接方式是直接concat。</blockquote>
</li>
</ul>
<blockquote>
<p>区别：（1）DDPM中是自注意力模块，而SD中替换成Transformer block。<br>(2) DDPM中自注意力仅在较深的几层，而SD中每个层都有Transformer block。</p>
</blockquote>
<ul>
<li><p>SD的训练过程</p>
<blockquote>
<p>训练：首先预训练Autoencoder，然后训练LDM（LDM恢复的是latent）。</p>
</blockquote>
</li>
<li><p>SD的生成结果评估指标</p>
<blockquote>
<p>IS(Inception Score)：<strong>用分类模型评测样本集的“类别确定性”和“类别多样性”，越大越好</strong>。用了一个图像分类网络来评估生成图片的质量。这个分类网络是在 ImageNet 数据集上训的，一共有1000类。</p>
<div aligh="center"><img src="/asset/2/is.png" class="lazyload" data-srcset="/asset/2/is.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ul>
<li>清晰度上：把一张生成的图像喂给分类模型，让模型输出最后一层的1000维向量，如果图像更清晰，那么分类的置信度会很高，其中的某个值会更接近1，其他值更接近0，这个1000维向量的熵就会更低。</li>
<li>多样性上：生成5000张图片给分类模型去分类，如果图像更多样，那么分出来的类型会更均匀，总体接近均匀分布，熵更高。</li>
</ul>
<p>IS指标就是在计算这两个熵的KL散度（可以简单理解为一种距离），图像越清晰、多样性约高，前一个熵越低，后一个熵越高，KL散度越大，所以 IS 指标越大表示生成的效果越好。</p>
<p>缺点： 评测方式的前提是，图像生成所用的训练图片必须得是与分类模型训练数据相同，否则这种评测方式就是有问题的。</p>
</blockquote>
</li>
</ul>
<blockquote>
<p>FID(Fréchet Inception Distance)：<strong>用两个分布的期望和方差，计算两个分布的距离，越小越好</strong>。</p>
<div aligh="center"><img src="/asset/2/fid.png" class="lazyload" data-srcset="/asset/2/fid.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ul>
<li>用一个去掉分类头的分类模型推理出生成图像和真实图像的特征层（一般是2048维）.</li>
<li>假设两个数据的特征层都服从正态分布</li>
<li>用上面这个公式计算两个分布的均值和协方差的“距离”</li>
</ul>
<p>缺点：没法衡量是否过拟合的问题。</p>
</blockquote>
<blockquote>
<p>CLIP score：计算prompt与生成图片的cos相似度。用于评测遵守prompt的程度。</p>
</blockquote>
<blockquote>
<p>R-precision：对提取的图像和文本特征之间的检索结果进行排序，来衡量文本描述和生成的图像之间的视觉语义相似性。</p>
</blockquote>
<h2 id="SD-2-x"><a href="#SD-2-x" class="headerlink" title="SD 2.x"></a>SD 2.x</h2><ul>
<li>SD 2.x与SD 1.x最主要区别<blockquote>
<ul>
<li><p>Text Encoder时使用了更大的CLIP ViT-H&#x2F;14，且使用倒数第二层的特征（SD 1.x使用的是倒数第一层）</p>
</li>
<li><p>SD 2.x使用了v-prediction，见下图</p>
<div aligh="center"><img src="/asset/2/v_pred.png" class="lazyload" data-srcset="/asset/2/v_pred.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="SD-XL"><a href="#SD-XL" class="headerlink" title="SD-XL"></a>SD-XL</h2><ul>
<li><p>SD-XL与SD 1.x的区别</p>
<blockquote>
<ul>
<li><p>增加一个 Refiner 模型，用于对图像进一步地精细化。分辨率可以达到1024x1024。</p>
</li>
<li><p>SDXL-Base使用 CLIP ViT-L 和 OpenCLIP ViT-bigG 两个 text encoder，将两个encoder的倒数第二成embedding相连起来作为text embedding。</p>
</li>
<li><p>训练方式上：（1）以图像大小作为条件。提出将原始图像分辨率作用于 U-Net 模型，并提供图像的原始长和宽（csize &#x3D; (h, w)）作为附加条件。 （2） 以裁剪参数作为条件。将裁剪的左上坐标（top, left）作为条件输入模型，和 size 类似。(3)基于多尺度分辨率训练。</p>
</li>
</ul>
</blockquote>
</li>
<li><p>SD-XL-turbo的改进</p>
<blockquote>
<p>引入蒸馏技术，以便减少 LDM 的生成步数，提升生成速度。</p>
</blockquote>
</li>
</ul>
<blockquote>
<div aligh="center"><img src="/asset/2/sdxl.png" class="lazyload" data-srcset="/asset/2/sdxl.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——iDDPM,ADM,GLIDE,IMAGEN</title>
    <url>/2024/10/31/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94iDDPM,ADM,GLIDE,IMAGEN/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h3 id="IDDPM"><a href="#IDDPM" class="headerlink" title="IDDPM"></a>IDDPM</h3><ul>
<li>IDDPM的原理<blockquote>
<ol>
<li>DDPM中将方差设置为常数。而IDDPM中，提出了可学习的方差（模型会额外输出一个向量用于预测方差），并把它加入到loss中（L_{vlb}），其中L_{vlb}表示变分下界损失（优化时采用重要性采样）。</li>
<li>IDDPM认为DDPM的加噪方式会导致早期阶段加噪过快，因此提出了<strong>cosine schedule</strong>的加噪方式。</li>
</ol>
</blockquote>
</li>
</ul>
<h3 id="ADM（Guided-diffusion）"><a href="#ADM（Guided-diffusion）" class="headerlink" title="ADM（Guided-diffusion）"></a>ADM（Guided-diffusion）</h3><ul>
<li><p>Guided-diffusion模型结构的改进</p>
<blockquote>
<ul>
<li>保持模型大小不变，增加深度，降低宽度</li>
<li>增加 Attention 头的数量</li>
<li>不只是在 16x16 分比率使用 Attention，在 32x32 和 8x8 也使用</li>
<li>在上采样和下采样激活时使用 BigGAN 的 residual block</li>
<li>在 residual connection 中采用 1&#x2F;sqrt(2) 的缩放</li>
</ul>
</blockquote>
</li>
<li><p>ADM使用的条件引导</p>
<blockquote>
<p>使用了Classifier Guidance。</p>
</blockquote>
</li>
</ul>
<h3 id="GLIDE"><a href="#GLIDE" class="headerlink" title="GLIDE"></a>GLIDE</h3><ul>
<li>GLIDE的贡献<blockquote>
<p>将Diffusion用于文本条件图像生成，并比较了两种不同的引导策略：CLIP Guidance和Classifier Free Guidance。</p>
</blockquote>
</li>
</ul>
<h3 id="IMAGEN"><a href="#IMAGEN" class="headerlink" title="IMAGEN"></a>IMAGEN</h3><ul>
<li>IMAGEN的模型结构<blockquote>
<div aligh="center"><img src="/asset/2/imagen.png" class="lazyload" data-srcset="/asset/2/imagen.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>文本编码器使用T5，训练过程中始终冻结。<br>扩散模型级联了三层，第一层去噪生成图像，第二、三层是超分辨率。<br>训练及采样过程中同样使用了CFG。</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——VAE,GAN系列</title>
    <url>/2024/10/31/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94VAE,GAN%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><h3 id="VAE-1"><a href="#VAE-1" class="headerlink" title="VAE"></a>VAE</h3><ul>
<li><p>VAE模型</p>
<blockquote>
<div aligh="center"><img src="/asset/2/vae_1.png" class="lazyload" data-srcset="/asset/2/vae_1.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
<li><p>VAE公式推导</p>
<blockquote>
<p>见<a href="https://zhuanlan.zhihu.com/p/711402258">知乎文章</a>。</p>
</blockquote>
</li>
<li><p>KL项的作用</p>
<blockquote>
<p>KL loss类似于一个正则项，用于规范latent的范围，这也是与AE的最大区别。</p>
</blockquote>
</li>
</ul>
<h3 id="VQ-VAE"><a href="#VQ-VAE" class="headerlink" title="VQ-VAE"></a>VQ-VAE</h3><ul>
<li><p>VQ-VAE模型结构</p>
<blockquote>
<div aligh="center"><img src="/asset/2/vq-vae.png" class="lazyload" data-srcset="/asset/2/vq-vae.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
前向时，Encoder出来的latent会与codebook中的code做最近邻，直接使用codebook中的code作为latent。

<p>VQ-VAE实质上是AE，用来将图像进行压缩。如果要做到图像生成，可以使用PixelCNN来拟合离散的分布。（原文中做法）</p>
</blockquote>
</li>
<li><p>VQ-VAE的优化目标</p>
<blockquote>
<p>有三部分组成：reconstruction loss，VQ loss, commitment loss。</p>
<ul>
<li><p>Reconstruction loss： 反向过程，encoder无法得到反传的梯度，这里采取的方法是将decoder的梯度复制到encoder。具体实现如下图。</p>
<div aligh="center"><img src="/asset/2/vq_recon.png" class="lazyload" data-srcset="/asset/2/vq_recon.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
即 L = x - decoder(z_e+(z_q-z_e).detach())
</li>
<li><p>VQ loss：为了学习codebook，将codebook中的code与encoder的输出拉近。这一部分不反传给encoder。可以使用EMA算法实现。</p>
<div aligh="center"><img src="/asset/2/vq_loss.png" class="lazyload" data-srcset="/asset/2/vq_loss.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</li>
<li><p>Commitment loss：Encoder和codebook的学习速度不同，为了不让encoder的输出偏离codebook太多，使用了commitment loss。这一部分不反传给codebook。</p>
<div aligh="center"><img src="/asset/2/vq_commit.png" class="lazyload" data-srcset="/asset/2/vq_commit.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></li>
</ul>
</blockquote>
</li>
</ul>
<blockquote>
<p>Encoder由Reconstruction loss和commitment loss优化，Decoder由Reconstruction loss优化，Codebook由VQ loss优化。</p>
</blockquote>
<ul>
<li>VAE与VQ-VAE的区别<blockquote>
<p>VAE是假设latent分布为高斯分布；VQ-VAE则是假设latent分布为类别分布。</p>
</blockquote>
</li>
</ul>
<h2 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h2><h3 id="GAN-1"><a href="#GAN-1" class="headerlink" title="GAN"></a>GAN</h3><ul>
<li><p>GAN模型结构</p>
<blockquote>
<p>生成器G,判别器D，先验分布Z（可以采用高斯噪声）。</p>
<div aligh="center"><img src="/asset/2/gan.png" class="lazyload" data-srcset="/asset/2/gan.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
<li><p>GAN的损失函数</p>
<blockquote>
<p>首先考虑判别器D，这是一个二分类问题，那么其损失函数为:</p>
<div aligh="center"><img src="/asset/2/gan_d.png" class="lazyload" data-srcset="/asset/2/gan_d.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
再考虑生成器G。G是要与D唱反调，那么G的目标就是最小化-L_D,即：
<div aligh="center"><img src="/asset/2/gan_g.png" class="lazyload" data-srcset="/asset/2/gan_g.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
最终形成的损失函数，是如下的形式：
<div aligh="center"><img src="/asset/2/gan_loss.png" class="lazyload" data-srcset="/asset/2/gan_loss.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
改进的版本如下：
<div aligh="center"><img src="/asset/2/gan_loss_2.png" class="lazyload" data-srcset="/asset/2/gan_loss_2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
该损失函数可以证明其收敛性。（如果D对于所有的G都能够迅速变化为最优判别器，那么生成器G实际上是在最小化真实数据分布P_r与伪造数据分布P_f之间的JS散度）
</blockquote>
</li>
<li><p>GAN的训练流程</p>
<blockquote>
<p>每一轮更新中，（1）固定G，更新D；（2）固定D，更新G。</p>
<div aligh="center"><img src="/asset/2/gan_train.png" class="lazyload" data-srcset="/asset/2/gan_train.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
</li>
<li><p>GAN的问题</p>
<blockquote>
<ul>
<li>Mode collapse：生成器生成的图像都特别像。可能是数据集过小导致过拟合。</li>
<li>Diminished gradient：判别器太强，导致gradients vanish使得生成器学不到新东西。</li>
<li>不收敛、经常过拟合、对超参数敏感。</li>
</ul>
</blockquote>
</li>
<li><p>WGAN与GAN的区别</p>
<blockquote>
<div aligh="center"><img src="/asset/2/wgan.png" class="lazyload" data-srcset="/asset/2/wgan.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
除此之外还需要做梯度裁剪。</blockquote>
</li>
</ul>
<h3 id="VQ-GAN"><a href="#VQ-GAN" class="headerlink" title="VQ-GAN"></a>VQ-GAN</h3><div aligh="center"><img src="/asset/2/vq_gan.png" class="lazyload" data-srcset="/asset/2/vq_gan.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<ul>
<li><p>VQ-GAN与VQ-VAE的区别</p>
<blockquote>
<ul>
<li>将VQ-VAE中的先验生成器从PixelCNN改成了Transformer。</li>
<li>重建损失中将L2 loss换成了Perceptual loss。</li>
<li>在训练过程中使用PatchGAN的判别器加入对抗损失。</li>
</ul>
</blockquote>
</li>
<li><p>VQ-GAN的GAN部分训练过程</p>
<blockquote>
<p>这一部分学习训练CNN Encoder,CNN Decoder和Codebook。</p>
<p>对于VQ部分，与VQ-VAE类似。将图片H* W* 3经过卷积后得到h* w* n的feature，与codebook进行最近邻后得到新的latent。这里将重建的L2 loss换成了Perceptual loss。</p>
<div aligh="center"><img src="/asset/2/vq_gan_1.png" class="lazyload" data-srcset="/asset/2/vq_gan_1.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
判别器使用了PatchGAN，这部分的loss为：
<div aligh="center"><img src="/asset/2/vq_gan_2.png" class="lazyload" data-srcset="/asset/2/vq_gan_2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
最后的优化目标为：
<div aligh="center"><img src="/asset/2/vq_gan_3.png" class="lazyload" data-srcset="/asset/2/vq_gan_3.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
</li>
<li><p>Perceptual loss是什么？</p>
<blockquote>
<div aligh="center"><img src="/asset/2/per_loss.png" class="lazyload" data-srcset="/asset/2/per_loss.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
</li>
<li><p>VQ-GAN的Transformer部分</p>
<blockquote>
<p>这一部分训练Transformer作为先验分布来自回归地生成图像code，然后送给decoder得到生成的图像。训练过程与GPT训练过程类似，做Next-word-prediction；同时这里还进行了mask，所以不是传统的GPT训练方法。</p>
</blockquote>
</li>
</ul>
<h3 id="Classifier-Guidance"><a href="#Classifier-Guidance" class="headerlink" title="Classifier Guidance"></a>Classifier Guidance</h3><ul>
<li><p>Classifier Guidance的推导</p>
<blockquote>
<div aligh="center"><img src="/asset/2/cls_guidance.jpg" class="lazyload" data-srcset="/asset/2/cls_guidance.jpg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>分类器是需要预训练的，训练数据和扩散模型的训练数据一致，且由于分类过程贯穿整个采样（i.e. t从T到0），因此分类器的训练数据还需要加上不同程度的噪音，来使得该分类器在噪音图片下依然可以起到引导作用。</p>
<p>s是一个大于1的系数。当s较大时，会更关注分类器，生成样本更符合y，但多样性更低，因此是一个保真度与多样性之间的trade-off。</p>
</blockquote>
</li>
<li><p>DDPM中Classifier Guidance采样过程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/cls_guidance_ddpm.png" class="lazyload" data-srcset="/asset/2/cls_guidance_ddpm.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
<li><p>DDIM中Classifier Guidance采样过程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/cls_guidance_ddim.png" class="lazyload" data-srcset="/asset/2/cls_guidance_ddim.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
<li><p>Classifier Guidance的优点和缺点</p>
<blockquote>
<p>优点： 不需要重新训练扩散模型，只需要在采样时根据分类器梯度修改采样的均值即可。</p>
<p>缺点：推理时每一步需要过分类器，速度较慢。</p>
</blockquote>
</li>
</ul>
<h3 id="Classifier-free-Guidance"><a href="#Classifier-free-Guidance" class="headerlink" title="Classifier-free Guidance"></a>Classifier-free Guidance</h3><ul>
<li><p>CFG的推导</p>
<blockquote>
<div aligh="center"><img src="/asset/2/cfg.jpg" class="lazyload" data-srcset="/asset/2/cfg.jpg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
</li>
<li><p>CFG的训练过程</p>
<blockquote>
<p>训练时，Classifier-Free Guidance需要训练两个模型，一个是无条件生成模型，另一个是条件生成模型。但这两个模型可以用同一个模型表示，训练时只需要以一定概率将条件置空即可。</p>
</blockquote>
</li>
<li><p>CFG的推理过程</p>
<blockquote>
<p>推理时，需要同时得到cond和uncond的推理结果，按照上述公式对预测噪声进行线性外推。</p>
<p>其中，uncond也可以用negative prompt代替，这样可以实现不生成某些内容。</p>
</blockquote>
</li>
<li><p>为什么需要两次推理分别得到cond和uncond？</p>
<blockquote>
<p>通过线性外推方式，获得更多的生成多样性。</p>
</blockquote>
</li>
<li><p>CFG的guidance_scale的作用</p>
<blockquote>
<p>一般取7.5。guidance scale越大，生成的结果越倾向于输入条件，图像质量更高，多样性会下降；越小，多样性越大。</p>
</blockquote>
</li>
<li><p>相比于Classifier Guidance的优点</p>
<blockquote>
<ol>
<li>不需要额外训练分类器。</li>
<li>更加灵活，因为它不依赖于特定的分类器，可以处理更广泛的条件。</li>
<li>采样时效率更高。</li>
</ol>
</blockquote>
</li>
<li><p>DPM,DPM++</p>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM论文阅读——BERT系列</title>
    <url>/2024/10/31/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94BERT%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h2><ul>
<li><p>BERT模型结构</p>
<blockquote>
<p>由多个Transformer Encoder堆叠。分为12层的BERT-base和24层的BERT-large。<br>最长序列：512个token,超过的需要截断。<br>Tokenizer: WordPiece。每个句子首个token都是[CLS]，分隔符用[SEP]。会拆分Subword，例如playing拆分为play和###ing。<br>Embedding:见下图</p>
<div aligh="center"><img src="/asset/2/bert_embedding.png" class="lazyload" data-srcset="/asset/2/bert_embedding.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<ul>
<li>Token Embedding：分词后转为词向量。</li>
<li>Segement Embedding：用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务。（在句子对任务中，第一个句子为0，第二个为1；在文本分类中只有一个句子，则全部为0）</li>
<li>Position Embedding：使用可学习的Position Embedding。</li>
</ul>
</blockquote>
</li>
<li><p>BERT预训练任务</p>
<blockquote>
<p>由MLM和NSP两个自监督任务组成。</p>
<ul>
<li><p>Masked Language Modeling(MLM)：在BERT的实验中，15%的WordPiece Token会被随机Mask掉。在训练模型时，一个句子会被多次喂到模型中用于参数学习，但是Google并没有在每次都mask掉这些单词，而是在确定要Mask掉的单词之后，做以下处理：（1）80%的时候会直接替换为[Mask]，将句子 “my dog is cute” 转换为句子 “my dog is [Mask]”。（2）10%的时候将其替换为其它任意单词，将单词 “cute” 替换成另一个随机词，例如 “apple”。将句子 “my dog is cute” 转换为句子 “my dog is apple”。（3）10%的时候会保留原始Token，例如保持句子为 “my dog is cute” 不变。<br>*<div aligh="center"><img src="/asset/2/bert_mlm.png" class="lazyload" data-srcset="/asset/2/bert_mlm.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div></p>
</li>
<li><p>Next Sentence Prediction（NSP）:判断句子B是否是句子A的下文。如果是的话输出’IsNext‘，否则输出’NotNext‘。训练数据的生成方式是从平行语料中随机抽取的连续两句话，其中50%保留抽取的两句话，它们符合IsNext关系，另外50%的第二句话是随机从预料中提取的，它们的关系是NotNext的。</p>
</li>
</ul>
</blockquote>
</li>
<li><p>[CLS]的作用</p>
<blockquote>
<p>BERT在第一句前会加一个[CLS]标志，最后一层该位对应向量可以作为整句话的语义表示，从而用于下游的分类任务等。因为与文本中已有的其它词相比，这个无明显语义信息的符号会更“公平”地融合文本中各个词的语义信息，从而更好的表示整句话的语义。</p>
</blockquote>
</li>
<li><p>BERT的优缺点</p>
<blockquote>
<p>优点：（1）BERT 相较于原来的 RNN、LSTM 可以做到并发执行，同时提取词在句子中的关系特征，并且能在多个不同层次提取关系特征，进而更全面反映句子语义。（2）相较于 word2vec，其又能根据句子上下文获取词义，从而避免歧义出现。</p>
<p>缺点：（1）模型参数太多，而且模型太大，少量数据训练时，容易过拟合。（2）BERT的NSP任务效果不明显，MLM存在和下游任务mismathch的情况。（3）BERT对生成式任务和长序列建模支持不好。</p>
</blockquote>
</li>
<li><p>BERT和GPT区别</p>
<blockquote>
<ul>
<li>训练目标不同：BERT是MLM和NSP；GPT是自回归的LM。</li>
<li>模型结构不同：BERT是Transformer Encoder，双向注意力；GPT是Decoder，单向注意力。</li>
<li>应用场景不同：BERT由于其双向上下文理解能力，BERT在需要理解整个输入序列的任务中表现更好，如问答系统、命名实体识别（NER）和句子对分类；由于其生成能力，GPT在文本生成任务中表现更好，如文本续写、对话系统和文本摘要。</li>
<li>使用方式：BERT通常是pretrain+finetune；；GPT通常是pretrain+prompting。</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="BERT-wwm"><a href="#BERT-wwm" class="headerlink" title="BERT-wwm"></a>BERT-wwm</h2><ul>
<li>BERT-wwm与BERT的区别<blockquote>
<p>BERT在MLM过程中，可能只会mask掉某个Subword；BERT-wwm则是如果Subword被选中mask，那么整个单词都会进行mask。因此是全词掩码（Whole Word Mask）。</p>
</blockquote>
</li>
</ul>
<h2 id="RoBERTa"><a href="#RoBERTa" class="headerlink" title="RoBERTa"></a>RoBERTa</h2><ul>
<li>RoBERTa与BERT的区别<blockquote>
<ul>
<li>更多的预训练语料。</li>
<li>更大的batchsize。</li>
<li>更长的训练步数。</li>
<li>剔除NSP任务。</li>
<li>动态mask：BERT中，对于每一个样本序列进行mask之后，mask的tokens都固定下来了，即是静态mask的方式；而RoBERTa使用了动态mask的方式：对于每一个输入样本序列，都会复制10条，然后复制的每一个都会重新进行mask，即拥有不同的masked tokens。</li>
</ul>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM论文阅读——Attention优化</title>
    <url>/2024/10/31/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Attention%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<blockquote>
<p>Scaled dot-product attention的时间和空间复杂度都是O(n^2)的，当n较大时，会是很大的占用。</p>
</blockquote>
<h2 id="Sparse-attention"><a href="#Sparse-attention" class="headerlink" title="Sparse attention"></a>Sparse attention</h2><blockquote>
<p>将空洞注意力与局部注意力相结合，使得既可以学到局部的特性，又可以学到远程稀疏的相关性。使得大部分元素为0，时间与空间复杂度下降为O(kn)。但是其是对标准attention的近似。</p>
<div aligh="center"><img src="/asset/2/sparse_attn.png" class="lazyload" data-srcset="/asset/2/sparse_attn.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
<h2 id="Flash-attention"><a href="#Flash-attention" class="headerlink" title="Flash attention"></a>Flash attention</h2><ul>
<li><p>Flash Attention v1原理</p>
<blockquote>
<p>原理：减小MAC：使用Tiling技巧将Q,K,V分块后存入SRAM中，使用增量更新的技巧来计算Softmax值以及加权后的V值，再存回HBM，极大地减小了与HBM的IO开销。</p>
<p>大部分的Efficient transformer的目标都是减少FLOPs。Flash Attention的目标是降低MAC(Memory Access cost)，代价是增加了FLOPs. Flash attention是一种精准的优化策略，没有近似损失。</p>
<div aligh="center"><img src="/asset/2/gpu.png" class="lazyload" data-srcset="/asset/2/gpu.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>GPU的存储由SRAM和HBM组成。SRAM的读写速度远大于HBM，但其存储空间远小于HBM。<br>为了减少对HBM的读写，Flash attention将参与计算的矩阵进行分块送进SRAM，来提高整体读写速度。对于Flash attention来说，矩阵分块计算不是难事，重要的是Softmax值的计算——这里采用的是增量计算，具体请参考<a href="https://zhuanlan.zhihu.com/p/642962397">知乎文章</a>及下列伪代码：</p>
<div aligh="center"><img src="/asset/2/flash_attn.png" class="lazyload" data-srcset="/asset/2/flash_attn.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
<li><p>Flash Attention v1与Transformer的MAC分析。</p>
<blockquote>
<div aligh="center"><img src="/asset/2/standard_attn.png" class="lazyload" data-srcset="/asset/2/standard_attn.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
标准Transformer的MAC次数：

<p>第一行，读Q,K的MAC次数位2Nd，写S的MAC次数为N^2。</p>
<p>第二行，读S的MAC次数为N^2，写P的MAC次数为N^2。</p>
<p>第三行，读P的MAC次数为N^2，读V的MAC次数为Nd，写O的MAC次数为Nd。</p>
<p>上述总MAC开销为4Nd+4N^2，复杂度为O(Nd+N^2)。</p>
<p>Flash attention v1的MAC开销：</p>
<p>上述伪代码中，一次完整的内循环需要读取完整的Q，MAC开销为Nd。</p>
<p>外循环的次数为T_c次，即T_c &#x3D; 4dN&#x2F;M，可知开销为O(N^2 * d^2 * M^-1)。因为M(100KB)通常远远大于d（几K），所以Flash attention的MAC远小于标准attention。</p>
<p>Flash attention能够将标准的self-attention的计算速度提升2至4倍。</p>
<div aligh="center"><img src="/asset/2/flash-attn_hf.png" class="lazyload" data-srcset="/asset/2/flash-attn_hf.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>代码见<a href="https://zhuanlan.zhihu.com/p/696850636">此知乎文章</a></p>
</blockquote>
</li>
<li><p>Flash attention如何减少显存？</p>
<blockquote>
<p>与Gradient checkpointing类似，只保存部分中间激活值，在反向传播时重新计算。用时间换空间。</p>
</blockquote>
</li>
</ul>
<h2 id="KV-cache"><a href="#KV-cache" class="headerlink" title="KV cache"></a>KV cache</h2><ul>
<li><p>KV cache的适用范围。</p>
<blockquote>
<p>适用于Decoder-only的LLM的推理过程（每一个token的输出只依赖于它自己以及之前的输出）；并且每次新添加token作为输入后，原token的输入输出不会变。（一旦输入预处理层不满足KVCache的条件，后续transformer层的输入（即预处理层的输出）就发生了改变，也将不再适用于KVCache。）</p>
</blockquote>
</li>
<li><p>KV cache的工作原理。</p>
<blockquote>
<p>思想：以空间换时间，减少重复计算。将FLOPs从O(n2)降低到O(n)。<br>Decoder-only的attention，每次附加上新的token后，下一个token的生成只依赖于当前token的query以及所有token的key和value。因此在推理的自回归过程中，我们只需要计算当前token的qkv，然后将它的kv与之前的kv进行concat后计算attention score即可，其他部分是不变的。</p>
<div aligh="center"><img src="/asset/2/kvcache.png" class="lazyload" data-srcset="/asset/2/kvcache.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
<div aligh="center"><img src="/asset/2/kvcache2.png" class="lazyload" data-srcset="/asset/2/kvcache2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
<li><p>KV cache占用内存大小</p>
<blockquote>
<p>假设Transformer有n_layers层，每个多头注意力层有n_heads个头，维度为d_head，需要为K和V都缓存一份；最大上下文长度为n_context，精度为n_bytes，推理的批量为batch_size。</p>
<p>则KV cache需要的内存大小为2 * n_layers * n_heads * d_head *n_context * n_bytes * batch_size。 </p>
</blockquote>
</li>
<li><p>为什么是KV cache而不是Q cache？</p>
<blockquote>
<p>因为下一个token只取决于最新token的Q和所有token的KV（因果性），每一次用的Q都是最新的。</p>
</blockquote>
</li>
</ul>
<h2 id="Multi-Query-Attention-MQA-与Group-Query-Attention-GQA"><a href="#Multi-Query-Attention-MQA-与Group-Query-Attention-GQA" class="headerlink" title="Multi Query Attention(MQA)与Group Query Attention(GQA)"></a>Multi Query Attention(MQA)与Group Query Attention(GQA)</h2><ul>
<li><p>MQA的动机及原理</p>
<blockquote>
<p>动机：KV cache中对于多头，每个头都会有一个K,V矩阵。因此KV cache较大。</p>
<p>原理：MQA让所有头之间共享同一份K和V矩阵，从而大大减少KV参数量和cache量。这样在decoder上推理时，可以大大减少KV cache大小。能提高 30%-40% 的吞吐。</p>
</blockquote>
</li>
<li><p>GQA的原理</p>
<blockquote>
<p>将query分为N组，每个组共享一个K和V矩阵。</p>
<div aligh="center"><img src="/asset/2/mqa_gqa.png" class="lazyload" data-srcset="/asset/2/mqa_gqa.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
<li><p>MQA和GQA的共同原理</p>
<blockquote>
<ol>
<li><p>降低了从内存中读取的数据量，所以也就减少了计算单元等待时间，提高了计算利用率；</p>
</li>
<li><p>KV cache 变小了 head_num 倍，也就是显存中需要保存的 tensor 变小了，空出来空间就可以加大 batch size，从而又能提高利用率。</p>
</li>
</ol>
</blockquote>
</li>
<li><p>手撕MQA和GQA</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GroupQueryAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, heads, d_model,group_num</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line">        <span class="variable language_">self</span>.d_k = d_model // heads  <span class="comment"># 每个“头”对应的维度</span></span><br><span class="line">        <span class="variable language_">self</span>.h = heads  <span class="comment"># “头”的数量</span></span><br><span class="line">        <span class="variable language_">self</span>.group_num = group_num <span class="comment"># 分组数,当group_num=1时，为MQA</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化线性层，用于生成Q，K，V</span></span><br><span class="line">        <span class="variable language_">self</span>.q_linear = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.k_linear = nn.Linear(d_model, <span class="variable language_">self</span>.d_k*group_num)</span><br><span class="line">        <span class="variable language_">self</span>.v_linear = nn.Linear(d_model, <span class="variable language_">self</span>.d_k*group_num)</span><br><span class="line">        <span class="comment"># 输出线性层</span></span><br><span class="line">        <span class="variable language_">self</span>.out = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># q,k,v [...,d]</span></span><br><span class="line">        <span class="comment"># 计算点积，并通过 sqrt(d_k) 进行缩放</span></span><br><span class="line">        scores = torch.matmul(q, k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(<span class="variable language_">self</span>.d_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果有 mask，应用于 scores</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对 scores 应用 softmax</span></span><br><span class="line">        scores = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 获取输出</span></span><br><span class="line">        output = torch.matmul(scores, v)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">split_head</span>(<span class="params">self,x,group_num=<span class="literal">None</span></span>):</span><br><span class="line">        batch_size, seq_len = x.size()[:<span class="number">2</span>]  <span class="comment"># 获取批量大小和序列长度</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> group_num <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> x.reshape(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.num_heads, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 将 hidden_size 分割为 group_num 和 head_dim</span></span><br><span class="line">            x = x.reshape(batch_size, -<span class="number">1</span>, group_num, <span class="variable language_">self</span>.head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            <span class="comment"># 再将其手动 expand 到相同大小</span></span><br><span class="line">            x = x[:, :, <span class="literal">None</span>, :, :].expand(batch_size, group_num, <span class="variable language_">self</span>.num_heads // group_num, seq_len, <span class="variable language_">self</span>.head_dim).reshape(batch_size, <span class="variable language_">self</span>.num_heads, seq_len, <span class="variable language_">self</span>.head_dim)</span><br><span class="line">            <span class="keyword">return</span> x 	<span class="comment"># 形状: (batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># q,k,v:[B,T,d]</span></span><br><span class="line">        batch_size = q.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对 q，k，v 进行线性变换</span></span><br><span class="line">        q = <span class="variable language_">self</span>.q_linear(q) <span class="comment"># [B,T,d_model]</span></span><br><span class="line">        k = <span class="variable language_">self</span>.k_linear(k) <span class="comment"># [B,T,d_k*group_num]</span></span><br><span class="line">        v = <span class="variable language_">self</span>.v_linear(v) <span class="comment"># [B,T,d_k*group_num]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 分组</span></span><br><span class="line">        q = <span class="variable language_">self</span>.split_head(q) <span class="comment"># [B,num_heads,T,head_dim]</span></span><br><span class="line">        k = <span class="variable language_">self</span>.split_head(k,group_num=<span class="variable language_">self</span>.group_num) <span class="comment"># [B,num_heads,T,head_dim]</span></span><br><span class="line">        v = <span class="variable language_">self</span>.split_head(v,group_num=<span class="variable language_">self</span>.group_num) <span class="comment"># [B,num_heads,T,head_dim]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行多头注意力计算</span></span><br><span class="line">        scores = <span class="variable language_">self</span>.attention(q, k, v, mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将多个头的输出拼接回单个张量</span></span><br><span class="line">        concat = scores.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.d_model)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过输出线性层</span></span><br><span class="line">        output = <span class="variable language_">self</span>.out(concat)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure></li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM论文阅读——GPT系列</title>
    <url>/2024/10/31/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94GPT%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="GPT-1"><a href="#GPT-1" class="headerlink" title="GPT-1"></a>GPT-1</h2><ul>
<li><p>简述一下GPT的训练过程。</p>
<blockquote>
<p>GPT的训练过程采用了预训练和微调的二段式训练策略。</p>
<ul>
<li>非监督式预训练： 利用大规模无标记语料，构建预训练单向语言模型。训练目标是Language Modeling loss。</li>
<li>监督式微调： 用预训练的结果作为下游任务的初始化参数，增加一个线性层，匹配下游任务。训练目标是有监督的目标函数，并加上Language Modeling作为辅助目标。</li>
</ul>
</blockquote>
</li>
<li><p>GPT的Decoder与Transformer Decoder的区别</p>
<blockquote>
<ul>
<li>激活函数为GELU</li>
<li>位置编码为Learning Position embedding</li>
<li>去除了Cross attention。</li>
<li>Tokenizer采用的是BPE。</li>
</ul>
</blockquote>
</li>
<li><p>为什么GPT是Decoder-only？</p>
<blockquote>
<p>Transformer 结构提出是用于机器翻译任务，机器翻译是一个Seq2Seq的任务，因此 Transformer 设计了Encoder 用于提取源端语言的语义特征，而用 Decoder 提取目标端语言的语义特征，并生成相对应的译文。GPT目标是服务于单序列文本的生成式任务，所以舍弃了关于 Encoder部分以及包括 Decoder 的 Cross Attention 层。</p>
</blockquote>
</li>
</ul>
<h2 id="GPT-2"><a href="#GPT-2" class="headerlink" title="GPT-2"></a>GPT-2</h2><ul>
<li>GPT-2与GPT的区别？<blockquote>
<p>主推zero-shot，而GPT-1为pre-train+fine-tuning。<br>模型更大。参数量达到1.5B，而GPT只有0.117B.<br>数据集更大。<br>训练参数变化，batch_size 从 64 增加到 512，上文窗口大小从 512 增加到 1024。<br>模型结构变化：</p>
<ul>
<li>后置层归一化（ post-norm ）改为前置层归一化（ pre-norm ）;</li>
<li>在模型最后一个自注意力层之后，额外增加一个层归一化;</li>
<li>调整参数的初始化方式，按残差层个数进行缩放，缩放比例为;</li>
<li>输入序列的最大长度从 512 扩充到 1024;<div aligh="center"><img src="/asset/2/diff_gpt.png" class="lazyload" data-srcset="/asset/2/diff_gpt.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div></li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="GPT-3"><a href="#GPT-3" class="headerlink" title="GPT-3"></a>GPT-3</h2><ul>
<li>GPT-3与GPT-2区别？<blockquote>
<p>GPT-2虽然提出zero-shot，比bert有新意，但是有效性方面不佳。GPT-3考虑few-shot，用少量文本提升有效性。<br>模型结构：</p>
<ul>
<li>大部分和GPT-2一样，但应用了Sparse attention。<br>论文尝试了四种方式的评估方法：</li>
<li>fine-tuning：预训练 + 训练样本计算loss更新梯度，然后预测。<strong>会更新模型参数</strong>.</li>
<li>zero-shot：预训练 + task description + prompt，直接预测。不更新模型参数.</li>
<li>one-shot：预训练 + task description + example + prompt，预测。不更新模型参数.</li>
<li>few-shot（又称为in-context learning）：预训练 + task description + examples + prompt，预测。<strong>不更新模型参数</strong>.</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="Instruct-GPT"><a href="#Instruct-GPT" class="headerlink" title="Instruct-GPT"></a>Instruct-GPT</h2><ul>
<li>介绍一下InstructGPT。<blockquote>
<div aligh="center"><img src="/asset/2/instructgpt.png" class="lazyload" data-srcset="/asset/2/instructgpt.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
为了和人类的需求对齐——
主要由三个阶段组成 ：
(1) SFT(Supervised Fine-tuning)：收集一系列人工标注的(Question,Response)作为数据集，使用LM目标函数监督学习微调GPT-3（16个epoch）。根据验证集上的RM分数，选择最终的SFT模型。
(2) RM(Reward Modeling)：RM是训练一个Reward Model，将SFT模型最后的嵌入层去掉后的模型，它的输入是prompt和response，输出是标量的奖励值。奖励模型的损失函数如下，这里使用的是排序中常见的pairwise ranking loss。这是因为人工标注的是答案的顺序，而不是分数，所以中间需要转换一下。
(3) RL(PPO)：训练RL policy，即之前SFT过的GPT-3。用SFT的GPT-3输出Response，用上一步训练的Reward Model输出标量作为Reward，来训练这个RL policy。为了确保输出的质量不降低，有时也会在PPO的目标函数之外额外加上加权的LM目标函数。</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM论文阅读——LLAMA系列</title>
    <url>/2024/10/31/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94LLAMA%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>


<h2 id="LLAMA"><a href="#LLAMA" class="headerlink" title="LLAMA"></a>LLAMA</h2><ul>
<li><p>LLaMa介绍</p>
<blockquote>
<p>预训练数据：全是开源数据。<br>Tokenizer: BPE implemented by SentecePiece，分词后训练集中共包含1.4T tokens。<br>模型架构：相比与原始Transformer架构，有如下改动</p>
<ul>
<li>Pre-norm：参考GPT3，在transformer sub-layer前进行norm。使用的是RMSNorm。</li>
<li>SwiGLU: 参考PaLM,使用SwiGLU作为激活函数。</li>
<li>RoPE: 将PE换成了RoPE。</li>
<li>在casual MHA这里使用了更efficient的实现方式。</li>
<li>保存线性层输出的activation。<br>Optimizer: AdamW，同时用CosLR schedule。同时有0.1的weight decay和1.0的grad clip。有2000 steps的warm up。<br>在2048台A100-80GB上训练了21天。</li>
</ul>
</blockquote>
</li>
<li><p>LLAMA中的RMSNorm</p>
<blockquote>
<p>RMSNorm(Root Mean Square Layer Normalization)<br>提出动机：LayerNorm计算量比较大。<br>优点：（1）计算效率高。不需要同时计算均值和方差两个统计量，而只需要计算均方根这一个统计量（没有去中心化操作）。同时也减少了Norm中的参数量（只有lambda一个参数）。 （2）稳定性好。能缓解梯度消失和梯度爆炸。</p>
<div aligh="center"><img src="/asset/2/rmsnorm.png" class="lazyload" data-srcset="/asset/2/rmsnorm.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div></blockquote>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaRMSNorm</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        LlamaRMSNorm is equivalent to T5LayerNorm</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.weight = nn.Parameter(torch.ones(hidden_size))</span><br><span class="line">        <span class="variable language_">self</span>.variance_epsilon = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states</span>):</span><br><span class="line">        input_dtype = hidden_states.dtype</span><br><span class="line">        hidden_states = hidden_states.to(torch.float32)</span><br><span class="line">        variance = hidden_states.<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        hidden_states = hidden_states * torch.rsqrt(variance + <span class="variable language_">self</span>.variance_epsilon)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.weight * hidden_states.to(input_dtype)</span><br></pre></td></tr></table></figure>

<ul>
<li>LLAMA的loss函数<blockquote>
<p>Language Modeling，即自回归预测next word的概率，实现方式为Cross Entropy。<br>也会有其他的预训练损失函数。</p>
</blockquote>
</li>
</ul>
<h2 id="LLAMA2"><a href="#LLAMA2" class="headerlink" title="LLAMA2"></a>LLAMA2</h2><ul>
<li>LLAMA2的改进<blockquote>
<div aligh="center"><img src="/asset/2/llama2.png" class="lazyload" data-srcset="/asset/2/llama2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
* 预训练语料扩充到2T token。
* 上下文长度从2048翻倍到4096.
* 引入了Grouped-query attention、KV cache等技术
* 在LLAMA2基础上进一步SFT（Supervised Fine-tuning）和RLHF，得到LLAMA2-Chat。
<div aligh="center"><img src="/asset/2/llama2_ft.png" class="lazyload" data-srcset="/asset/2/llama2_ft.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div></blockquote>
</li>
</ul>
<h2 id="LLAMA3"><a href="#LLAMA3" class="headerlink" title="LLAMA3"></a>LLAMA3</h2><ul>
<li>LLAMA3的改进<blockquote>
<ul>
<li>训练数据集比LLAMA2大7倍。</li>
<li>采用了数据并行、模型并行、管道并行等并行化技术。</li>
<li>结合了SFT，Rejection sampling、PPO、DPO对预训练模型进行指令微调。</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="Vicuna"><a href="#Vicuna" class="headerlink" title="Vicuna"></a>Vicuna</h2><blockquote>
<p>Vicuna是在LLaMa-13B的基础上使用监督数据微调得到的模型，数据集来自于ShareGPT.com 产生的用户对话数据，共70K条。</p>
<p>Vicuna在训练中将序列长度由512扩展到了2048，并且通过梯度检测和flash attention来解决内存问题；调整训练损失考虑多轮对话，并仅根据模型的输出进行微调。</p>
</blockquote>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM论文阅读——Parameter-efficition finetuning(PEFT)系列</title>
    <url>/2024/10/31/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Parameter-efficition%20finetuning(PEFT)%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h2><ul>
<li><p>LoRA原理</p>
<blockquote>
<p>LoRA假设微调变化矩阵的内在秩远低于原矩阵维度d，因此将变化矩阵分解为B和A，而原矩阵的权重不发生变化。这样使得可训练参数数量极大减少，降低显存消耗量。见下图：</p>
<div aligh="center"><img src="/asset/2/lora.png" class="lazyload" data-srcset="/asset/2/lora.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
初始时将A矩阵高斯随机初始化，将B矩阵初始化为0，这样变化矩阵在开始训练时是0。还需要将<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>W</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">\Delta Wx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Δ</span><span class="mord mathnormal" style="margin-right:0.13889em;">W</span><span class="mord mathnormal">x</span></span></span></span>进行scale: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mi>α</mi><mi>r</mi></mfrac></mrow><annotation encoding="application/x-tex">\frac{\alpha}{r}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0404em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.6954em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.0037em;">α</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，其中分子为r中的一个常数，r为选取的秩。</blockquote>
</li>
<li><p>LoRA推理过程中有额外计算吗？</p>
<blockquote>
<p>LoRA在推理时通常会将变化矩阵加在原矩阵上，这样并没有额外的计算开销，因此没有额外计算。</p>
</blockquote>
</li>
<li><p>Lora初始化方式</p>
<blockquote>
<p>A矩阵进行高斯分布初始化，B矩阵初始化为0.</p>
</blockquote>
</li>
<li><p>LoRA应用于网络的哪些部分？</p>
<blockquote>
<p>在Transformer中，可以应用于Attention中的Q,K,V矩阵和线性层，以及FFN中的两个MLP模块。能够大大降低微调参数量。通常用在Attention的q和v效果最好。</p>
<div aligh="center"><img src="/asset/2/lora_place.png" class="lazyload" data-srcset="/asset/2/lora_place.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
</li>
<li><p>LoRA的r一般选取多少？</p>
<blockquote>
<p>对于一般的任务，r&#x3D;1,2,4,8 就足够了。而一些领域差距比较大的任务可能需要更大的r 。</p>
</blockquote>
</li>
</ul>
<h2 id="QLoRA"><a href="#QLoRA" class="headerlink" title="QLoRA"></a>QLoRA</h2><h2 id="Adapter-tuning"><a href="#Adapter-tuning" class="headerlink" title="Adapter tuning"></a>Adapter tuning</h2><ul>
<li><p>Adapter tuning原理</p>
<blockquote>
<p>在模型内部外置一些轻量级的层，通过训练这些层来适应新的数据变化。下图中为Bottleneck adapter。</p>
<div aligh="center"><img src="/asset/2/adapter.png" class="lazyload" data-srcset="/asset/2/adapter.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
</li>
<li><p>与LoRA的区别</p>
<blockquote>
<p>Adapter会有新的模型层和参数；LoRA训练得到的部分最后会合并回原模型。</p>
</blockquote>
</li>
</ul>
<h2 id="Prefix-tuning"><a href="#Prefix-tuning" class="headerlink" title="Prefix tuning"></a>Prefix tuning</h2><ul>
<li>Prefix tuning原理<blockquote>
<p>主要适配NLG任务。<br>与Full-finetuning更新所有参数的方式不同，该方法是在输入token之前构造一段任务相关的virtual tokens作为Prefix，然后训练的时候只更新Prefix部分的参数，而Transformer中的其他部分参数固定。(该方法其实和构造Prompt类似，只是Prompt是人为构造的“显式”的提示,并且无法更新参数，而Prefix则是可以学习的“隐式”的提示。)同时，为了防止直接更新Prefix的参数导致训练不稳定的情况，他们在Prefix层前面加了MLP结构(相当于将Prefix分解为更小维度的Input与MLP的组合后输出的结果)，训练完成后，只保留Prefix的参数。在每个Transformer layer前都会有prefix。</p>
<div aligh="center"><img src="/asset/2/prefix_2.jpeg" class="lazyload" data-srcset="/asset/2/prefix_2.jpeg" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<div aligh="center"><img src="/asset/2/prefix.png" class="lazyload" data-srcset="/asset/2/prefix.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<div aligh="center"><img src="/asset/2/prefix_3.png" class="lazyload" data-srcset="/asset/2/prefix_3.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
主要适配NLG任务。</blockquote>
</li>
</ul>
<h2 id="Prompt-tuning"><a href="#Prompt-tuning" class="headerlink" title="Prompt tuning"></a>Prompt tuning</h2><ul>
<li>Prompt tuning原理<blockquote>
<p>该方法可以看作是 Prefix Tuning 的简化版本，它给每个任务定义了自己的Prompt，然后拼接到数据上作为输入，但只在输入层加入prompt tokens，并且不需要加入 MLP 进行调整来解决难训练的问题。 它在预训练语言模型的输入中添加可学习的嵌入向量作为提示。这些提示被设计成在训练过程中更新，以引导模型输出对特定任务更有用的响应。同时，Prompt Tuning 还提出了 Prompt Ensembling，也就是在一个批次（Batch）里同时训练同一个任务的不同 prompt（即采用多种不同方式询问同一个问题），这样相当于训练了不同模型。</p>
<div aligh="center"><img src="/asset/2/prompt_tuning.png" class="lazyload" data-srcset="/asset/2/prompt_tuning.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
<div aligh="center"><img src="/asset/2/prompt_ensemble.png" class="lazyload" data-srcset="/asset/2/prompt_ensemble.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div></blockquote>
</li>
</ul>
<h2 id="P-tuning"><a href="#P-tuning" class="headerlink" title="P-tuning"></a>P-tuning</h2><ul>
<li><p>P-tuning V1原理</p>
<blockquote>
<p>目的：使GPT适配NLU任务;避免人工设计prompt。<br>做法：该方法将Prompt转换为可以学习的Embedding层，并用MLP+LSTM的方式来对Prompt Embedding进行一层处理。相比Prefix Tuning，P-Tuning加入的可微的virtual token，但仅限于输入层，没有在每一层都加；另外，virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual token。</p>
<div aligh="center"><img src="/asset/2/p-tuning.png" class="lazyload" data-srcset="/asset/2/p-tuning.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
</li>
<li><p>P-tuning V2原理</p>
<blockquote>
<p>方法：该方法在每一层都加入了Prompts tokens作为输入，而不是仅仅加在输入层，这带来两个方面的好处：（1）更多可学习的参数（从P-tuning和Prompt Tuning的0.01%增加到0.1%-3%），同时也足够参数高效。（2）加入到更深层结构中的Prompt能给模型预测带来更直接的影响（如Prefix tuning）.具体做法基本同Prefix Tuning，可以看作是将文本生成的Prefix Tuning技术适配到NLU任务中.</p>
<div aligh="center"><img src="/asset/2/p-tuning_v2.png" class="lazyload" data-srcset="/asset/2/p-tuning_v2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
<li><p>P-tuning V1&#x2F;V2的区别。</p>
<blockquote>
<p>参见上图。可以简单的将 P-Tuning 认为是针对 Prompt Tuning 的改进， P-Tuning v2 认为是针对 Prefix Tuning 的改进。</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM论文阅读——Qwen系列</title>
    <url>/2024/10/31/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Qwen%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="Qwen"><a href="#Qwen" class="headerlink" title="Qwen"></a>Qwen</h2><ul>
<li><p>Qwen的技术要点</p>
<blockquote>
<ul>
<li>数据：公共网络文档，百科全书，书籍，代码等。此外，数据集是多语言的，其中很大一部分数据是英语和中文的。最终数据集多达3万亿token。</li>
<li>Tokenizer：采用开源的BPE，并以vocabulary C1100K base作为起始点，增加了常用的中文字符和单词以及其他语言的词汇。此外，仿照LLAMA2，将数字分为单个数字，最终词汇量大约是152K.</li>
<li>模型结构：<br>(1) Embedding和Output project：解耦嵌入。<br>(2) Positional embedding： RoPE，使用FP32的逆频率矩阵。<br>(3) Bias，除了Attention中的QKV，其他层的bias都去除，以增强外推能力，并防止过拟合。<br>(4) Pre-Norm &amp; RMSNorm<br>(5) Activation：选用SwiGLU。</li>
<li>对于文本长度的拓展，使用了Dynamic NTK-aware interpolation，可以在无需训练的方法下调整尺度以防止高频信息的丢失，有效地扩展Transformer模型的上下文长度。<div aligh="center"><img src="/asset/2/qwen.png" class="lazyload" data-srcset="/asset/2/qwen.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div></li>
</ul>
</blockquote>
</li>
<li><p>Qwen的预训练</p>
<blockquote>
<ul>
<li>采用标准的自回归语言模型训练目标</li>
<li>训练时上下文长度为2048</li>
<li>注意力模块采用Flash Attention技术，以提高计算效率并减少内存使用</li>
<li>采用AdamW优化器，设置β1&#x3D;0.9，β2&#x3D;0.95,，&#x3D;1e-8</li>
<li>使用余弦学习率计划，为每种模型设定一个峰值学习率，学习率会衰减到峰值的10%</li>
<li>使用BFloat16混合精度加速训练</li>
</ul>
</blockquote>
</li>
<li><p>Qwen的对齐</p>
<blockquote>
<p>SFT:采用ChatML格式的数据进行模型训练。<br>RM：偏好模型预训练和微调<br>PPO：使用RM进行RLHF。</p>
</blockquote>
</li>
</ul>
<h2 id="Qwen-2"><a href="#Qwen-2" class="headerlink" title="Qwen-2"></a>Qwen-2</h2><ul>
<li>Qwen2相对于Qwen的改进<blockquote>
<ul>
<li>多种模型规模</li>
<li>多语言支持：在训练数据中增加了27种语言的高质量数据，增强了模型的多语言能力。</li>
<li>代码和数学能力</li>
<li>长文本处理：Qwen2模型能够处理更长的上下文，最高可达128K tokens。</li>
<li>架构创新：引入了Group Query Attention。</li>
</ul>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM论文阅读——Tokenizer系列</title>
    <url>/2024/10/31/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Tokenizer%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<ul>
<li>Tokenizer的种类和区别<blockquote>
<p>Tokenize有三种粒度：</p>
<ol>
<li>Word：对于英文等自然语言来说，存在着天然的分隔符；但是对于一些东亚文字包括中文来说，就需要某种分词算法才行。由于长尾现象的存在，词汇表可能会超大。</li>
<li>Char：词汇表只有最基本的字符。这样做的问题是，由于字符数量太小，我们在为每个字符学习嵌入向量的时候，每个向量就容纳了太多的语义在内，学习起来非常困难。</li>
<li>Subword：介于字符和单词之间，平衡了词汇量和语义独立性。处理原则是，常用词应该保持原状，生僻词应该拆分成子词以共享token压缩空间。</li>
</ol>
</blockquote>
</li>
</ul>
<blockquote>
<p>常用的BPE、WordPiece等都属于Subword的tokenization。</p>
</blockquote>
<h2 id="BPE-Byte-Pair-Encoding"><a href="#BPE-Byte-Pair-Encoding" class="headerlink" title="BPE(Byte Pair Encoding)"></a>BPE(Byte Pair Encoding)</h2><ul>
<li><p>BPE的原理</p>
<blockquote>
<p>BPE每一步都将最常见的一对相邻数据单位替换为该数据中没有出现过的一个新单位，反复迭代直到满足停止条件。</p>
<ol>
<li>首先Pre-tokenization得到所有单词及频次的集合，初始的vocabulary为基础字符。（每个单词末尾会添加&lt;/w&gt;来区分词尾和词中）</li>
<li>挑选出频次最高的vocab pair，将其合并然后加入到vocabulary中构成新token。</li>
<li>将（单词，频次）集合中的单词使用vocabulary更新。</li>
<li>重复上述2-3，直到vocab达到要求。</li>
</ol>
</blockquote>
</li>
<li><p>BPE的编码与解码</p>
<blockquote>
<p>编码：对于每个空格分开的单词使用merge rule进行贪婪匹配。编码时对于没见过的用<unk> token代替，BPE 只会将不在词汇表中的单个字符 tokenize 为 unknown 。 例如 “bum”，由于”##m” 不在词表中，由此产生的tokenization 是 [“b”, “##u”, “[UNK]”]。</p>
<p>解码：对于每个token，根据是否有&lt;/w&gt;加上空格，然后进行拼接。</p>
</blockquote>
</li>
<li><p>Byte-level BPE</p>
<blockquote>
<p>BPE的一个问题是，如果遇到了unicode，基本字符集可能会很大。一种处理方法是我们以一个字节为一种“字符”，不管实际字符集用了几个字节来表示一个字符。这样的话，基础字符集的大小就锁定在了256。</p>
<p>例如，像GPT-2的词汇表大小为50257 &#x3D; 256 + <EOS> + 50000 mergers，<EOS>是句子结尾的特殊标记。</p>
</blockquote>
</li>
</ul>
<h2 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h2><p>很多著名的Transformer模型如BERT&#x2F;DistilBERT都使用了它。</p>
<ul>
<li><p>WordPiece的原理。</p>
<blockquote>
<p>与BPE一样，WordPiece从一个小的vocab开始学习merge规则。二者的区别在于：WordPiece不是选择最高频的pair，而是选择最大化如下score的一对token。</p>
<p>score(t1,t2) &#x3D; freq(t12) &#x2F; (freq(t1)*freq(t2))，freq(t)为t在语料库中出现的频次。<br>WordPiece的物理意义为：通过将t1和t2合并为t12后，语料库的对数似然的增量最大化。<br>具体步骤与BPE相似。注意：注意：WordPiece 通过添加前缀（在 BERT 中是 ##）来识别子词，这可以识别一个子词是否是单词的开始。这里通过将前缀添加到单词内的每个字符来拆分的，单词的首字符不添加前缀。</p>
</blockquote>
</li>
<li><p>WordPiece的编码和解码</p>
<blockquote>
<p>编码： 找到词表中能够匹配到的最长的子词，然后对单词进行拆分。当tokenization 无法在词表中找到子词时，整个单词被 tokenize 为 unknown 。 例如 “bum”，由于”##m” 不在词表中，由此产生的tokenization 将只是 [“[UNK]”], 不是 [“b”, “##u”, “[UNK]”]。<br>解码：与BPE类似，对于有前缀的合并成一个单词，无前缀的加上空格。</p>
</blockquote>
</li>
<li><p>WordPiece与BPE的区别</p>
<blockquote>
<ol>
<li>WordPiece 仅保存最终词表，而不保存学到的 merge rule 。</li>
<li>对于Unknown字符的处理不同。</li>
</ol>
</blockquote>
</li>
</ul>
<h2 id="SentencePiece"><a href="#SentencePiece" class="headerlink" title="SentencePiece"></a>SentencePiece</h2><blockquote>
<p>一般与Unigram算法连用。Unigram的算法思想是从一个巨大的词汇表出发，再逐渐删除trim down其中的词汇，直到size满足预定义。初始的词汇表可以采用所有预分词器分出来的词，再加上所有高频的子串。每次从词汇表中删除词汇的原则是使预定义的损失最小。<br>Unigram算法每次会从词汇表中挑出使得loss增长最小的10%~20%的词汇来删除。</p>
</blockquote>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM论文阅读——RAG系列</title>
    <url>/2024/10/31/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94RAG%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<ul>
<li>RAG流程<blockquote>
<div aligh="center"><img src="/asset/2/rag.png" class="lazyload" data-srcset="/asset/2/rag.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>LLM论文阅读——模型训练技巧</title>
    <url>/2024/10/31/LLM%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%8A%80%E5%B7%A7/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="训练显存"><a href="#训练显存" class="headerlink" title="训练显存"></a>训练显存</h2><ul>
<li><p>FP32,FP16,BF16</p>
<blockquote>
<p>FP32:1位符号，8位指数，23位尾数。<br>FP16:1位符号，5位指数，10位尾数。<br>BF16:1位符号，8位指数，7位尾数。BF16提供了与FP32相同的动态范围，但精度低于FP32和FP16。</p>
</blockquote>
</li>
<li><p>如何计算训练大模型需要的显存？</p>
<blockquote>
<ul>
<li>模型参数：如果模型有P个参数，使用FP32保存的话，需要4P字节的显存。</li>
<li>梯度：大小与模型参数相同。使用FP32保存的话，需要4P字节的显存。</li>
<li>优化器状态：Adam需要保存参数的一阶和二阶矩估计。因此显存翻倍，使用FP32保存的话，需要4P*2&#x3D;8P字节的显存。</li>
<li>激活和中间变量：前向传播和反向传播过程中的激活值和中间变量也需要存储，这与批量大小（batch size）和序列长度有关。这些是动态的显存。</li>
</ul>
</blockquote>
</li>
<li><p>解决训练显存不够的几种办法。</p>
<blockquote>
<ol>
<li>减小batchsize。通过减小中间激活值来减少显存占用量。</li>
<li>使用混合精度训练。</li>
<li>梯度累积(Gradient accumulation)：在多个小批次（mini-batches）上累积梯度，然后一次性更新模型参数。这样可以在不增加显存消耗的情况下，模拟更大的批次大小，从而提高训练速度和模型性能（可以证明与大batchsize训练效果完全等价：只需要将每次计算出的loss除以一个accumulation step，就能保证最后的梯度一致）。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br><span class="line"><span class="keyword">for</span> i, (inputs, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(mini_batches, mini_targets)):</span><br><span class="line">    outputs = model(inputs)</span><br><span class="line">    loss = criterion(outputs, labels)</span><br><span class="line">    </span><br><span class="line">    loss = loss / accumulation_steps  </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播，累积梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 每 accumulation_steps 步进行一次参数更新</span></span><br><span class="line">    <span class="keyword">if</span> (i + <span class="number">1</span>) % accumulation_steps == <span class="number">0</span>:</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br></pre></td></tr></table></figure></li>
<li>梯度检查点(Gradient checkpointing)：在反向传播时重新计算中间层的激活值，而不是在整个训练过程中都保持它们。这样可以大大减少显存占用，但可能会增加一些计算成本。（前向传播过程中，以torch.no_grad()方式运行，不保存中间激活；反向时，检索保证的输入和函数，然后计算梯度）</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.utils.checkpoint <span class="keyword">as</span> checkpoint</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.layer1 = nn.Linear(<span class="number">100</span>, <span class="number">200</span>)</span><br><span class="line">        <span class="variable language_">self</span>.layer2 = nn.Linear(<span class="number">200</span>, <span class="number">200</span>)</span><br><span class="line">        <span class="variable language_">self</span>.layer3 = nn.Linear(<span class="number">200</span>, <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = checkpoint.checkpoint(<span class="variable language_">self</span>.layer1, x)  <span class="comment"># 在第一层使用梯度检查点</span></span><br><span class="line">        x = checkpoint.checkpoint(<span class="variable language_">self</span>.layer2, x)  <span class="comment"># 在第二层使用梯度检查点</span></span><br><span class="line">        x = <span class="variable language_">self</span>.layer3(x)  <span class="comment"># 第三层不使用梯度检查点</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<blockquote>
<ol start="5">
<li>Parameter Efficient Finetuning。</li>
<li>ZeRO-Offload：将未使用的数据暂时卸载到CPU或不同的设备中，然后在需要时再将其读回，参数动态地从 GPU -&gt; CPU, CPU -&gt; GPU 进行转移，从而节省 GPU 内存。这个想法的一个具体实现是ZeRO，它将参数、梯度和优化器状态分割到所有可用的硬件上，并根据实际需要再将它们具体化。</li>
</ol>
</blockquote>
</li>
</ul>
<h2 id="混合精度训练"><a href="#混合精度训练" class="headerlink" title="混合精度训练"></a>混合精度训练</h2><blockquote>
<p>使用混合精度训练，内存占用更少，计算更快（不少GPU都有对fp16计算的优化）。</p>
</blockquote>
<blockquote>
<p>FP16存在的问题：</p>
<ol>
<li>数据溢出：容易上溢或者下溢。</li>
<li>舍入误差。</li>
</ol>
</blockquote>
<blockquote>
<p>解决办法：</p>
<ol>
<li><p>FP32权重备份：用于解决舍入误差。weights, activations, gradients 等数据在训练中都利用FP16来存储，同时拷贝一份FP32的weights，用于更新。<br>这主要是因为，权重&#x3D;旧权重+lr*梯度，而lr * 梯度往往很小，使用fp16相加会有舍入误差。，在训练过程中，内存中占据大部分的基本都是 activations 的值（动态内存）。特别是在batchsize 很大的情况下， activations 更是特别占据空间（一般为静态内存的3-4倍），因此使用fp16可以大大减少显存占用。</p>
</li>
<li><p>Loss Scale：主要为了解决fp16下溢的问题。训练到了后期，梯度会特别小（特别是激活函数饱和段的梯度），fp16会产生下溢现象。<br>对计算出来的loss进行scale，由于链式法则的存在，loss上的scale也会作用到梯度上。这样，scaled-gradient 就可以一直使用 fp16 进行存储了。只有在进行更新的时候，才会将 scaled-gradient 转化为 fp32，同时将scale抹去。</p>
</li>
<li><p>精度累加：主要是为了减少加法过程中的舍入误差，保证精度不损失。在某些模型中，fp16矩阵乘法的过程中，需要利用 fp32 来进行矩阵乘法中间的累加(accumulated)，然后再将 fp32 的值转化为 fp16 进行存储。即利用fp16进行乘法和存储，利用fp32来进行加法计算。</p>
</li>
</ol>
</blockquote>
<blockquote>
<p>具体步骤：</p>
<div aligh="center"><img src="/asset/2/mixed_precision.png" class="lazyload" data-srcset="/asset/2/mixed_precision.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ol>
<li>维护一个 FP32 数值精度模型的副本</li>
<li>在每个iteration</li>
</ol>
<ul>
<li>拷贝并且转换成 FP16 模型</li>
<li>前向传播（FP16 的模型参数)，此时 weights, activations 都是 FP16</li>
<li>loss 乘 scale factor s</li>
<li>反向传播（FP16 的模型参数和参数梯度)， 此时 gradients 也是 FP16</li>
<li>参数梯度乘 1&#x2F;s</li>
<li>利用 FP16 的梯度更新 FP32 的模型参数</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Pytorch中amp(automatic mixed precision)示例</span></span><br><span class="line"><span class="keyword">from</span> torch.cuda.amp <span class="keyword">import</span> autocast, GradScaler</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">scaler = GradScaler()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> step, batch <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader, <span class="number">1</span>):</span><br><span class="line"></span><br><span class="line">    <span class="comment"># prepare inputs and targets for the model and loss function respectively.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># forward pass with `autocast` context manager</span></span><br><span class="line">    <span class="keyword">with</span> autocast(enabled=<span class="literal">True</span>):</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># computing loss</span></span><br><span class="line">    loss = loss_fn(outputs, targets)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># scale gradint and perform backward pass</span></span><br><span class="line">    scaler.scale(loss).backward()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># before gradient clipping the optimizer parameters must be unscaled.</span></span><br><span class="line">    scaler.unscale_(optimizer)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># perform optimization step</span></span><br><span class="line">    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)</span><br><span class="line"></span><br><span class="line">    scaler.step(optimizer)</span><br><span class="line">    scaler.update()</span><br></pre></td></tr></table></figure>

<h2 id="并行训练"><a href="#并行训练" class="headerlink" title="并行训练"></a>并行训练</h2><h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><ul>
<li><p>数据并行原理</p>
<blockquote>
<p>模型一台设备装得下，所以同一个模型同时用多份数据分开来训练。</p>
<p>同一个batch的数据分成多个部分，每个部分分配到一个设备上，每台设备上持有一个完整的模型副本。每台设备在分配的数据上进行训练；在反向传播后，模型的梯度将会进行聚合(All reduce)，以便在不同设备上的模型参数保持同步。</p>
<p>主要方案：Ring All-reduce（无master节点）和Parameter server（有master节点）。Ring All-reduce是以环形的方式传递梯度，减小时延；Parameter server则是有一个master节点和多个worker节点。</p>
</blockquote>
</li>
<li><p>Pytorch中DP的原理</p>
<blockquote>
<div aligh="center"><img src="/asset/2/dp.png" class="lazyload" data-srcset="/asset/2/dp.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>采用的是parameter server模式。</p>
<p>缺点：</p>
<ul>
<li>DP使用单进程多线程实现，但受困于GIL，会带来性能开销，速度很慢。</li>
<li>主卡性能和通信开销容易成为瓶颈，GPU 利用率通常很低。（主卡占用多，其他卡占用少）</li>
<li>不支持模型并行。</li>
</ul>
</blockquote>
</li>
<li><p>Pytorch中DDP的原理</p>
<blockquote>
<div aligh="center"><img src="/asset/2/ddp.png" class="lazyload" data-srcset="/asset/2/ddp.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>基于多进程实现，每个进程都有独立的优化器，执行自己的更新过程。每个进程都执行相同的任务，并且每个进程都与所有其他进程通信。进程（GPU）之间只传递梯度，这样网络通信就不再是瓶颈。</p>
<ol>
<li>首先将 rank&#x3D;0 进程中的模型参数广播到进程组中的其他进程</li>
<li>每个 DDP 进程都会创建一个 local Reducer 来负责梯度同步</li>
<li>训练过程中，每个进程从磁盘加载 batch 数据，并将它们传递到其 GPU。每个 GPU 都有自己的前向过程，完成前向传播后，梯度在各个 GPUs 间进行 Ring All-Reduce，每个 GPU 都收到其他 GPU 的梯度，从而可以独自进行反向传播和参数更新。</li>
<li>同时，每一层的梯度不依赖于前一层，所以梯度的 All-Reduce 和后向过程同时计算，以进一步缓解网络瓶颈。</li>
</ol>
</blockquote>
</li>
<li><p>DP与DDP的区别</p>
<blockquote>
<ul>
<li>DP是基于单进程多线程实现，只用于单机；DDP是多进程实现的，并且因为每个进程都是独立的Python的解释器，避免了GIL带来的性能开销。</li>
<li>参数更新的方式不用。DP将各个GPU上梯度汇总后求平均，在主卡进行参数更新，然后将模型参数广播到其他GPU。DDP只传播梯度，且各进程中模型初始参数一致，因此更新后的参数也一致。</li>
<li>DDP支持模型并行，而DP并不支持。</li>
</ul>
</blockquote>
</li>
</ul>
<h3 id="模型并行-1-：张量并行"><a href="#模型并行-1-：张量并行" class="headerlink" title="模型并行(1)：张量并行"></a>模型并行(1)：张量并行</h3><blockquote>
<div aligh="center"><img src="/asset/2/tensor_para.png" class="lazyload" data-srcset="/asset/2/tensor_para.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>张量并行，将计算图中的层内的参数（张量）切分到不同设备（即层内并行），每个设备只拥有模型的一部分，以减少内存负荷。</p>
</blockquote>
<blockquote>
<p>张量并行方式，有行并行和列并行。</p>
<div aligh="center"><img src="/asset/2/row_col_para.png" class="lazyload" data-srcset="/asset/2/row_col_para.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<div aligh="center"><img src="/asset/2/row_col_para_2.png" class="lazyload" data-srcset="/asset/2/row_col_para_2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
<blockquote>
<div aligh="center"><img src="/asset/2/tensor_para_2.png" class="lazyload" data-srcset="/asset/2/tensor_para_2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<ul>
<li>1D张量并行（Megatron-LM）:张量并行则涉及到不同的分片 (sharding)方法，现在最常用的都是 1D 分片，即将张量按照某一个维度进行划分（横着切或者竖着切）。但每个处理器仍需要存储整个中间激活，在处理大模型时会消耗大量的显存空间。</li>
<li>多维张量并行（Colossal-AI）</li>
</ul>
</blockquote>
<h3 id="模型并行-2-：流水线并行"><a href="#模型并行-2-：流水线并行" class="headerlink" title="模型并行(2)：流水线并行"></a>模型并行(2)：流水线并行</h3><blockquote>
<div aligh="center"><img src="/asset/2/pipeline_para.png" class="lazyload" data-srcset="/asset/2/pipeline_para.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>由于模型太大，无法将整个模型放置到单张GPU卡中；因此，将模型的不同层放置到不同的计算设备，降低单个计算设备的显存消耗，从而实现超大规模模型训练。各个设备上的网络层会使用反向传播过程计算得到的梯度更新参数。由于各个设备间传输的仅是相邻设备间的输出张量，而不是梯度信息，因此通信量较小。</p>
</blockquote>
<blockquote>
<p>朴素流水线并行</p>
<div aligh="center"><img src="/asset/2/naive_pipe.png" class="lazyload" data-srcset="/asset/2/naive_pipe.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
<blockquote>
<p>微批次流水线并行</p>
<div aligh="center"><img src="/asset/2/micro_pipe.png" class="lazyload" data-srcset="/asset/2/micro_pipe.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
<h3 id="DeepSpeed"><a href="#DeepSpeed" class="headerlink" title="DeepSpeed"></a>DeepSpeed</h3><ul>
<li>ZeRO-DP的三种状态<blockquote>
<p>ZeRO-DP简单来说就是想办法在数据并行的管线上把模型的参数分配到不同的显卡上，而不用所有显卡都装载所有参数。<br>它把训练期间模型状态的内存消耗归为三类：</p>
<ul>
<li>OS(Optimizer State)：优化器状态（如Adam的momentum和variance）(fp32的模型参数备份，fp32的momentum和fp32的variance)</li>
<li>G(Gradient)：模型梯度(fp16)</li>
<li>P(Parameter)：模型参数(fp16)<br>针对此，有三种优化方案。（3D parallelism）</li>
</ul>
</blockquote>
</li>
</ul>
<blockquote>
<p>Stage 1: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>每台设备上都有完整的梯度和模型参数，但是只保留1&#x2F;N的优化器状态，练时也只更新这部分状态对应的参数，每轮训练完成后进行reduce-scatter将所有机器上所有参数的梯度合并到负责的机器上去，然后再用all-gather将每台机器计算出的参数分发给全局。</p>
</blockquote>
<blockquote>
<p>Stage 2: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi><mo>+</mo><mi>g</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os+g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2583em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">os</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>假设在于每台设备只能训练部分参数，所以单台设备上其他参数的梯度其实不需要保存。每台设备有完整的模型参数，但是只保留1&#x2F;N的优化器状态和梯度。反向传播时每经过一层参数就开启一轮reduce-scatter将梯度整合到一个节点，计算出下一层梯度后删除前一层梯度。</p>
<div aligh="center"><img src="/asset/2/p_os_g.png" class="lazyload" data-srcset="/asset/2/p_os_g.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
<blockquote>
<p>Stage 3: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi><mo>+</mo><mi>g</mi><mo>+</mo><mi>p</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os+g+p}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2583em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">os</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">p</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span></p>
<p>在Stage 2基础上每一轮加上一次broadcast，把模型参数分发到各个设备上。</p>
</blockquote>
<blockquote>
<p>三个Stage的内存优化效率图</p>
<div aligh="center"><img src="/asset/2/zero_stage.png" class="lazyload" data-srcset="/asset/2/zero_stage.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
<ul>
<li><p>ZeRO-Offload</p>
<blockquote>
<div aligh="center"><img src="/asset/2/zero_offload.png" class="lazyload" data-srcset="/asset/2/zero_offload.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>ZeRO-Offload的核心思路就是让CPU和内存也参与到训练中去。计算流程是，在GPU上面进行前向和后向计算，将梯度传给CPU，进行参数更新，再将更新后的参数传给GPU（可以开启Optimizer Offload和Param Offload，性能会受到影响。）</p>
</blockquote>
</li>
<li><p>DeepSpeed各stage的速度以及显存消耗</p>
<blockquote>
<p>速度:<br>Stage 0 &gt; Stage 1 &gt; Stage 1+Offload &gt; Stage 2 &gt; Stage 2+Offload &gt; Stage 3 &gt; Stage 3+Offload</p>
<p>显存消耗：<br>Stage 0 &lt; Stage 1 &lt; Stage 1+Offload &lt; Stage 2 &lt; Stage 2+Offload &lt; Stage 3 &lt; Stage 3+Offload</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>LLM</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>LLM</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态论文阅读——BLIP系列</title>
    <url>/2024/10/31/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94BLIP%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="BLIP"><a href="#BLIP" class="headerlink" title="BLIP"></a>BLIP</h2><ul>
<li><p>BLIP的主要贡献</p>
<blockquote>
<ol>
<li>提出了一种Multimodal mixture of Encoder-Decoder(MED)的多模态预训练模式。</li>
<li>提出了一种Captioning and Filtering的Dataset Bootstrapping机制，对原始数据集进行清洗。</li>
<li>微调后在下游Image-text retrieval、Image captioning、VQA等任务上达到了SOTA。</li>
</ol>
</blockquote>
</li>
<li><p>BLIP的模型架构</p>
<blockquote>
<div aligh="center"><img src="/asset/2/blip.png" class="lazyload" data-srcset="/asset/2/blip.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ul>
<li><p>Unimodal Vision Encoder：采用的是在ImageNet-1k预训练的ViT（与LLaVa使用的不同）。</p>
</li>
<li><p>Unimodal Text Encoder：采用的是Bert-base，取[CLS] token的特征作为文本特征。</p>
</li>
<li><p>Image-grounded Text Encoder：采用Bert，在SA和FFN中间加上Cross Attention层，和图像特征交融。对于文本的输入，会在开头加上task-specific的[Encode]。</p>
</li>
<li><p>Image-grounded Text Decoder：将Image-grounded Text Encoder中的Bi Self-Att改为Causal Self-Att,同时也会和图像特征通过CA交融。会在开头加上[Decoder]表示序列起点，以及[EOS]表示序列终点。</p>
</li>
<li><p>后两部分除了Self-Att层不共享，其余部分参数共享。</p>
</li>
</ul>
</blockquote>
</li>
<li><p>BLIP的预训练目标</p>
<blockquote>
<ul>
<li>Image-Text Contrastive Loss：将文本和图像在特征空间对齐，使用的是InfoNCE。实现方式上采用了ALBEF中的Momentum encoder。</li>
<li>Image-Text Matching Loss：学习文本和图像之间的细粒度匹配，使用的是BCE loss。实现方式上采用了ALBEF中的Hard negative mining策略，即对于一个batch中用于更高对比相似度的负样本，更容易被选择来计算loss。</li>
<li>Language-modeling loss：以自回归的方式进行极大似然估计，使用的是CE loss。计算时候使用了0.1的label smoothing。</li>
</ul>
</blockquote>
</li>
<li><p>BLIP的CapFilt机制</p>
<blockquote>
<div aligh="center"><img src="/asset/2/capfilt.png" class="lazyload" data-srcset="/asset/2/capfilt.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
为了对含有噪声的Alt-text对进行过滤清洗，人工标注部分数据集得到高质量文本-图像对，然后分别微调得到一个Filter和一个Captioner，对原数据集进行重Caption与过滤，得到更高质量数据集再重新预训练MED。
</blockquote>
</li>
<li><p>BLIP是如何微调的？</p>
<blockquote>
<ul>
<li>Image-Text Retrieval：对预训练模型使用ITC和ITM loss，在COCO和Flickr30K上进行微调。</li>
<li>Image Captioning：在COCO上使用LM loss进行微调。</li>
<li>VQA：如下图，使用LM loss微调。<div aligh="center"><img src="/asset/2/blip_vqa.png" class="lazyload" data-srcset="/asset/2/blip_vqa.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div></li>
<li>NLVR：略。</li>
<li>VisDial：略。</li>
</ul>
</blockquote>
</li>
<li><p>BLIP预训练使用的数据集</p>
<blockquote>
<p>预训练数据集为:</p>
<ul>
<li>Conceptual Captions</li>
<li>SBU Captions</li>
<li>COCO</li>
<li>Visual Genome<br>还引入了噪声更大的Conceptual 12M；还尝试了额外的Web数据集LAION.</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="BLIP2"><a href="#BLIP2" class="headerlink" title="BLIP2"></a>BLIP2</h2><ul>
<li><p>BLIP2与BLIP的区别</p>
<blockquote>
<ul>
<li>BLIP2使用预训练的LLM。</li>
<li>BLIP2引入了Q-former作为预训练视觉模型与LLM之间的桥梁，大大减少了预训练的训练参数与训练成本。</li>
</ul>
</blockquote>
</li>
<li><p>BLIP2的模型架构</p>
<blockquote>
<div aligh="center"><img src="/asset/2/blip2.png" class="lazyload" data-srcset="/asset/2/blip2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
由一个冻结的Vision Encoder，一个冻结的LLM，一个Q-Former和一个FC的Connector组成。其中在预训练过程中，Vision encoder和LLM始终冻结。其中Vision Encoder采用了两种 (1) CLIP-ViT-L/14 (2) EVA-CLIP-VIT-g/14。LLM有两种：(1) Decoder-based的OPT （2）Encoder-Decoder-based FlanT5。
<div aligh="center"><img src="/asset/2/qformer.png" class="lazyload" data-srcset="/asset/2/qformer.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
Q-former 由两个Transformer子模块组成，两部分共享Self-attention层。Image transformer通过Cross attention与图片特征交互，Text transformer可以通过Self attention从可学习Queries获取视觉特征。根据预训练任务的不同，Self-attention的mask也会不同。Q-former的权重由预训练的BERT-base初始化来，而Cross attention部分随机初始化。BLIP-2中使用了32个Queries，每个维度768，这远小于提取到的图像特征(32x768 << 257x1024)。
</blockquote>
</li>
<li><p>BLIP2的预训练过程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/blip2_pretrain.png" class="lazyload" data-srcset="/asset/2/blip2_pretrain.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
由两阶段组成：第一阶段Vision-language表征学习，第二阶段Vision-to-language生成学习。

<p>(1) 第一阶段：表征学习，将冻结的Image encoder和可学习的Q-former连在一起，同时优化三种优化目标。</p>
<ul>
<li>Image-Text Contrastive Learning：对齐文本与图像，最大化两者之间的互信息。这一部分采用的是Image transformer输出的Query representation和Text transformer输出的[CLS] token。由于Queries有32个，因此这里会每个计算相似度然后取最高的作为Image-Text similarity。为了避免信息泄露，这里采用的是Unimodal self-attention mask。</li>
<li>Image-grounded Text Generation：训练Q-former根据图像生成文本。由于text transformer不能直接和视觉特征交互，因此首先由Queries提取视觉信息，然后通过Self attention传递给text token。这里采用了Multimodal causal Self-attention mask。同时也会将开头的[CLS] token更换为[DEC]。</li>
<li>Image-Text Matching：对文本和图像进行更细粒度的对齐。这里采用了Bi-directional Self-attention mask，因此输出的query embedding能够捕获多模态信息。然后将query embedding输入二分类头获得logit后进行average得到输出的matching score。<br>(这一部分实现原理是：attenion时把query和text 拼接起来输入到一个self-attention模块,然后通过mask控制query和text之间的交互,然后attention之后再把query和text分开进行后面的操作，所以实际上是同一个Bert模型。参考BLIP2官方代码)</li>
</ul>
<p>(2) 第二阶段：视觉到语言生成学习。将Q-former和冻结的LLM连接，并通过FC对齐query embedding和text embedding的维度。然后将投影过的query embeddings附加在text input embedding的前面，作为Soft Visual Prompt，为LLM提供有用的视觉信息并去除无关的视觉信息。这里进行了两种LLM的实验，对于Decoder-based使用LM loss，对于Encoder-decoder的使用Prefix LM loss。</p>
</blockquote>
</li>
<li><p>BLIP2预训练使用的数据集</p>
<blockquote>
<p>和BLIP一样使用下面6个数据集，图片加起来约为129M.</p>
<ul>
<li>Conceptual Captions</li>
<li>SBU Captions</li>
<li>COCO</li>
<li>Visual Genome</li>
<li>噪声更大的Conceptual 12M</li>
<li>额外的Web数据集LAION400M的一部分。</li>
</ul>
</blockquote>
</li>
<li><p>BLIP2是如何微调的？</p>
<blockquote>
<ul>
<li><p>Image Cptioning：用”A photo of”作为初始输入给LLM，使用LM loss训练。<strong>保持LLM冻结，更新Q-former和Image encoder的参数。</strong></p>
</li>
<li><p>VQA：<strong>保持LLM冻结，更新Q-former和Image encoder的参数。</strong> 使用Open-ended answer generation loss微调。同时额外为Q-former注入question信息，能够指导Q-former在cross attention层聚焦于更含信息的区域。</p>
<div aligh="center"><img src="/asset/2/blip2_vqa.png" class="lazyload" data-srcset="/asset/2/blip2_vqa.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</li>
<li><p>Image-Text Retrieval：直接在第一阶段预训练模型上微调。同时微调Image encoder和Q-former。推理时，先根据Image-text feature similarity选出128个candidate，然后根据成对的Image-text matching scores进行重排序。</p>
</li>
</ul>
</blockquote>
</li>
<li><p>BLIP-2的局限性</p>
<blockquote>
<p>LLM 一般具备 In-Contet Learning 的能力，但是在 In-Context VQA 的场景下，BLIP-2 没观察到好的结果。对于这种上下文学习能力的缺失，作者把原因归结为预训练数据集中的每个数据只包含一个图像-文本对，导致 LLM 无法从中学习单个序列中多个图像-文本对之间的相关性。</p>
<p>BLIP-2 的图文生成能力不够令人满意，可能是 LLM 知识不准确带来的。同时 BLIP-2 继承了冻结参数的 LLM 的风险，比如输出攻击性语言，传播社会偏见。解决的办法是指令微调，或者过滤掉有害的数据集。</p>
<p>BLIP-2生成的caption一般比较简短，这与训练数据集有关。</p>
</blockquote>
</li>
<li><p>Q-former作用？</p>
<blockquote>
<p>图文对齐、维度压缩。</p>
</blockquote>
</li>
<li><p>可学习的Query作用是什么？</p>
<blockquote>
<ul>
<li>从视觉信息中提取与文本最相关的信息。</li>
<li>使得提取出的信息能够被LLM理解</li>
</ul>
</blockquote>
</li>
<li><p>Q-former和MLP的优劣在哪里？</p>
<blockquote>
<p>(1) Q-former会导致视觉token的有损压缩，会把任意长度的visual token转译成32个token，丢失部分空间信息。<br>(2) Q-former相比于MLP参数量更大，收敛更慢，小数据量不如MLP，大数据量对比MLP也没有优势。</p>
</blockquote>
</li>
</ul>
<h2 id="InstructBLIP"><a href="#InstructBLIP" class="headerlink" title="InstructBLIP"></a>InstructBLIP</h2><h2 id="BLIP3（xGen-MM）"><a href="#BLIP3（xGen-MM）" class="headerlink" title="BLIP3（xGen-MM）"></a>BLIP3（xGen-MM）</h2><ul>
<li><p>BLIP3的模型结构</p>
<blockquote>
<div aligh="center"><img src="/asset/2/blip3.png" class="lazyload" data-srcset="/asset/2/blip3.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>Vision encoder采用了SigLIP，Adaptor使用了Flamingo的perceiver resampler，LLM使用了Phi3-mini。</p>
<p>同时，采用了LLaVa-v1.5中的Any-Res策略（动态分辨率），将高分辨率图片切分成patch过后，分别进行encoding后拼接，同时将downsize的原图也进行encoding后接在后面。然后分别将每个patch过resampler，concat之后作为visual prompt。</p>
<p>在所有阶段中，Vision encoder冻结，只训练resampler和LLM。</p>
</blockquote>
</li>
<li><p>BLIP3的训练recipe</p>
<blockquote>
<ol>
<li>Pre-training：不再使用（ITC,ITG,ITM），统一使用Next token prediction作为loss。预训练时图像分辨率为384x384，这与SigLIP对齐。使用的数据集包括：图文交错数据集；高质量Caption和OCR数据集；高质量Visual Grounding数据集；其他公共数据集的混合。只训练resampler和LLM。</li>
<li>SFT：这一阶段采用Any-Res策略进行动态分辨率的输入。使用的数据集包括：单图的Visual instruction数据（对话，Captioning，VQA，OCR等），以及只有文本的instruction数据。SFT了一个epoch。只训练resampler和LLM。</li>
<li>Interleaved Multi-Image SFT：同样使用Any-Res策略。使用的数据集包括：单图和多图的Visual instruction数据。这样能够提高模型的in-context能力以及多图推理能力。只训练resampler和LLM。</li>
<li>Post-training：有两步后训练：第一步使用DPO提高模型的helpfulness和visual faithfulness；第二步使用safety fine-tuning提高模型的harmlessness。这两步都是用LoRA仅训练LLM backbone。</li>
</ol>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态论文阅读——CLIP及其微调方法</title>
    <url>/2024/10/31/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94CLIP%E5%8F%8A%E5%85%B6%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="CLIP"><a href="#CLIP" class="headerlink" title="CLIP"></a>CLIP</h2><ul>
<li><p>CLIP模型结构</p>
<blockquote>
<p>Image Encoder有两种架构：一种是ResNet50，将全局平均池化替换为注意力池化；第二种是ViT(Pre-norm)。<br>Text Encoder实际上是GPT-2架构，即Transformer decoder，将文本用[SOS]和[EOS]括起来，取[EOS]上的feature过一层Linear作为文本特征。</p>
</blockquote>
</li>
<li><p>CLIP训练时的损失函数</p>
<blockquote>
<p>InfoNCE，一种用于自监督学习的特征表示学习损失函数。<br>公式：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>I</mi><mi>n</mi><mi>f</mi><mi>o</mi><mi>N</mi><mi>C</mi><mi>E</mi><mo>=</mo><mo>−</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><msubsup><mo>∑</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mfrac><mrow><msub><mi>q</mi><mi>i</mi></msub><mo>∗</mo><msub><mi>k</mi><mrow><mi>i</mi><mo>+</mo></mrow></msub></mrow><mi>τ</mi></mfrac><mo stretchy="false">)</mo></mrow><mrow><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mfrac><mrow><msub><mi>q</mi><mi>i</mi></msub><mo>∗</mo><msub><mi>k</mi><mrow><mi>j</mi><mo>−</mo></mrow></msub></mrow><mi>τ</mi></mfrac><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">InfoNCE=-\frac{1}{N}\sum\limits_{i=1}^{N}log(\frac{exp(\frac{q_i*k_{i+}}{\tau})}{\sum_{j=1}^N exp(\frac{q_i * k_{j-}}{\tau})})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.07847em;">I</span><span class="mord mathnormal">n</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.05764em;">NCE</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.506em;vertical-align:-0.9777em;"></span><span class="mord">−</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8451em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5283em;"><span style="top:-2.1223em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop op-symbol small-op">∑</span></span></span><span style="top:-3.95em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3235em;"><span style="top:-2.3617em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mop mtight"><span class="mop op-symbol small-op mtight" style="position:relative;top:0em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8852em;"><span style="top:-2.1786em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-2.8971em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4603em;"><span></span></span></span></span></span></span><span class="mspace mtight" style="margin-right:0.1952em;"></span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">p</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1832em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.1132em;">τ</span></span></span></span><span style="top:-3.2255em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.6872em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.0359em;margin-right:0.1em;"><span class="pstrut" style="height:2.6595em;"></span><span class="mord mathnormal mtight">i</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3147em;"><span></span></span></span></span></span></span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.0315em;margin-right:0.1em;"><span class="pstrut" style="height:2.6595em;"></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span><span class="mord mtight">−</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5092em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.5508em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight">x</span><span class="mord mathnormal mtight">p</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mopen nulldelimiter sizing reset-size3 size6"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.1038em;"><span style="top:-2.656em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.1132em;">τ</span></span></span></span><span style="top:-3.2255em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line mtight" style="border-bottom-width:0.049em;"></span></span><span style="top:-3.6078em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.0359em;margin-right:0.1em;"><span class="pstrut" style="height:2.6595em;"></span><span class="mord mathnormal mtight">i</span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3147em;"><span></span></span></span></span></span></span><span class="mbin mtight">∗</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3448em;margin-left:-0.0315em;margin-right:0.1em;"><span class="pstrut" style="height:2.6595em;"></span><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mord mtight">+</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3981em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.344em;"><span></span></span></span></span></span><span class="mclose nulldelimiter sizing reset-size3 size6"></span></span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9605em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span>，N是样本的数量，q是查询样本的编码，k是与查询样本对应的正样本或负样本的编码。<br>目的：最大化正样本对相似度，最小化负样本对相似度。</p>
</blockquote>
</li>
<li><p>CLIP的loss中为什么是加上两部分的CE？</p>
<blockquote>
<p>仅优化I(i,t)是最大化单向的互信息，CLIP的目标是学习联合分布，因此是两项之和，即每一项在一个方向上最大化互信息。</p>
</blockquote>
</li>
<li><p>CLIP训练的伪代码</p>
<blockquote>
<div aligh="center"><img src="/asset/2/clip.png" class="lazyload" data-srcset="/asset/2/clip.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div></blockquote>
</li>
<li><p>CLIP训练代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">image_embeds = vision_outputs[<span class="number">1</span>]</span><br><span class="line">image_embeds = <span class="variable language_">self</span>.visual_projection(image_embeds)</span><br><span class="line"> </span><br><span class="line">text_embeds = text_outputs[<span class="number">1</span>]</span><br><span class="line">text_embeds = <span class="variable language_">self</span>.text_projection(text_embeds)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># normalized features</span></span><br><span class="line">image_embeds = image_embeds / image_embeds.norm(p=<span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">text_embeds = text_embeds / text_embeds.norm(p=<span class="number">2</span>, dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"> </span><br><span class="line"><span class="comment"># cosine similarity as logits</span></span><br><span class="line">logit_scale = <span class="variable language_">self</span>.logit_scale.exp()</span><br><span class="line">logits_per_text = torch.matmul(text_embeds, image_embeds.t()) * logit_scale</span><br><span class="line">logits_per_image = logits_per_text.t()</span><br><span class="line"> </span><br><span class="line">loss = <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> return_loss:</span><br><span class="line">    loss = clip_loss(logits_per_text)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">contrastive_loss</span>(<span class="params">logits: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">    <span class="keyword">return</span> nn.functional.cross_entropy(logits, torch.arange(<span class="built_in">len</span>(logits), device=logits.device))</span><br><span class="line"> </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clip_loss</span>(<span class="params">similarity: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">    caption_loss = contrastive_loss(similarity)</span><br><span class="line">    image_loss = contrastive_loss(similarity.t())</span><br><span class="line">    <span class="keyword">return</span> (caption_loss + image_loss) / <span class="number">2.0</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
<li><p>CLIP损失函数中温度系数的作用</p>
<blockquote>
<p>温度系数的作用是调节<strong>对困难样本的关注程度</strong>：越小的温度系数越关注于将本样本和最相似的其他样本分开<br>如果温度系数设的越大，logits分布变得越平滑，那么对比损失会对所有的负样本一视同仁，导致模型学习没有轻重。<br>如果温度系数设的过小，logits分布会很尖锐，则模型会越关注特别困难的负样本，但其实那些负样本很可能是潜在的正样本，这样会导致模型很难收敛或者泛化能力差。</p>
</blockquote>
</li>
<li><p>CLIP的位置编码，如何外推？</p>
<blockquote>
<p>CLIP的text encoder是GPT2，因此使用的Learable Positional Encoding，是绝对位置编码。理论上不能外推，但也许可以将超过长度的部分随机初始化然后微调。</p>
</blockquote>
</li>
</ul>
<h2 id="CLIP少样本微调"><a href="#CLIP少样本微调" class="headerlink" title="CLIP少样本微调"></a>CLIP少样本微调</h2><h3 id="Linear-Probe"><a href="#Linear-Probe" class="headerlink" title="Linear Probe"></a>Linear Probe</h3><blockquote>
<div aligh="center"><img src="/asset/2/lp.png" class="lazyload" data-srcset="/asset/2/lp.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
Encoder的embedding后接分类头，进行微调。
</blockquote>
<h3 id="Context-Optimization"><a href="#Context-Optimization" class="headerlink" title="Context Optimization"></a>Context Optimization</h3><blockquote>
<div aligh="center"><img src="/asset/2/coop.png" class="lazyload" data-srcset="/asset/2/coop.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
针对CLIP中直接使用"A photo of"作为prompt可能不是最优的，CoOp提出使用可学习的token embedding，让模型自己调优prompt（可使用前人总结的prompt做初始化）。
</blockquote>
<h3 id="CLIP-Adapter"><a href="#CLIP-Adapter" class="headerlink" title="CLIP-Adapter"></a>CLIP-Adapter</h3><blockquote>
<div aligh="center"><img src="/asset/2/clip_adapter.png" class="lazyload" data-srcset="/asset/2/clip_adapter.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
<div aligh="center"><img src="/asset/2/clip_adapter_1.png" class="lazyload" data-srcset="/asset/2/clip_adapter_1.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
<div aligh="center"><img src="/asset/2/clip_adapter_2.png" class="lazyload" data-srcset="/asset/2/clip_adapter_2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
相较于CoOp，CLIP-Adapter的训练方式更加轻量化，只有两个残差连接的MLP。将两部分按照一定比例进行blend。（为什么要用残差连接？为了保留CLIP的原始能力）(为什么不全参微调？容易Over-fit以及catastrophic forgetting。)
</blockquote>
<h2 id="SigLIP"><a href="#SigLIP" class="headerlink" title="SigLIP"></a>SigLIP</h2><ul>
<li><p>SigLIP的主要改进点</p>
<blockquote>
<p>CLIP的对比学习损失InfoNCE，需要对一整个batch内的sample求L2 norm，这对多机不友好；并且性能也受到batch size大小的限制。</p>
<p>SigLIP提出使用每一对之间的sigmoid loss来作为学习目标，这样在概念上不受到batch size的限制；其次，也能通过一种高效的实现方式来对多卡通信速度进行提升。</p>
</blockquote>
</li>
<li><p>SigLIP的损失项以及训练伪代码</p>
<blockquote>
<div aligh="center"><img src="/asset/2/siglip_loss.png" class="lazyload" data-srcset="/asset/2/siglip_loss.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>其中当为正样本对时，z为1；为负样本对时，z为-1。b是一个可学习的偏置项，其使用原因是训练样本中大部分为负样本对，需要在训练初期纠正这种偏置。t为可学习的温度参数。</p>
<div aligh="center"><img src="/asset/2/siglip_train.png" class="lazyload" data-srcset="/asset/2/siglip_train.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
<li><p>SigLIP如何在多卡之间减少gather开销？</p>
<blockquote>
<div aligh="center"><img src="/asset/2/siglip_chunk.png" class="lazyload" data-srcset="/asset/2/siglip_chunk.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>每次计算本卡上batch size的loss，然后每次互相交换text embeddings，最后将每个chunk的loss相加。</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态论文阅读——Flamingo,Mini-GPT4,ViLA</title>
    <url>/2024/10/31/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Flamingo,Mini-GPT4,ViLA/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="Flamingo"><a href="#Flamingo" class="headerlink" title="Flamingo"></a>Flamingo</h2><ul>
<li><p>Flamingo的贡献</p>
<blockquote>
<ol>
<li>桥接预训练好的视觉模型和语言模型</li>
<li>可以处理任意交错的图文对数据</li>
<li>可以同时以图像和视频数据作为输入<br>Flamingo实现了多模态领域的Few-shot learning(in-context learning)能力，即多模态领域的GPT-3。</li>
</ol>
</blockquote>
</li>
<li><p>Flamingo模型结构</p>
<blockquote>
<div aligh="center"><img src="/asset/2/flamingo.png" class="lazyload" data-srcset="/asset/2/flamingo.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
通过Perceiver Resampler和Gated Xatten-dense，与新插入到LM中的层计算Cross-attention，从而将视觉信息注入的LM的生成过程中。</blockquote>
</li>
</ul>
<blockquote>
<p>Perceiver Resampler:</p>
<div aligh="center"><img src="/asset/2/perceiver.png" class="lazyload" data-srcset="/asset/2/perceiver.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
1. 可以接收任意多个的视频帧（如果是图像，可以视作是单帧视频），经过视觉编码器提取特征，加上time embedding 之后全部展平，得到一个视觉 token 序列.
2. 同时有一个可学习的固定长度的latent query序列。
3. 计算CA时，Q是learnable query,KV是query和image feature拼接起来的。（这里和Q-former区别，Q-former是Q是query，KV是视觉feature）
4. 通过该模块，将视觉特征转成了少量低维度的query embedding。
</blockquote>
<blockquote>
<p>Gated Xattn Dense</p>
<div aligh="center"><img src="/asset/2/gated_xattn.png" class="lazyload" data-srcset="/asset/2/gated_xattn.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
将固定长度的视觉 query 注入到语言模型。在预训练好的 LM 的各层交替地插入一些随机初始化的CA。Gated门控，指的是在每一新插入的层之后的残差链接之前添加一个 tanh gating，即tanh(a)，其中a是一个可学习的标量值，初始值为 0，从而保证初始化时的输出与原 LM 一致。
</blockquote>
<ul>
<li>Flamingo的输入以及训练数据<blockquote>
<p>Flamingo的few-show能力来自交错图文对。训练数据中每次最多输入5个交错的图文对。</p>
</blockquote>
</li>
</ul>
<h2 id="MiniGPT-4"><a href="#MiniGPT-4" class="headerlink" title="MiniGPT-4"></a>MiniGPT-4</h2><ul>
<li><p>MiniGPT-4模型结构</p>
<blockquote>
<div aligh="center"><img src="/asset/2/minigpt4.png" class="lazyload" data-srcset="/asset/2/minigpt4.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>视觉部分： BLIP2中预训练的ViT-G&#x2F;14和Q-former；<br>LLM部分：Vicuna<br>模态对齐：一个Linear Projection Layer<br>所有过程中，LLM和视觉部分都冻结，仅训练Linear Projection。</p>
</blockquote>
</li>
<li><p>MiniGPT-4训练过程</p>
<blockquote>
<p>第一阶段预训练，模态对齐。使用Conceptual Caption、SBU和LAION等数据集训练，仅训练Linear Projection。经过第一阶段的pretrain，作者发现了一些模型很难产生连贯的语言输出的例子，而且会输出一些重复的单词或句子、支离破碎的句子或无关的内容。<br>第二阶段微调，与人类对话对齐。在构建的包含3500对&lt;图片，Instruction，Answer&gt;数据集上进行微调。仅训练Linear Projection。</p>
</blockquote>
</li>
<li><p>MiniGPT-4数据集构建</p>
<blockquote>
<div aligh="center"><img src="/asset/2/minigpt4_ds.png" class="lazyload" data-srcset="/asset/2/minigpt4_ds.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<ul>
<li>从Conceptual Caption中采样5000张图片，让第一阶段的模型去输出详细的描述。如果模型输出的token不够80个token，就使用continue命令让模型一直输出，直到产生足够的图片描述。</li>
<li>数据后处理，使用ChatGPT对图像描述进行纠错，然后人工核验数据。最终从5000条数据中得到3500条高质量训练数据。</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="VILA"><a href="#VILA" class="headerlink" title="VILA"></a>VILA</h2><ul>
<li><p>VILA的贡献</p>
<blockquote>
<ol>
<li>预训练时解冻LLM可以模型的in-context learning能力。</li>
<li>使用图文交错的数据进行预训练对于VLM的多图推理能力和in-context能力有提升，同时能保持住LLM本身的能力。</li>
<li>SFT时使用混合的text-instruction data和visual-instruction data不仅能够弥补在text-only task上的损失，还能提高VLM task的准确率。</li>
</ol>
</blockquote>
</li>
<li><p>VILA的训练过程</p>
<blockquote>
<div aligh="center"><img src="/asset/2/vila.png" class="lazyload" data-srcset="/asset/2/vila.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ol>
<li>初始化Projector：使用image-caption对初始化Projector，冻结LLM，训练Projector。</li>
<li>VL预训练：使用图文交错数据(MMC4)和图文对数据(COYO)作图文对齐，同时训练Projector和LLM。</li>
<li>Visual instruction-tuning：使用FLAN风格的visual instruction数据与text instruction数据混合训练，同时训练LLM和Projector。</li>
</ol>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态论文阅读——LLaVa系列</title>
    <url>/2024/10/31/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94LLaVa%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="LLaVa"><a href="#LLaVa" class="headerlink" title="LLaVa"></a>LLaVa</h2><ul>
<li><p>LLaVa数据集的构建</p>
<blockquote>
<p>根据COCO中的caption和bbox，可以利用language-only GPT-4，对其进行In-context learning来生成三种instruction data.</p>
<ul>
<li>Conversation: 多轮对话，根据caption中的每一部分，生成一个人不断提问某张图片的instruction数据。</li>
<li>Detailed description：详细描述，由原始caption生成详细的描述。</li>
<li>Complex reasoning：对图片中内容的一些复杂推理。<br>LLaVa收集了158K的instruction-following数据，包括58K的conversations，23K的detailed description，77K的complex reasoning。（LLaVa-Instruct-158K）</li>
</ul>
</blockquote>
</li>
<li><p>LLaVa的结构</p>
<blockquote>
<div aligh="center"><img src="/asset/2/llava.png" class="lazyload" data-srcset="/asset/2/llava.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
vision encoder采用预训练的CLIP-ViT-L/14,取最后一层或倒数第二层特征;language decoder采用Vicuna。视觉与文本之间的对齐通过一层线性层对齐。</blockquote>
</li>
<li><p>LLaVa训练过程</p>
<blockquote>
<ul>
<li>第一阶段：特征对齐预训练。训练数据为过滤后的595K对CC3M.每个图片文本对被视为单轮对话数据（即response为图片的caption）。该过程中冻结Vision encoder和LLM，仅训练Projector。</li>
<li>第二阶段：端到端SFT。这一过程中将Vision encoder冻结，训练LLM和projector。有两种场景的微调：多模态聊天机器人，使用前面采集的三种response作为训练数据，训练过程中均匀采样；SicenceQA，对于给定的问题提供完整的推理过程和选择的答案，训练数据以单轮对话形式输入。</li>
</ul>
</blockquote>
</li>
<li><p>LLaVa实验设置</p>
<blockquote>
<div aligh="center"><img src="/asset/2/llava_exp.png" class="lazyload" data-srcset="/asset/2/llava_exp.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></blockquote>
</li>
</ul>
<h2 id="LLaVa-v1-5"><a href="#LLaVa-v1-5" class="headerlink" title="LLaVa-v1.5"></a>LLaVa-v1.5</h2><ul>
<li><p>LLaVa1.5的改进</p>
<blockquote>
<div aligh="center"><img src="/asset/2/llava_v15.png" class="lazyload" data-srcset="/asset/2/llava_v15.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ul>
<li>结构改进：(1)Projector换成了2层MLP，Vision encoder变成了CLIP-ViT-L-336px，LLM升级为Vicuna1.5。(2)LLaVa-v1.5-HD版本增加了对高分辨率图片的支持，将高分辨率分成多个grid分别输入原始encoder，然后concat起来获得视觉信息。</li>
<li>数据改进：</li>
</ul>
<ol>
<li>增加了具有简单响应格式提示词的学术导向的VQA数据集，其中简单格式响应提示词例如：Answer the question using a single word or phrase，主要是为了配合数据集简短的答案，避免歧义。</li>
<li>预训练数据集使用LAION&#x2F;COC&#x2F;SBU的子集，共558K；instruction数据集共665K，在1.0基础上增加了ShareGPT数据集、学术导向的VQA数据集、OCR数据集、Region-level的VQA数据集。</li>
</ol>
</blockquote>
</li>
<li><p>LLaVa1.5存在的限制</p>
<blockquote>
<ul>
<li>（HD版本）使用全图补丁，增加了训练时间。同时视觉重采样器中引入了更多的可训练参数，当训练数据数量有限时可能不能有效收敛。</li>
<li>不能处理多图，缺少In-context learning能力，因为缺少这样的指令微调数据，以及上下文长度的限制。</li>
<li>在一些特定领域效果不佳。</li>
<li>依旧存在幻觉和错误信息。</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="LLaVa-NEXT（20240130）"><a href="#LLaVa-NEXT（20240130）" class="headerlink" title="LLaVa-NEXT（20240130）"></a>LLaVa-NEXT（20240130）</h2><div aligh="center"><img src="/asset/2/any_res.png" class="lazyload" data-srcset="/asset/2/any_res.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<blockquote>
<ol>
<li>使用Any-Res增加更大的输入分辨率：三种aspect ratio：672x672,336x1344,1344x336。能够捕捉更多的视觉细节。</li>
<li>更好的Visual reasoning和OCR能力：使用的高质量的User Instruct数据，包括LAION-GPT-V和ShareGPT-4V；使用多模态的表格&#x2F;文档数据。</li>
<li>对于更多场景的视觉对话能力：拥有更好的world knowledge和逻辑推断能力。</li>
</ol>
</blockquote>
<h2 id="LLaVa-NEXT（20240525）"><a href="#LLaVa-NEXT（20240525）" class="headerlink" title="LLaVa-NEXT（20240525）"></a>LLaVa-NEXT（20240525）</h2><blockquote>
<p>模型结构：scaling LLM比scaling Vison Encoder更有效提升模型性能。视觉信息的提升主要受分辨率、特征空间token数的限制。选用SigLIP-SO400M-ViT作为vision encoder，Qwen-1.5 0.5B作为LLM。Connector还是2层带ReLu的MLP。</p>
</blockquote>
<blockquote>
<p>视觉表征：使用更多的grid，以及双线性插值。</p>
<div aligh="center"><img src="/asset/2/higher_any_res.png" class="lazyload" data-srcset="/asset/2/higher_any_res.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
<blockquote>
<p>训练策略：<br>Stage 1-Text-image alignment：使用Public data和Web data做特征对齐。<br>Stage 1.5-High-Quality Knowledge Learning：使用更高质量的数据集强化VLM的知识。包括Re-captioned detailed description data，Document&#x2F;OCR data，ShareGPT4V Chinese detailed caption。<br>Stage 2-Visual Instruction Tuning：与之前的一致。</p>
</blockquote>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>多模态论文阅读——Qwen-VL系列</title>
    <url>/2024/10/31/%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Qwen-VL%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="Qwen-VL"><a href="#Qwen-VL" class="headerlink" title="Qwen-VL"></a>Qwen-VL</h2><ul>
<li><p>Qwen-VL的模型架构</p>
<blockquote>
<p>LLM：使用Qwen-7B初始化<br>Vision Encoder：采用OpenCLIP-ViT-bigG-14<br>Adapter：Position-aware VL Adapter。</p>
<ul>
<li>该adapter包含一个随机初始化的单层cross-attention模块，该模块使用一组可训练的embedding作为query向量，使用视觉编码器输出的图像特征作为key、value。</li>
<li>另外，考虑到位置信息对细粒度图像理解的重要性，2D绝对位置编码被结合到cross-attention的query-key pairs中，以减轻压缩过程中位置细节的潜在损失。<br>作用：（1）对图片token数压缩，压缩到固定长度256；（2）同时与LLM文本模态对齐<div aligh="center"><img src="/asset/2/qwen-vl.png" class="lazyload" data-srcset="/asset/2/qwen-vl.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div></li>
</ul>
</blockquote>
</li>
<li><p>Qwen-VL的输入和输出</p>
<blockquote>
<p>输入：</p>
<ul>
<li>文本输入。</li>
<li>图像输入：与其他MLM一样。（由<img>和&lt;/img&gt;将图片token框起来）</li>
<li>bbox输入：为了增强模型对细粒度视觉理解和定位的能力，Qwen-VL的训练涉及区域描述、问题和检测的数据形式。对于任何给定的边界框，会进行归一化处理 (在[0,1000)范围内)，并转换为指定的字符串格式：”(X_{topleft},Y_{topleft}),(X_{bottomright},Y_{bottomright})”。该字符串作为文本被token化，不需要额外的位置词汇表。为了区分检测字符串和常规文本字符串，在边界框字符串的开头和结尾添加两个特殊token (<box>和</box>)。另外，为了适当地将边界框与其对应的描述词或句子相关联，引入了另一组特殊token (<ref>和</ref>)，标记边界框所指的内容。</li>
</ul>
</blockquote>
</li>
<li><p>Qwen-VL的训练过程</p>
<blockquote>
<p>训练由三个阶段组成，见上图。<br>Stage 1：预训练。使用清洗后的图文对，原始有5B的数据，清洗后有1.4B。此阶段冻结LLM，训练视觉编码器和Adapter，输入分辨率224。</p>
<div aligh="center"><img src="/asset/2/qwen-vl_ds1.png" class="lazyload" data-srcset="/asset/2/qwen-vl_ds1.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>Stage 2：多任务预训练。使用更好的图文对数据以及图文交错数据，包括captioning、VQA、Grounding、OCR等。此阶段所有部分都要训练。使用更大分辨率448.</p>
<div aligh="center"><img src="/asset/2/qwen-vl_ds2.png" class="lazyload" data-srcset="/asset/2/qwen-vl_ds2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>

<p>Stage 3：SFT。采用多模态数据（图文对和图文交错）和纯文本数据混合微调，得到350K指令微调数据，以增强其遵循指令和对话能力，得到Qwen-VL-Chat（数据中有多图数据，能够有多图对话的能力）。此阶段冻结视觉编码器，训练LLM和Adapter。这样的好处是，减弱文本能力的遗忘。</p>
</blockquote>
</li>
</ul>
<h2 id="Qwen-VL2"><a href="#Qwen-VL2" class="headerlink" title="Qwen-VL2"></a>Qwen-VL2</h2>]]></content>
      <categories>
        <category>论文阅读</category>
        <category>多模态</category>
      </categories>
      <tags>
        <tag>论文阅读</tag>
        <tag>多模态</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——Consistency Model系列</title>
    <url>/2024/11/01/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Consistency%20Model%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="Consistency-Model"><a href="#Consistency-Model" class="headerlink" title="Consistency Model"></a>Consistency Model</h2><h2 id="Latent-Consistency-Model-LCM"><a href="#Latent-Consistency-Model-LCM" class="headerlink" title="Latent Consistency Model(LCM)"></a>Latent Consistency Model(LCM)</h2>]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——DiT</title>
    <url>/2024/11/04/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94DiT/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="DiT"><a href="#DiT" class="headerlink" title="DiT"></a>DiT</h2><ul>
<li><p>DiT的主要结构</p>
<blockquote>
<div aligh="center"><img src="/asset/2/dit.png" class="lazyload" data-srcset="/asset/2/dit.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
DiT是从LDM改进而来的，将LDM中的U-Net替换为Transformer，VAE仍然使用Stable Diffusion的。

<p>最后一个DiT block之后，将tokens decode为output noise prediction和covariance prediction。</p>
</blockquote>
</li>
<li><p>DiT的Patchify过程</p>
<blockquote>
<p>与ViT的一样，使用卷积将压缩后的z切分成一个一个的token，并嵌入位置信息。</p>
</blockquote>
</li>
<li><p>DiT的不同条件注入机制</p>
<blockquote>
<ul>
<li>In-context conditioning：将去噪时间步和条件c作为额外两个token附在输入序列前；最后一个block之后再将它们移除。</li>
<li>Cross-attnetion：将去噪时间步和条件c连接成一个长度为2的序列，在原始transformer block中插入cross attention并注入条件。</li>
<li>adaLN：将原始layerNorm替换为adaLN,即从t和c中回归得到scale和shift。</li>
<li>adaLN-Zero：将每个ResBlock初始化为identity function。同时除了回归scale和shift外，同时在每个Residual connection前回归dimension-wise scale的参数。</li>
</ul>
</blockquote>
</li>
<li><p>AdaLN的实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">AdaLayerNorm</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embedding_dim: <span class="built_in">int</span>, num_embeddings: <span class="built_in">int</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="variable language_">self</span>.emb = nn.Embedding(num_embeddings, embedding_dim)</span><br><span class="line">        <span class="variable language_">self</span>.silu = nn.SiLU()</span><br><span class="line">        <span class="variable language_">self</span>.linear = nn.Linear(embedding_dim, embedding_dim * <span class="number">2</span>)</span><br><span class="line">        <span class="variable language_">self</span>.norm = nn.LayerNorm(embedding_dim, elementwise_affine=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x: torch.Tensor, timestep: torch.Tensor</span>) -&gt; torch.Tensor:</span><br><span class="line">        emb = <span class="variable language_">self</span>.linear(<span class="variable language_">self</span>.silu(<span class="variable language_">self</span>.emb(timestep)))</span><br><span class="line">        scale, shift = torch.chunk(emb, <span class="number">2</span>)</span><br><span class="line">        x = <span class="variable language_">self</span>.norm(x) * (<span class="number">1</span> + scale) + shift <span class="comment"># 用 timestep 进行 affine</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="SiT"><a href="#SiT" class="headerlink" title="SiT"></a>SiT</h2>]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——FLUX</title>
    <url>/2024/11/04/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94FLUX/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——Rectified Flow</title>
    <url>/2024/11/01/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Rectified%20Flow/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>

<h2 id="Rectified-Flow"><a href="#Rectified-Flow" class="headerlink" title="Rectified Flow"></a>Rectified Flow</h2><h3 id="Rectified-Flow的基本思想"><a href="#Rectified-Flow的基本思想" class="headerlink" title="Rectified Flow的基本思想"></a>Rectified Flow的基本思想</h3><blockquote>
<p>用模型去学习两个分布数据点之间的变化速度v，通过<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>x</mi><mi>t</mi></msub><mo>+</mo><mi>v</mi><mi>t</mi><mo separator="true">,</mo><mi>t</mi><mo>∈</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_{t+1}=x_t+vt,t\in (0,1) </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6389em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.1944em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mord mathnormal">t</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>，以实现将源数据转换为目标数据，v通过神经网络学习，并通过多次ReFlow（用训练好的模型生成配对数据进行第二轮的模型训练，模型参数复制上一轮），可以拉直生成轨迹，加快生成速度。</p>
</blockquote>
<h3 id="Rectified-Flow的思路"><a href="#Rectified-Flow的思路" class="headerlink" title="Rectified Flow的思路"></a>Rectified Flow的思路</h3><blockquote>
<p>给定从两个分布<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>π</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\pi_0,\pi_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>中的采样，希望找到一个传输映射T使得，当<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Z</mi><mn>0</mn></msub><mo>∽</mo><msub><mi>π</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">Z_0 \backsim \pi_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel amsrm">∽</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>时，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Z</mi><mn>1</mn></msub><mo>=</mo><mi>T</mi><mo stretchy="false">(</mo><msub><mi>Z</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>∽</mo><msub><mi>π</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Z_1=T(Z_0) \backsim \pi_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel amsrm">∽</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。<br>映射T可以通过以下常微分方程ODE，或者叫流模型，来隐式定义：</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>d</mi><mtext> </mtext><msub><mi>Z</mi><mi>t</mi></msub></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mo>=</mo><mi>v</mi><mo stretchy="false">(</mo><msub><mi>Z</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msub><mi>Z</mi><mn>0</mn></msub><mo>∽</mo><msub><mi>π</mi><mn>0</mn></msub><mo separator="true">,</mo><mi>t</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\frac{d\ Z_t}{dt}=v(Z_t,t),Z_0\backsim\pi_0,t\in[0,1].</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2412em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8962em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">t</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em;"><span style="top:-2.357em;margin-left:-0.0715em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel amsrm">∽</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8095em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span><span class="mord">.</span></span></span></span>

<p>可以想象从<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>π</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\pi_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>里采样出来的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Z</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">Z_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>是一个粒子，它从t&#x3D;0时刻开始连续运动，在t时刻以v为速度。知道t&#x3D;1时刻到达<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Z</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">Z_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>。这里用神经网络来学习v。<br>上式使用Euler法（或其变种）用离散时间进行近似计算：</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Z</mi><mrow><mi>t</mi><mo>+</mo><mi>ϵ</mi></mrow></msub><mo>=</mo><msub><mi>Z</mi><mi>t</mi></msub><mo>+</mo><mi>ϵ</mi><mi>v</mi><mo stretchy="false">(</mo><msub><mi>Z</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">Z_{t+\epsilon}=Z_t+\epsilon v(Z_t,t),</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8917em;vertical-align:-0.2083em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mathnormal mtight">ϵ</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2083em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ϵ</span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mpunct">,</span></span></span></span>

<p>其中<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span>是一个步长参数，越小越精确，但生成速度就慢。为了用较大的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϵ</mi></mrow><annotation encoding="application/x-tex">\epsilon</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em;"></span><span class="mord mathnormal">ϵ</span></span></span></span>（例如1）还保持高精度，提出“走直线”。如下图中，蓝色为真实ODE轨迹，绿色为Euler法得到的离散轨迹。</p>
<div aligh="center"><img src="/asset/2/ode.png" class="lazyload" data-srcset="/asset/2/ode.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
<blockquote>
<p>假设从两个分布中各自随机采样，有<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>∽</mo><msub><mi>π</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub><mo>∽</mo><msub><mi>π</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">x_0\backsim\pi_0,x_1\backsim\pi_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel amsrm">∽</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel amsrm">∽</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，这时的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">x_0,x_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.1944em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>并不配对。简单对它们线性插值得到<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>:</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub><mo>=</mo><mi>t</mi><msub><mi>x</mi><mn>1</mn></msub><mo>+</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>t</mi><mo stretchy="false">)</mo><msub><mi>x</mi><mn>0</mn></msub><mo separator="true">,</mo><mi>t</mi><mo>∈</mo><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">x_t=tx_1+(1-t)x_0,t\in[0,1]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7651em;vertical-align:-0.15em;"></span><span class="mord mathnormal">t</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">[</span><span class="mord">0</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">]</span></span></span></span>,

<p>对它求导得到一个简单的ODE：</p>
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>d</mi><mtext> </mtext><msub><mi>x</mi><mi>t</mi></msub></mrow><mrow><mi>d</mi><mi>t</mi></mrow></mfrac><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><msub><mi>x</mi><mn>0</mn></msub></mrow><annotation encoding="application/x-tex">\frac{d\ x_t}{dt}=x_1-x_0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2412em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8962em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mord mathnormal mtight">t</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.4101em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">d</span><span class="mspace mtight"><span class="mtight"> </span></span><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2963em;"><span style="top:-2.357em;margin-left:0em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>

<p>但这个在前向过程中不可用，因为x1并不知道，因此提出一个可前向模拟的<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo stretchy="false">(</mo><msub><mi>Z</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v(Z_t,t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">Z</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2806em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span>来逼近这个导数：</p>
<div aligh="center"><img src="/asset/2/rf_obj.png" class="lazyload" data-srcset="/asset/2/rf_obj.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
<blockquote>
<p>通过上一步，得到的ODE轨迹虽然避免了交叉，但轨迹仍然是弯曲的，这代表生成无法一步实现，因此提出<strong>ReFlow</strong>方法将轨迹进一步拉直。</p>
<p>首先用两个分布中随机采样的非配对数据训练一个Flow，称为1-Rectified Flow，然后用训练好的模型采样：<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo>=</mo><mi>F</mi><mi>l</mi><mi>o</mi><msub><mi>w</mi><mn>1</mn></msub><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_1=Flow_1(x_0)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">Fl</span><span class="mord mathnormal">o</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>，如此得到的(x0,x1)就是比较配对的数据对，用它的参数再去训练一个2-Rectified Flow。</p>
<p>2-Rectified Flow和1-Rectified Flow在训练过程中唯一的区别就是数据配对不同。理论上可以重复Reflow多次，可证明该过程其实是在单调地减小最优传输理论中的传输代价(transport cost)，而且最终收敛到完全直的状态。</p>
</blockquote>
<blockquote>
<div aligh="center"><img src="/asset/2/rf_alg.png" class="lazyload" data-srcset="/asset/2/rf_alg.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>当ODE轨迹完全拉直时，就可以通过<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>0</mn></msub><mo>=</mo><msub><mi>x</mi><mn>1</mn></msub><mo>−</mo><mi>v</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x_0 = x_1-v(x_1,1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.7333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3011em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span>一步得到去噪后的图像。</p>
</blockquote>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——PixArt系列</title>
    <url>/2024/11/04/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94PixArt%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>
<h2 id="PixArt-alpha"><a href="#PixArt-alpha" class="headerlink" title="PixArt-alpha"></a>PixArt-alpha</h2><h2 id="PixArt-sigma"><a href="#PixArt-sigma" class="headerlink" title="PixArt-sigma"></a>PixArt-sigma</h2>]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——Stable Diffusion 3</title>
    <url>/2024/11/04/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Stable%20Diffusion%203/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><span id="more"></span>]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
  <entry>
    <title>AIGC论文阅读——Stylized Generation系列</title>
    <url>/2024/11/11/AIGC%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E2%80%94%E2%80%94Stylized%20Generation%E7%B3%BB%E5%88%97/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><h2 id="StyleAligned"><a href="#StyleAligned" class="headerlink" title="StyleAligned"></a>StyleAligned</h2><ul>
<li><p>StyleAligned做的事情</p>
<blockquote>
<p>给定一张reference image，无需训练或者微调，得到一组style-consistent的生成图像。</p>
</blockquote>
</li>
<li><p>StyleAligned的做法</p>
<blockquote>
<p>StyleAligned将每个self-attention layer更改为shared attention layer，如下图：</p>
<div aligh="center"><img src="/asset/2/style_aligned.png" class="lazyload" data-srcset="/asset/2/style_aligned.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
</ul>
<blockquote>
<p>将每个batch中的第一张图片作为reference image，其他为target image。首先将target Q和K用AdaIN进行自适应normalization，如下图：</p>
<div aligh="center"><img src="/asset/2/style_aligned_2.png" class="lazyload" data-srcset="/asset/2/style_aligned_2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>然后将target的KV和reference的KV拼接，如下图：</p>
<div aligh="center"><img src="/asset/2/style_aligned_3.png" class="lazyload" data-srcset="/asset/2/style_aligned_3.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
<blockquote>
<p>对于使用真实图片的输入，先使用DDIM Inversion，然后在每一步使用shared attention。</p>
</blockquote>
<h2 id="B-LoRA"><a href="#B-LoRA" class="headerlink" title="B-LoRA"></a>B-LoRA</h2><ul>
<li><p>B-LoRA做的事情</p>
<blockquote>
<p>给定单张图片，仅在两个transformer block训练LoRA，能够将图片的style和content解耦，并进行组合。</p>
</blockquote>
</li>
<li><p>B-LoRA针对SDXL结构的发现</p>
<blockquote>
<p>通过将不同的prompt注入不同attention layer的cross attention中，发现W2和W4对生成图片的内容影响最大，W5对生成图片的style影响最大。最终选用W4来学习content，W5来学习style。</p>
<div aligh="center"><img src="/asset/2/b_lora.png" class="lazyload" data-srcset="/asset/2/b_lora.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="80%"/></div>
</blockquote>
</li>
<li><p>B-LoRA的训练方法，以及其应用</p>
<blockquote>
<p>给定单张图片，使用Dreambooth LoRA的方法，在W4和W5注入LoRA权重，来训练该LoRA权重。使用prompt “ A [v]” 来指示学习的内容。（不区分content还是style）</p>
<div aligh="center"><img src="/asset/2/b_lora_2.png" class="lazyload" data-srcset="/asset/2/b_lora_2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>可以进行style和content的组合使用。</p>
<p>训练细节见原论文，在单卡A100上对单张图片优化需要10min。</p>
</blockquote>
</li>
</ul>
<h2 id="InstantStyle"><a href="#InstantStyle" class="headerlink" title="InstantStyle"></a>InstantStyle</h2><ul>
<li><p>InstantStyle的动机</p>
<blockquote>
<ol>
<li>Style的定义不够统一：style通常不是一种元素，而是多种元素的组合。因此想要收集成对的style数据很困难。</li>
<li>通过DDIM Inversion再重建图片，会损失掉原图片的一些style细节。</li>
<li>Adapter-based方法中，过低的strength会导致image condition的intensity不够；过高的strength会导致image condition的内容泄露，使得SD的text controllability降低。</li>
</ol>
</blockquote>
</li>
<li><p>InstantStyle的一些观察点</p>
<blockquote>
<ol>
<li>IP-Adapter中图片的style和content是coupled，因此当scale比较大的时候，会有content leakage。</li>
<li>CLIP的feature space支持减法操作（震惊！）。例如对一张图片，使用CLIP Vision Encoder得到它的img feat，再对它的content使用Text encoder得到content feat（因为content很容易用text描述），将img feat减去content feat，可以得到解耦的style representation。</li>
<li>SD的Unet中不同块的作用不同。与B-LoRA类似，它们发现有两个层在style preservation中起到关键作用：up_blocks.0.attentions.1能够捕捉到style(color,material,atmosphere等)，down_blocks.2.attentions.1能够捕捉到layout(structure,composition等)。</li>
</ol>
</blockquote>
</li>
<li><p>InstantStyle的方法</p>
<blockquote>
<p>基于前面的观察点，InstantStyle提出两个简单的策略：</p>
<ol>
<li>将content从图片中解耦，即输给IP-Adapter的embedding是解耦的。</li>
<li>只将Adapter注入到Style block，即up_blocks.0.attentions.1中。</li>
</ol>
<p>经过这两个策略，能够大大减小content leakage的现象。<br>InstantStyle使用的都是pretrained-IP-Adapter，没有经过其他微调。</p>
<div aligh="center"><img src="/asset/2/instantstyle.png" class="lazyload" data-srcset="/asset/2/instantstyle.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div></blockquote>
</li>
</ul>
<h2 id="InstantStyle-Plus"><a href="#InstantStyle-Plus" class="headerlink" title="InstantStyle-Plus"></a>InstantStyle-Plus</h2><ul>
<li><p>InstantStyle-Plus做的事情</p>
<blockquote>
<p>IS+要做的使用是在做到原图content preserving的基础上来做Style transfer。</p>
</blockquote>
</li>
<li><p>InstantStyle-Plus的做法</p>
<blockquote>
<p>将任务分成了三个部分来解决：Style injection, Spatial structure preservation, Semantic content preservation。所有部分都是用现成的模型。</p>
<div aligh="center"><img src="/asset/2/instantstyle_plus.png" class="lazyload" data-srcset="/asset/2/instantstyle_plus.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ul>
<li>Style Injection：注入Style的方式与InstantStyle完全一样，只在style block注入IP-Adapter。</li>
<li>Spatial Structure Preserving：采用两种方式来保持空间结构。（1）Initial Content Latent，使用ReNoise-Inversion来做Inversion得到原图的latent，在此latent上做img2img。（2）Tile ControlNet，使用Tile ControlNet来保持空间结构及一些细节。</li>
<li>Semantic Content Preserving：同时，将原图的Global image feature用全局的IP-Adapter注入，来保持原图的语义信息。</li>
<li>Supplementary Style Guidance:同时为了更加保持style的相似性，使用CSD计算生成图片与style图片的style相似度，将其作为style guidance gradient反传回去噪过程中。</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="CSGO-Content-Style-Composition-in-Text-to-Image-Generation"><a href="#CSGO-Content-Style-Composition-in-Text-to-Image-Generation" class="headerlink" title="CSGO:Content-Style Composition in Text-to-Image Generation"></a>CSGO:Content-Style Composition in Text-to-Image Generation</h2><ul>
<li><p>CSGO做的事情</p>
<blockquote>
<ol>
<li>构建了一个Content-Style-Stylized的Triplet数据集，以及其构建的pipeline。</li>
<li>提出了一个E2E的Style-transfer框架。</li>
</ol>
</blockquote>
</li>
<li><p>CSGO数据集的构建pipeline</p>
<blockquote>
<div aligh="center"><img src="/asset/2/csgo_1.png" class="lazyload" data-srcset="/asset/2/csgo_1.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<ol>
<li>风格化图片生成：针对选取的content image和style image都训练B-LoRA，然后将它们的B-LoRA进行组合生成风格化图片。</li>
<li>风格化图片清洗：生成的图片中可能会有内容不一致的，使用一个提出的CAS(Contetn alignment score)来进行选取。具体计算：将生成图片和content image的content feature计算特征距离。如下图所示，Ada是与AdaIN类似的与风格无关的特征表示。</li>
</ol>
<div aligh="center"><img src="/asset/2/csgo_2.png" class="lazyload" data-srcset="/asset/2/csgo_2.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
<div aligh="center"><img src="/asset/2/csgo_3.png" class="lazyload" data-srcset="/asset/2/csgo_3.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>
</blockquote>
</li>
<li><p>CSGO提出的Style transfer框架</p>
<blockquote>
<div aligh="center"><img src="/asset/2/csgo_4.png" class="lazyload" data-srcset="/asset/2/csgo_4.png" srcset="data:image/gif;base64,R0lGODlhAQABAIAAAP///////yH5BAEKAAEALAAAAAABAAEAAAICTAEAOw==" width="100%"/></div>

<p>Content Control：与InstantStyle-Plus类似，一部分使用Tile ControlNet来注入content信息；另一部分，使用IP-Adapter将content注入到Base model的content block中（这里与IP+有点区别）。</p>
<p>Style Control：使用IP-Adapter-Plus的结构（resampler）将style信息注入到style bolck中。同时，为了防止controlnet中的style信息泄露，会用一个cross attention将style信息注入到controlnet中，让其提前适应注入的style。</p>
<p>整个过程中只有两个IP-Adapter和Cross attention被训练，Base model和ControlNet都冻结。使用前面采集得到的Content-Style-Stylized三元组进行训练，使用Denoising loss。</p>
</blockquote>
</li>
</ul>
]]></content>
      <categories>
        <category>论文阅读</category>
        <category>AIGC</category>
      </categories>
      <tags>
        <tag>AIGC</tag>
        <tag>论文阅读</tag>
      </tags>
  </entry>
</search>
